Notes file for object co-detection work

27 Dec 14 
--Starting notes file: general idea for this is 3 steps: 
  1) proposals (machine-detected bounding boxes for objects within images)

  2) similarity measure (a measure of the match between boxed objects in
  successive frames)

  3) graphical model solution (formulate boxes with similarity measures as graph
and then find max score path through graph by some method).

--Beginning with tang2014co paper and associated co-localize-v1 code.  Also have
the other 10 papers on co-detection that Haonan posted to Mendeley.  

--Need to look through robot video to find examples of frames with objects in
the field of view.  Right now just need series of frames of same object, not
necessarily from the same video. 

 -----------------git commit 861b30----------------

5 Jan 15 

--Epoxied tilt servo back onto pan servo gears.  Need to let set for 24 hours, 
  then will reattach camera screw.   

--First attempt at running colocalize-v1 code--had to change algorithm parameter
  in solve_qp.m from 'interior-point-convex' to 'interior-point'.  Not very good
  results on example horse pictures.  Tried on cone images from our rover
  training dataset (MSEE1dataset), got equally poor results.

--Noticed in readme for colocalize-v1 code that it is just an implementation of
  the box model from tang2014co paper--does not implement the image model or the
  saliency prior.  This could explain why results on provided test images are
  poor. 

--Got forests_edges_boxes.zip code from Haonan--used for proposal generation.
  Need to start looking at this code.  Also still need to get similarity measure
  code from Haonan.

--Talking with Dan regarding poor results of colocalize-v1 on our cone images,
  came up with the idea to infer a ground plane in the images based on our
  knowledge of the dimensions of the robot (specifically camera height off
  ground and camera tilt angle).  That should allow us to eliminate detections
  that include the floor itself.  Also discussed how to infer a 3D location for
  an object using the height/size of the object in the image with the (known)
  location of the robot.  Can use the distance from the bottom of the image to
  the bottom of the object box to infer a distance from the robot to the
  object.  With that distance, can infer the object's size based on the
  height/width of the box.  Can use this inferred 3D location to limit object
  box comparisons--only need to check similarity of boxes at the same 3D
  location.  Jeff also mentioned using Optical Flow to also aid in codetection,
  since we know the motion of the robot and can apply that to the images.

 -----------------git commit 699a5c5----------------

7 Jan 15

--Got vlfeat.zip from Haonan, which has his similarity measure.  Started working
  on getting this code running and understanding it.

--Added file edgeBoxesOut.m to ./forests_edges_boxes/.  File was missing from
  original zip file.  Also added it to forests_edges_boxes.zip.

--Followed instructions in the readme.txt in ./forests_edges_boxes/ to install
  required dependencies, such as Piotr's Matlab Toolbox and others, detailed in
  section 3 of the readme.

--Got phow_box_match.m from Haonan's vlfeat.zip running on images of traffic
  cones.  His 'scores' variable is the chi-squared distance between the
  phow_hist of boxes in the two images--ultimately a similarity score (lower =
  better).  This replaces the 5th column of the output of edgeBoxesOut, which is
  the box score (higher = better??).  Need to develop a cost function that uses
  both box and similarity score to relate a pair of boxes to each other.  Then
  can optimize this cost function using OpenGM (LOOK UP!!).

-----------------git commit eb7408d----------------

8 Jan 15 

--Used OpenCV camera calibration to get intrinsic matrix for robot front camera.
  Data is saved in camera_calibration/out_camera_data.xml.  Using that intrinsic
  matrix in Dan's  scheme projection code seems to improve the accuracy of the
  projection, but it is still off a bit.

-----------------git commit ccea402----------------

9 Jan 15

--got rover-projection.sc and toollib-perspective-projection.sc from Dan.  Can
  use pixel-and-height->world in rover-projection.sc to go from a pixel location
  to a world location.  Need to look into matlab->scheme interface in
  darpa-collaboration/ideas/toollib-matlab.

-----------------git commit e4cbb6e----------------

14 Jan 15

--Updated notes and added some papers.

-----------------git commit 49760af----------------

15 Jan 15

--Started working on some MATLAB code to approximate what is in
  rover-projection.sc.  The idea is to use some of the various MATLAB-based
  proposal generators along with the ability to map a pixel to a 3-D point (from
  the Scheme code) to help in the eventual cost function for the similarity
  between object boxes.  Have video imported and broken into frame images, and
  sentence imported.  Next need to work on getting odometry and frame timing
  information into MATLAB in order to associate frames with locations.  Then
  need to get efficient method to find location of proposal box (probably center
  of bottom of box), using the assumption that it is on the ground plane.  This
  assumption won't be always valid, but using it should give us bogus (or at
  least unique) locations for bad boxes.  For good boxes we should get a
  relatively consistent 3-D location for the objects.

--Had to tweak ~/Documents/MATLAB/startup.m to ensure that Piotr's Computer
  Vision Matlab Toolbox (http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html)
  works every time MATLAB starts.  Saved a copy in MATLABstartup.m

--Also downloaded EKFMonoSLAM and two related papers by Civera.  Need to get up
  to speed on these, since at first glance they seem to be doing exactly what
  we're doing.

-----------------git commit 891328a----------------

16 Jan 15

--Started working with EKFMonoSLAM code, trying to adapt it to use our video.
  Having a few problems with getting initial point matches on first iteration of
  main loop--I think it is because we aren't getting enough candidate matches.
  I think I need to either read their papers or dig a little deeper into the
  code in order to figure out how they're doing their thing and see if I can
  adapt the code to our images.  Already had to make a few mods, so have virgin
  copy of code in ~/Downloads/EKF...

-----------------git commit 73de280----------------

22 Jan 15

--More work on getting EKFMonoSLAM code to work with our videos.  I think I need
  to get a camera calibration with our camera that is compatible with their
  format.  Still working on this.

-----------------git commit ef63018----------------

23 Jan 15

--Redid OpenCV camera calibration using PPM images instead of JPGs.  Tried
  calibration with a varying number of images to use for calibration.  Saved
  each in camera_calibration/out_camera_data_#ppms.xml.  Ended up getting the
  best results (judged by wall/door lines in background being parallel) from the
  least number of images, 25.  Higher numbers had significant curvature in lines
  that should have been straight.  Next step is to take calibration output and
  figure out how to convert it into the format used by EKFMonoSLAM.
--Figured out the mapping between the OpenCV camera parameters and the
  parameters in the Bouguet model, which is the one that the EKFMonoSLAM code
  can convert from into their model using the code in
  EKF_monoSLAM_1pRANSAC/matlabcalibration2ourcalibration/Bouguet2Tsai.m.
  Original data is saved in matlab_calibration.mat and output saved in
  my_cam_params.m, both in the above directory.
--Got EKFMonoSLAM to start working by doubling the window sizes in
  initialize_a_feature.m (since original code was meant for 320x240 images and
  we have 640x480).  It runs for a few steps and then starts throwing the
  same error it was before--when in select_random_match.m, tries to index an
  element of positions_individually_compatible but can't because
  numel(positions_individually_compatible)=0. I need to figure out why it
  doesn't find any positions that are compatible between frames.

-----------------git commit 6ad0f80----------------


Email to Jeff 27 Jan 15

I'm making progress.  I got the camera calibration and image size (they were
using 320x240 images, and ours are 640x480, so I had to find and change a number
of hard-coded parameters) issues figured out and now I can run their code on our
videos.

The problem I'm running into now is that the code runs for a few frames and then
hits an error.  If I'm interpreting the error message correctly, what's
happening is that it's not finding enough potential matches between the interest
points detected in the two frames.  That is critical to their algorithm because
they are using just the images to localize the camera.  However, since we have
odometry data to work with, that's not as important to us.  Since we can
localize the camera, the rough estimates of where a point is in relation to the
camera can be useful.

What I'm trying to do now is look within the data structures they use to find
the location (presumably with error) estimates for interest points that they
come up with via their algorithm.  Since I'll just be using these to trim the
number of proposals, I don't think I need anything terribly accurate--I just
want to be able to reject proposals that are clearly above or below the ground
plane.


Reply from Jeff 28 Jan 15

   The problem I'm running into now is that the code runs for a few frames and
   then hits an error.  If I'm interpreting the error message correctly, what's
   happening is that it's not finding enough potential matches between the
   interest points detected in the two frames.  That is critical to their
   algorithm because they are using just the images to localize the camera.

I'm not sure I understand how doing what appears below addresses the issue
raised above.

   However, since we have odometry data to work with, that's not as important to
   us.  Since we can localize the camera, the rough estimates of where a point
   is in relation to the camera can be useful.

   What I'm trying to do now is look within the data structures they use to find
   the location (presumably with error) estimates for interest points that they
   come up with via their algorithm.  Since I'll just be using these to trim the
   number of proposals, I don't think I need anything terribly accurate--I just
   want to be able to reject proposals that are clearly above or below the
   ground plane.

Are you saying that

 1. You are not going to run their full algorithm. That their algorithm has two
    parts or stages. The first works and the second doesn't. So you are going to
    discard the second and just use the first because that is all you need?

or

 2. Part of their algorithm fails. And you will replace or augment that part
    with odemtry data and attempt to run their full algorithm with this change?

    Jeff (http://engineering.purdue.edu/~qobi)


My reply to Jeff 28 Jan 15

I was trying to describe something like 1, where I would use parts of their
algorithm to get additional data to use with proposals and similarity measures.

However, after digging deeper into their code and talking with Dan yesterday,
I've decided to put the monocular SLAM aside for now.  Their code is simply too
opaque for me to easily find the data that I'm looking for.  As you said before,
it's starting to become a rabbit hole.

What I'm going to try instead is to use the code that Dan and I came up with a
few weeks ago (where we were projecting the lines and shapes onto the video) to
extract more data from the images in order to build additional similarity
measures.

The general idea is this: (Dan, please jump in if you see anything I missed in
what we discussed yesterday)

1) Get proposal boxes from one of the various proposal mechanisms we have.

2) Use our code, along with the assumption that the bottom of the proposal box
rests on the ground plane, to determine a world location for the box.  I know
that the assumption will be invalid for some boxes, but those boxes will still
get a world location. We can then use the known boundaries of our driving area
to eliminate proposals that are outside our boundaries.  That should do roughly
half of what I was trying to do with the monocular SLAM part.

3) For the boxes that remain after this cut, do 3 different similarity measures:
    1) Visual similarity (what the default similarity measures use) 
    2) Location similarity (from 2 above) 
    3) Object size similarity--like in 2, we can also compute the world height
    and width of a box once we know its distance from the camera.  The thought
    here is that boxes that surround the same object should be more similar in
    size than two random boxes. 

4) These 3 similarity measures should give us a reasonable amount of data to
work with when formulating the graphical model.  Once we have the model
formulated and have boxes grouped into clusters that represent the same object,
we can then use some sort of average of the locations of those boxes to
determine the object location.

We also considered that we will probably get a lot of proposal boxes that detect
the ground right in front of the rover.  These will not get rejected by 2
because they will be in bounds.  While these boxes will probably get high visual
similarity, I doubt that size and location similarity will be very high.  But
should these boxes pass through and get clustered into one or more objects, we
can still look at the area where the box locations are and set some sort of
threshold to reject clusters where the proposal locations are spread over too
wide an area.

This is of course a work in progress.  I'm sure the plan will continue to evolve
as we run into problems or think of things we hadn't thought of before.

28 Jan 15

--Stopped working on EKFMonoSLAM because it started to look like the
  effort/reward ratio for using their code would not be worth it.  I think I can
  get roughly half the effect of their code by using the code Dan and I
  developed to project from pixel location to world
  location. (source/rover-projection.sc) 

--Resuming work on robottest.m to unite the projection stuff in Scheme with the
  proposal and similarity stuff from MATLAB.

--Using the assumption that all proposal boxes are on the ground plane, our code
  can find the distance of a pixel from the cam, and with the odometry can turn
  that into a world location.  We know this assumption is not valid in all
  cases, but it should be the case that proposal boxes that are actually above
  the ground plane will get distances that are farther away from the camera.
  Since we know the boundaries of our driving area, we can exclude boxes that
  have locations outside the boundaries.

--From there, we can establish 3 similarity measures:
       1) Visual similarity (should be the default for current methods).
       2) Location similarity (from location determined as above).
       3) Object size similarity--We can compute the world height and width of a
       box once we know its distance from the camera.  The idea is that boxes
       that surround the same object should have a similar size.

--Use these 3 similarity measures in the graphical model formulation.  This
  should allow us to group boxes in successive frames into clusters that
  represent an object.  We can then use some sort of average (weighted somehow?)
  of the box locations to determine object location.

--May also have a lot of proposal boxes that detect the ground right in front of
  the rover.  These will probably not get rejected based on distance because
  they will be in bounds.  Visual similarity of these might be high, but
  location similarity may be low, and size similarity most likely will be low.
  Should these proposals cluster into an object anyways, we can look at the area
  over which the locations are distributed and set some sort of threshold to
  eliminate clusters spread over too wide an area.

-----------------git commit e46b217----------------

29 Jan 15

--Modified log_to_track.cpp (in vader-rover/position) to have estimates inserted
  with original sensor readings.  Need this so that the timing info in the
  sensors is preserved, so that we can associate a time with a position
  estimate.  Then we can use the times to link a frame to a position.

--***WILL NEED TO USE THIS TO PRODUCE imu-log-with-estimates.txt FOR DATA ON
     MANUALLY DRIVEN RUNS****

--Have the reading of the position estimates (with time) is working from
  imu-log-with-estimates.txt.  Should also work with imu-log.txt produced from
  autodrive (have not tested this yet).  Next step is to match frames to
  positions using the time data.

-----------------git commit 7232960----------------

30 Jan 15

--Have frames matched with position using time data.  For test data, the mean
  difference between frame time and position time is 0.97ms, with a max of
  around 2 ms.  This should be good.

--Working with just 2 images for now (frames 21 and 22) that are temporally
  adjacent and have the cone prominent in them.

--Jeff gave me the idea to use optical flow between frames for reprojecting
  proposals.  See email dated 30Jan15.  Email has C code to do this.  Found some
  MATLAB code (flow_code.zip) that claims to do similar stuff--need to check
  this out more.

--Also need to put camera calibration stuff into MATLAB to do projection of
  boxes into world coordinates.

--Talking with Dan and recognized the need to get at least a very simple dummy
  GM solver built (with MATLAB-scheme-C glue) in order to test out how it all
  will work together.  Look at code in ~/codetection/opengm.

----------------git commit 6484f48----------------

5 Feb 15

--Started working with Haonan's updated sentence-codetection code.  It's
  currently built to be a stand-alone executable, but I think for my purposes it
  would be better to have it built into DSCI so that I can call its functions
  from the interpreter.  Going to look at how to compile it in, similar to how I
  compiled in the stuff for the rover.  The goal of this part is to be able to
  get the results of the GM solver (from C++) back into Scheme in order to use
  Scheme to visualize the boxes.

--Still working on getting codetection.sc (now called toollib-codetection.sc) to
  compile into DSCI.  Getting compilation errors with duplicate
  definitions--need to go through and resolve those.

----------------git commit 77c1855----------------

6 Feb 15

--Got toollib-codetection.sc compiled into DSCI.  Had to comment out most of
  codetectionlib-c.{c,h} because of conflicts with idealib-c.  Seems to be
  working now.  Next step is to figure out how to load up my video (or a subset
  of frames), run the GM solver, and visualize the output.

--Have video loading, frame subset selection, and box drawing and visualization
  working.  Next step is to look at how the GM is formulated and see about
  adding classes.

----------------git commit d94100b----------------

11 Feb 15

--Have box->world projection working. It eliminates both boxes that are outside
  of boundaries as well as those that plot behind the camera (i.e., ones above
  ground plane).  First test has 100 original boxes going down to 39 valid
  boxes.

--Next thought is to look at doing some sort of clustering based on box
  locations.  General idea is to use a triangular or gaussian function with a
  certain radius to build a count for each box based off the distance from that
  box to the others (function value at that distance).  This might be suitable
  for forming a new score for the box.

----------------git commit 1348533----------------

12 Feb 15

--Have unary scoring function based on distance between a box and all other
  boxes.  This distance is used as an input to gaussmf (parameters are sigma=.25
  and mu=0) to get a score (higher=better) for that neighbor.  These are summed
  and then divided by the total number of proposals generated (top_k).  This
  score seems to be a big improvement over old proposal score.  Testing with 2
  frames (1 w/obj, 1 w/o) showed 39 valid boxes with scores max=.1689,
  mean=.0982 for object, 9 valid boxes with scores max=.0382, mean=.0243 for no
  object.  Need to think about how to incorporate--replace old score or do a
  linear combination with some multipliers?  Also, can we use some of this info
  to figure out a way to enable the "dummy" state?

--Need to finish implementing this as a plug-in function to the existing Scheme
  code.  Want to try it as a comparison against the original codetection.  Need
  to figure out how to get frame times/locations synced up with video outside of
  MATLAB-->Dan's code already does this, so need to adapt and use.

--Also want to think about adding to the binary (similarity) score.  The one
  Haonan uses seems pretty good, but I might be able to improve on it.  Two easy
  ones are to compare the world location and world width of boxes.  Can compute
  that easily from the data I have now.  Can't quite figure out how to do the
  world height of the box though, so can't do area so easily.

--Jeff also suggested using other scores, like distance from center of frame
  (unary), optical flow (binary or unary???), and using pb or region contour
  (??) edge detector within the box.  Not sure how to implement the last 2.

--Want to focus on running the full detector on the shortened video in
  training-videos/short (with corresponding video_front.txt and
  imu-log-with-estimates.txt).  This is 18 frames of the cone being in the
  frame.

----------------git commit 470062b----------------
 
13 Feb 15

--Have function in Scheme to align poses with frames.  Need to finish writing
  scott-run-codetection-full-video and corresponding MATLAB function to do
  different computations.  Want to do a linear combination of original proposal
  score, my gaussian distance score, and maybe another gaussian distance score
  based on distance from center of frame.  Later, look at doing a unary optical
  flow score to compare whole frame's average flow to box's avg flow-->look at
  computing using scheme->c bindings and then passing the result into MATLAB.

--Also want to relook binary scores, such as flow between boxes in subsequent
  frames, world distance between boxes, world box width (height is a problem),
  etc.  Jeff also talked about doing binary scores between all frames (not just
  subsequent ones), but can't quite wrap my head around that yet.

----------------git commit 15011c3----------------

16 Feb 15

--Started looking at Multiscale Combinatorial Grouping (arbelaez2014multiscale)
  paper and associated code in source/sentence-codetection/MCG-PreTrained.
  General idea is to have this code output a contour map for each frame and then
  compare box proposals to contour map.  

  Not exactly sure how to do comparison--could we compare area of box to area of
  contour within box?  How to compute this? Also, need a way to prevent tiny
  boxes that are completely inside contour from getting high score.

--Want to use gaussian distance measure as a unary score across multiple
  images.  Have first try at this implemented in
  source/sentence-codetection/scott_proposals_similarity.m.  Had trouble with
  calls to matlabpool() this afternoon--license wasn't available.  Need to try
  again later. 

----------------git commit 26a82bc----------------

17 Feb 15

--matlabpool() call now working--must have been licenses taken up elsewhere on
  campus yesterday afternoon.

--Now having problem within the parfor loop because the number of boxes at each
  iteration is different.  Tried padding out temp_bboxes with zeros, but that
  caused problems with the references to img using bi(n) in the phow_hist()
  call.  Then tried putting an if bi(1)>0 around that call, but got an error
  with imresize.  Next want to try either padding temp_bboxes with ones, or
  doing the parfor loop only from 1:num_nboxes.  If I do the latter, will
  probably need to do something different with the simi(:,:,t-1) assignment.

--Have Matlab code (scott_proposals_similarity.m) working correctly and
  producing matrices of f and g scores.  Now need to test with Scheme.

--Tested with Scheme and visualization and got good results.  Images saved in
  training-videos/short/detection-images-*.png.  Box seems to track the cone
  fairly well throughout, although in the first couple frames it is only on the
  base of the cone, not the body.  The output of libDAI Belief Propagation for
  this run is -2.19992 (not sure what this means--need to ask Haonan)

--Compared these results to the original version run on the same frames.  These
  images are saved in
  test-run-data/codetection-17feb15-selected-frames-cone/detection-images*.png.
  My version seems to work much better--the images from original version seem
  random.  The output of libDAI Belief Propagation for this run is 1.2092.  I'm
  assuming that since we're minimizing here, lower is better...

--I think what I've got working here is probably good enough to use--now need to
  work on getting it to work when there are no objects in the image (i.e., dummy
  state, classes, etc...)

----------------git commit 1f0e61c----------------

27 Feb 15

--Started working with MCG-PreTrained (from arbelaez2014multiscale) as a
  replacement for EdgeBoxes as my proposal mechanism.  Trying to understand the
  input and output of this code in order to integrate it into my framework.


----------------git commit 10b5261..e7ce79d----------------

2 Mar 15

--First tests using MCG look promising.  Getting good results on test
  frames--top-scoring box is surrounding object pretty well.  

--Need to create new penalty term computation.  Thinking about using an
  exponential term to penalize boxes out of bounds, like e^(-xover)*e^(-yover).
  Also need to compute term for boxes that end up projected back behind camera.

--Since each frame takes ~5s to get boxes generated, might want to think about
  doing all box generation in a single parfor loop before doing anything else.

----------------git commit bb86297..fda6e12---------------

3 Mar 15

--Benchmarking MCGboxes (new proposal generation method) and got ~330s run time
  to find all boxes in a 204-frame video (run in parallel on 5 processors).
  That is just for proposal generation--no processing of similarity or anything
  else.

--Re-ran original scott_proposals_similarity.m to compare and ran into some
  issues.  Ended up having to put an 'if (num_nboxes > 0)' around entire binary
  score part.  This shouldn't be a problem in my new method, since the bad boxes
  will just have reduced scores, instead of being eliminated like they were
  here.  Run time for this whole proposals & similarity computation was ~230s.

--Rewriting scott_proposals_similarity2.m to use MCGboxes and different
  scoring.  Have penalty terms for out of bounds and behind camera written.
  Need to compute world width and use in binary score computations.  Need to
  remove unary score piece based on intra-frame distance (marked in comments).
  Also need to fix structure of how the calls to similarity scoring work.  I
  think between-frames distance measure still needs to be in separate loop--but
  can I move the current 1:T loop into the first parfor loop?

----------------git commit c661acb---------------

4 Mar 15

--Think I have scott_proposals_similarity2.m working with MCG boxes and penalty
  terms for unary scores.  Currently it uses Haonan's original adjacent-frame
  similarity for binary scores.  Need to fix that to have binary scores between
  all boxes in all frames.  Look at using pdist2 to do the distance computations
  between box locations--should be able to just vectorize the x,y locations of
  the boxes.  Also do width comparision (simple as abs(b1-b2)).  Then use that
  as a threshold to decide which to send to the visual similarity measure.

----------------git commit b6abd8d---------------

5 Mar 15

--Got good results from MCGboxes unary scores.  System picked up both cone
  and chair in all frames of test video that I expected it to do so.  Now need
  to work on binary scores (world distance & world width & similarity between
  boxes close in world location).

--Have distance (d_score) and width (w_score) measures working.  I think I have
  the visual similarity (s_score) worked out, currently testing.  Also need to
  add a temporal coherency term (based on Optical Flow) to boxes in adjacent
  frames.  Dan says there is Scheme code to do this--need to work with him to
  figure out how to use this.

----------------git commit ff66f12---------------

6 Mar 15

--Original s_score appears to be working, but it takes a LONG time (454s for 3
  frames and top_k = 100)

--Rewrote s_score to be more efficient in computation.  I realized that I could be
  computing the same histogram multiple times the way the loop was originally
  written. Changed to instead to compute and store ALL histograms in the first parfor
  loop (when boxes are found and scored) and then just reference the correct
  histogram when computing the s_score.  Run time now ~20-25s for same data.

--Also discovered that the order of the boxes coming out is not deterministic.
  My s_score had columns in different order from different runs on the same data.

----------------git commit af89418---------------

9 Mar 15

--Tried running a MATLAB test on the full video.  Had to manually kill all
  MATLAB processes after about an hour--froze my system.  Looking at htop, it
  appeared that the parallel workers were eating up all available RAM and
  hanging the system--processors were fine.

--After restarting MATLAB, had same problem as prior with inability to check out
  license for Parallel Computing Toolbox.  No known resolution to this other
  than to wait a while and try again when licenses might be available.

--Working again.  Cleaned up code a bit and working on getting binary score
  output into the format [f1,b1,f2,b2,g] for every link that exists in the giant
  binary scores matrix.

--Trying to run a test of the full code against the full video overnight.

----------------git commit 0fe710e..c487890---------------

10 Mar 15

--Overnight test run completed on full video with top_k=10 in ~409s.  Still need
  to test with larger top_k.

--Have MATLAB finished with output boxes_w_fscore in the same format as original
  (with added world x,y,w values in further columns, and second output gscore as
  a 2-D matrix with rows [f1,b1,f2,b2,g].  All scores coming from MATLAB are for
  actual boxes--will be doing dummy box stuff in C.  Now need to work on changing
  Scheme->C and C interfaces.

--Started on Scheme work.  Have get-matlab-proposals-similarity that gets the
  matlab outputs boxes_w_fscore and gscore back into Scheme.  Also have
  load-data that loads 4 different groups of data into scheme variables.

----------------git commit 978e470---------------

11 Mar 15

--Overnight test of (load-data) resulted in a crash when I came in this
  morning.  Tried each of the four data set loads individually and they all
  worked.  Running test-data-{small,medium,large} (4,40,100 frames) ran in ~7
  minutes.  Running test-data-full by itself worked, but putting all the output
  to my scheme buffer pretty much locked things up.  Had to kill the buffer and
  try again.  Ran (load-data) again (to suppress buffer output) and it ran in
  ~14 minutes.  Seems to work OK.

--Rewrote get-matlab-proposals-similarity to be more general with inputs of
  top-k, box-size, frames poses, and normalized coefficients.  Then wrote
  helper/wrapper functions -by-frame and -full-video to deal with those cases
  (and normalize the coefficients).

--Added function run-codetection-with-proposals-similarity to run codetection
  with the output from the get-matlab-proposals-similarity functions mentioned
  above.  Believe I have the structure set up correctly.

--Next starting work on changing the C++ code for bp_object_inference in
  inference.cpp.  Have to change the structure of the g function and how its
  factors are added.  Also need to add stuff here for dummy state.  Still
  working on this--looking into how OpenGM works and how to add functions,
  specifically trying a sparse function (SparseMarray) for my g function.

----------------git commit 6513816---------------

12 Mar 15

--Continuing to work on OpenGM code in C++.  Discovered that the SparseMarray
  template referenced in the documentation has been replaced by SparseFunction
  with different variable types in the template.  Trying to figure out how to
  use these.  Have an example that compiles, but with compiler warnings (see
  inference-google-groups-post.cpp and
  compiler-warnings-google-groups-post.txt).  Posted a question to the OpenGM
  Google Group, but not very hopeful for an answer (low traffic).  Still
  searching for other documentation or examples.  Found something from Jason
  Corso's course at UMich at https://github.com/wecacuee/opengmdemo, but it
  doesn't do anything with SparseFunction

--Abandoning SparseFunction now and instead trying to do my binary scores as an
  ExplicitFunction.  Not as memory efficient, but should still work.  Saved
  previous work with SparseFunction in
  inference-with-sparsefunction-questionable.cpp. 

--Working on adding binary score functions in a loop through gscore rows.  Once
  this is finished, should be able to run a full test and see if dummy boxes get
  selected in frames with bad boxes.  Also still need to add temporal coherence
  via optical flow as fourth element to binary score (second g-like structure
  where I add its binary value to the binary value coming from g).

----------------git commit 9efaaf3---------------

13 Mar 15

--Got a reply to my google groups post from one of the code authors(Jorg
  Kappes).  He said just to comment out the offending line and not worry about
  it.  Not sure it matters much, since I think it will just be easier to use
  Explicit Function instead of Sparse Function.

--Finished first draft of new OpenGM code.  Throwing a runtime error at this
  point.  Will investigate more.

----------------git commit 7b5a615---------------

14 Mar 15

--Worked on debugging run-codetection-with-proposals-similarity.  Believe I have
  it working correctly now.  It returns the correct list of box numbers for
  test-data-small. 

--Having problems when run against other data sets.  For test-data-medium, it
  never picks a dummy box when it should.  Strangely, changing the values for
  dummy-f and dummy-g seem to have no effect on this.  Need to troubleshoot
  more.

--Found in troubleshooting that when I print out my gg matrices, the columns
  that should have -1.0 for dummy_g are showing up with 0.0 instead.  This could
  be why dummy boxes are not getting selected.  Need to figure out why this is
  happening. 

----------------git commit ccaed7d---------------

16 Mar 15

--Fixed problem with dummy_g not in gg matrices.  Doing different initialization
  on gg fixed the problem.

--Now having problem that running run-codetection-with-proposals-similarity on
  full video causes the following error:
       terminate called after throwing an instance of 'opengm::RuntimeError'
         what():  OpenGM error: libdai Error: Quantity not normalizableQuantity
	   not normalizable 	 
  Not sure why this error occurs.  Initial googling leads me to believe it might
  have to do with an approximate marginal of the factor graph being zero--going
  to try converting all score values to log domain to see if this helps.

--Also seeing the problem that when I run against test-data-medium or
  test-data-large, I tend to get all dummy boxes when I have dummy_g = 1.0.
  Might need to try other combinations of dummy_g and dummy_f.

--Committing before making change to have scores in log domain.

----------------git commit 3d85740---------------

16 Mar 15 (2)

--Still having same error after changing to log space, but now with
  test-data-medium.  Error is happening in the call to bp.infer().  Tried
  changing around the data so that there are no 0's in the gg matrices (i.e.,
  dummy_f=0.99 instead of 1.0).  Still have inf in the gg matrices though--could
  that be the problem?  Is the problem even in the gg matrices?  

--Removed all zeros and infinities from the ff and gg matrices by using
  some_small_number (currently set to 1e-10).  Instead of infinity I use
  -log(some_small_number) and instead of 0 I use -log(1-some_small_number).  Got
  it to run on test-data-medium without crashing (although the result was all
  dummy boxes--that can probably be fixed by adjusting the dummy_f and dummy_g
  parameters).  Next testing on test-data-large and test-data-full.

--Ran without crashing on test-data-large, but still crashed on test-data-full
  on the bp.infer() call.  Not sure how to debug that.  Is the graphical model
  just too damn big?  Do I need to use a different inference algorithm--maybe
  the default BP instead of libDAI BP?

----------------git commit 4f10dce---------------

17 Mar 15

--Saved test data to test-data-*.sc in /home/sbroniko/codetection/testing-data
  to speed up loading of data.  Defined reading of each in codetection-test.sc.

--Still having crash on test-data-full when all ff and gg values set to
  -log(0.5).  Trying a restructuring of the code (suggested by Haonan) so that I 
  can declare a new gg function in every iteration of the loop--basically
  amounts to first finding the frame breaks in the g score matrix and then using
  that index to jump through the g score matrix.  Saving a copy of the old
  inference.cpp to inference-before-restructure.cpp and committing.

----------------git commit 80e7d34---------------

17 Mar 15 (2)

--Tried the restructure that Haonan suggested and still had the same crash when
  running against the full video. 

--Added (~line 130 of inference.cpp) frame_distance_threshold to limit the
  difference between frame numbers when adding a gg function.  The thought was
  to just reduce the number of gg functions/factors added, since it ran fine on
  smaller data sets.  This appears to work.  Got program to run against
  test-data-full with frame_distance_threshold = {50,60}, but it crashed with it
  = {75,100}.  Might this be related to the length of the video?  Probably
  should make this number a parameter to the function.  Also need to switch
  function back to using actual read-in data, rather than testing values.

----------------git commit 60b1a93---------------

18 Mar 15 

--Email conversation with Jeff helped me clarify some of my thoughts as to why I
  had to put the frame_distance_threshold hack into my code and why that hack is
  a bad idea.  Believe I'm hitting some sort of limit on the number of
  variables/labels/factors that can be added, and I think the problem is coming
  from libDAI.  Started conversations on both the OpenGM and libDAI Google
  Groups boards asking whether this limit exists.  If I know the limit, then I
  can try to find a way to stay within it.  Have some other ideas (switch to
  OpenGM BP, use a numerical or top-n limit on the number of binary scores taken
  from MATLAB, etc...) in email to Jeff.

--First trying the switch from libDAI BP to OpenGM BP to see if a crash still
  happens on the full dataset.  With frame_distance_threshold still in place,
  OpenGM BP ran successfully on test-data-full.  Was significantly slower (~5s
  vs. ~0.1s), but still pretty fast.  Ran with frame_distance_threshold removed,
  and it ran without crashing--took ~15s, but that's still not bad.  Committing
  now before I start changing back to using real data (probably in log space).

----------------git commit 33314c5---------------

18 Mar 15

--Wrote matlab-data-to-files in codetection-test.sc to run on a
  server and put matlab output into frame-data.sc files in each directory of
  training data (plan4 thru plan9, since plan0 thru plan3 had no objects in
  video).  Left running overnight on jalitusteabe.

--Next need to revise inference.cpp to use real data (try log space first).
  Then need to write a Scheme wrapper that loads the matlab data from file, does
  the codetection, and then visualizes and saves the result.  Could also try a
  system call that takes the saved frames and puts it into a video and then
  deletes the images.  ***NEED TO APPEND EMPTY LIST TO END OF BOXES, XY BEFORE
  THIS WILL WORK***

----------------git commit a5da3af---------------

19 Mar 15

--Ran into a problem with the overnight run of matlab-data-to-files.  For
  certain runs (only found 1 so far), I get an error num-frames != frame-poses
  from within align-frames-with-poses.  This appears to be happening because
  there are frame times that are later than the latest imu log time.  Need to
  figure out a way to overcome this--perhaps just copy the last pose for all the
  rest of the frames.

--Fix by copying last pose for rest of frames seems to work,
  matlab-data-to-files no longer errors out.  Will have to see if the
  visualization generated from this data is any good
  (.../training/plan4/2014-11-15-22:47:10/frame-data.sc) 

--Found a problem in the data--some of the camera_front.txt files were missing
  one of the footer lines (Camera logging stopped), which was causing
  align-frames-with-poses to error out.  That line was the last one written from
  the rover, so the connection must have been closed before those messages made
  it into the socket on the rover end.  I found the files with the line missing
  and added the line so that the code would work correctly.  Also relaxed the
  condition in align-frames-with-poses so that now as long as there are not more
  frames than frame times, it will run and just ignore the extra frame times at
  the end of the list.

--Changed inference.cpp back to using real data in log space.  Built a
  visualization wrapper around it so that we can visualize the effects of
  changing the parameters (dummy_{f,g}, alpha, beta, gamma, etc.).  Right now
  just playing with dummy_f and dummy_g values to see how that affects when a
  dummy box is selected.  Initial testing on short video segment gave fairly
  good results when dummy_f = dummy_g = 0.6.  Running test on full test video
  overnight to see how that does--timing it with (system "date").  Also need to run on
  other videos (should be able to once the job running on jalitusteabe saves all
  the matlab data).

----------------git commit a137c47---------------

20 Mar 15

--Overnight run completed in ~28 minutes (with precomputed Matlab data, which
  took ~10 minutes itself) for a single 204-frame video.  Results look somewhat
  reasonable--it usually got sensible boxes around the objects it was supposed
  to detect, but there were quite a few spurious boxes chosen (~40% of the time)
  when it should have been choosing the dummy box.  It did choose the dummy box
  ~60% of the time when it should have chosen it, and it never chose the dummy
  box when it should have been choosing an object box.  This run was done with
  the parameters set at dummy-f = dummy-g = 0.6.  Not sure how to change these
  parameters would improve results, or if that is even worth the effort.  It
  might be more beneficial to set up a large run on one of the servers that will
  processs many of the videos and save all the output to a .sc file.  

--Jaliusteabe job to precompute matlab data is still running--plan{4,5,6,7,8} are
  complete, currently working on plan9 (last one).

--Rebuilt visualize-results to take only a path and dummy-f and dummy-g values
  as arguments.  It saves the full results or
  run-codetection-with-proposals-similarity to results-dummy-f-dummy-g.sc, and
  puts image output into the subdirectory /images-dummy-f-dummy-g/.  

--Using this with Dan's synchronous-run-commands-in-parallel-with-queueing from
  toollib-multi-process.sc to run this against all the videos we currently have
  on the 2G servers over the weekend.  New procedure is (run-full-results
  dummy-f dummy-g output-directory).  I don't think this job will end up
  overtaking the matlab precompute job that's currently running on jalitusteabe
  (which is providing the data that run-codetection-with-proposals-similarity
  takes as input), but if it does and the job doesn't finish, we should still
  have usable results from the videos that it does process.

--Got run-full-results going after some debugging and figuring out some issues
  with my .bashrc file.  Job appears to be running and producing output.

--Will check tomorrow on the results of this run.  Will also be starting on
  preparing my prelim document this weekend.

----------------git commit eaf3e6f---------------

21 Mar 15

--Job from last night looks like it completed successfully, with a runtime of
  about 4 hours.  The data from plan9 didn't get processed completely because
  the matlab preprocessing job wasn't complete by the time the results job got
  to those directories.  Starting a new job just to run on plan9, and added a
  line to run-full-results to rsync all the data back to jalitusteabe when it's
  complete.  Will check on this job tomorrow--runtime should be less than an
  hour (only processing 25 runs, while the previous 4 hour job processed 125
  runs). 

----------------git commit 94fab98---------------

23 Mar 15

--Job for processing all videos in training/plan{4,5,6,7,8,9} completed and
  results rsynced back to jalitusteabe.  Will sync these results back to
  seykhl.  Want to think about setting up a job to process the auto-drive and
  generation runs as well.

--Go through processed videos and mark each in 1 of 3 states: out of scope (no
  objects in field of view in video), at least one target object detected
  (good), no target objects detected even though they were in field of view
  (bad).

--Think about adding a constraint on the boxes that they get penalized (or
  rejected) if their height or width is > half the image size--will probably
  have to implement that in MATLAB.

--Implemented size constraint for boxes in MATLAB.  Based the constraint on a
  world width_threshold--boxes that are > than this (currently arbitrarily set
  at 1.5m) have a penalty term of exp(width_threshold-wwidth).  Will have to see
  if it improves results.  Don't think I need to do anything for height, or
  anything based solely on box size within image, since very close objects WILL
  take up a large portion of the field of view.

--Built results-end-to-end that runs matlab results and then c results.  Had to
  make a few changes to scott_proposals_similarity2.m to stop using parallel
  toolbox--had license problems again.  Now just doing images in simple for
  loop, but should still speed up b/c running on servers in parallel.  Running
  this overnight

--Still need to finish going through videos processed over the weekend.

----------------git commit ae69da9---------------

24 Mar 15

--Run on generation dataset complete at 1154. Started at 1828 last night, so
  overall runtime was ~17.5 hours.  Rsync'd data from jalitusteabe back to
  seykhl.  Based on advice from Jeff, going to do the run on the auto-drive
  dataset on the workstations instead of the 2G servers.

--Started run on auto-drive dataset at 1350.  Using 7 workstations (all but
  mine, Dan's, Jeff's, Haonan's), 42 cores total.

--Had to change matlab code to replace pdist function because of issues with
  licenses for the Statistics Toolbox.  The norm function should be a direct
  replacement for computing Euclidean distance.  Testing in matlab seems to
  work. Runtime seems to be about the same.  Restarted job at 1430.

--Noticed problem with done-22, done-28, done-27.  All ran into the error
  'error: num-frames > cam-timing', which should be in align-frames-with-poses.
  Looks like it's the same problem I noticed before, with the 'Camera logging
  stopped' footer line missing in camera_front.txt.  Need to find all instances
  of that and fix, then restart job.

--Fixed bad camera_front.txt files and restarted job at 1544.  Scripts to check
  for footer line are saved below.

--Finished reviewing output of training dataset run.  Out of 150 videos: 82 out
  of scope, 21 bad, 47 good.  For in scope videos, percent good 69.1 (47/68),
  percent bad 30.9 (21/68).

--Need to review output from generation dataset run.

----------------git commit 13274c2---------------

25 Mar 15

--Run on auto-drive dataset complete at 0935 (120 videos).  Started at 1544
  yesterday, so overall runtime was ~17h 51m--nearly the same runtime as the
  generation dataset, but with 20% more videos.  So 7 workstations > 4 2G
  servers. 

--Based on initial observations in output from generation dataset, the penalty
  term on world width didn't seem to have much of an effect.  Decreased the
  width_threshold from 1.5m to 1.0m.  Also added a penalty term to the w_score
  component of the g_score (binary score).  **THIS HAS NOT BEEN TESTED YET**

--Also added a penalty term unary score to penalize boxes that have edges within
  10 pixels (arbitrarily set) of image boundary.

--Re-running job on generation dataset with new changes to matlab code.  Have
  data saving with -new appended to the end of file/dir names.  Will compare
  against first run to see if improvement (although since results/scores have
  some degree of randomness, can't be sure a direct comparison will be all that
  informative).  Basically looking to see if it gets rid of the spurious large
  boxes. Started job at 1445.

----------------git commit c6d16a0---------------

26 Mar 15

--Re-run of generation finished at 2155 last night.  Total runtime was 7h 10m.
  HUGE improvement over running on 2g servers.  Wonder if the changes to the
  matlab code had an effect.

--Going to re-run the auto-drive dataset with new matlab code.  Should probably
  re-run the training dataset as well.  For auto-drive dataset, using 6
  workstations plus the 4 2G servers.  Started job at 0933.  For some reason,
  the rsync of the data to all the machines is taking a LONG time, >2 hours...
  Turned out there was something funky going on with jalitusteabe, so I just
  removed that machine from the list of servers.  Now 6 workstations plus 3 2G
  servers, 60 cores total (51 jobs started in first batch since the job-starting
  code rounds down on the number of available processors).

--Had to restart the above at 1302 because of an error in
  get-matlab-data-auto-drive.  I forgot to append -new on the end of
  frame-data.sc, so it would have overwritten previous data if it had run, and
  also it would have errored when visualize-data tried to find
  frame-data-new.sc.  Also removed Abdullah's workstation from the pool.  Now
  using 5 workstations and 3 2G servers (54 cores).

--Initial look at one generation run with both methods one-by-one showed that
  the new method eliminated almost all of the spurious boxes, but it also
  eliminated many valid boxes as well.  Might need to relax some of the
  constraints there--I think the pixel limitation is still OK, but might need
  width threshold back to 1.5m, and maybe eliminate the width penalty in the
  binary score.  However, looking at the next run in the series didn't show that
  problem.  Maybe a way to evaluate would be to have the matching images
  stitched side-by-side into a single image.

--Finished analyzing first run on generation dataset.  Stats: total videos =
  100, out of scope = 0, good = 80, bad = 20.

----------------git commit 5984fe6---------------

27 Mar 15

--Re-run of auto-drive still going at 1055.  One last job running on
  cuddwybodaeth, a 536-frame video that's been running in the C (OpenGM) portion
  for >13 hours.  Run finished at 1149 (~26 hour runtime with restarts).

--Going to restart another run (suffix -new2) on the generation dataset.  The
  idea is to relax some constraints in matlab, specifically the width penalty in
  the binary score.  Then going to stitch those frames together with the 1x2
  already built so 3 frames will be in a row for comparison.  Going to add
  buddhi, rongovosai, jalitusteabe back to the server pool.

--Started new run at 1111.  Joined images (3-way w/ Original|25Mar run|27Mar
  run) will be in /joined2 when done.  Had to remove wywiad and jalitusteabe
  from the server pool because the rsync kept getting hung up.  Restarted at
  1410.

----------------git commit 55a56f5---------------

28 Mar 15

--Re-run of generation with binary width penalty removed didn't seem to fix the
  problem, based on early observations.  Redoing another run on generation with
  width_threshold set back to 1.5m, so that the only difference in the matlab
  code is the addition of the pixel threshold.  Results will be in -new3 and joined3.


----------------git commit 3f1925e---------------

30 Mar 15

--Saturday's re-run of generation dataset with threshold back to 1.5m still
  didn't produce the results I expected.  On the first run of plan0, around
  frame 60, the cone comes clearly into view and stays there for about 15
  frames.  The original run from 23 March finds the cone in these frames, but
  all later runs do not.  Currently looking at the diff between the current
  matlab code and the matlab code from that date to figure out why.  The only
  substantive changes I could find was the change from pdist to norm for
  computing wwidth, and the conditions to apply the large penalty term for boxes
  too close to the boundary.  I wrote a quick test that confirmed that the
  change from pdist to norm was not a problem--computed wwidth both ways, and
  looked at the sum of the absolute difference between the ways on 15 frames
  with 10 proposals each (150 measurements)-->the sum was 6e-15, which shows
  that there two functions give the same answers.  Next need to look at the
  penalty for boxes too close to image boundary-->perhaps a penalty term
  separate from the locflag penalty (behind camera).

--Trying to do separate flags for bad_r, bad_l, bad_t, bad_b, bad_w, bad_h and
  then only impose the penalty if 2 or more flags are set.

--Started new run with run-my-shit set at:
	  (define (run-my-shit)
	   (results-end-to-end "/aux/sbroniko/vader-rover/logs/MSEE1-dataset/generation/" 10 64 1 1 1 0 0.6 0.6 "/aux/sbroniko/vader-rover/logs/MSEE1-dataset/results20150330")
 	   (join-all-images "/aux/sbroniko/vader-rover/logs/MSEE1-dataset/generation" "joined" "images-0.6-0.6-new4"))
  Output will be in joined4.  Run started at 1715.

----------------git commit 02ce3f4---------------

31 Mar 15

--Last night's run errored out because I forgot to comment out the parfor stuff
  in matlab and I couldn't get enough licenses to run the parallel toolbox.
  Restarting job at 0941.

--Job appeared to run successfully.  At 1740, it is currently working through
  the join-all-images portion.

----------------git commit 24995ed---------------

1 Apr 15

--Run completed at 1856 yesterday.  Total run time of 9h 15m.  Looking at the
  results shows general improvement--most of the spurious boxes are suppressed,
  and while some objects that were detected in the original version are missed
  in the new one, some objects that were missed in the original version are now
  detected.  I think that the current version is good enough to move forward
  with. 

----------------git commit 17c4cce---------------

2 Apr 15

--Starting a run on plans 4,7,8 of training dataset (the only ones with objects
  in the videos) so that we can have at least 3 floorplans where we have 25 runs
  per plan, instead of the 10 per plan in generation and autodrive.

--Sent prelim abstract and check sheet to Matt Golden.

--Run completed in ~4 hours on 75 training videos.  Run errored out at join
  stage because of misconfiguration--fixed and rerunning the join.

--Started looking at plots of box xy locations on floorplan.  Have a function to
  plot the boxes from a single run, need to write another function that plots
  the boxes from all runs on a single floorplan.

--Talking with Dan, decided to do a small re-run over 30 runs (i.e., < # of
  processors available) to look a the effect of changing the values for dummy-f
  and dummy-g.  Going to start with dummy-f first.  Since original value was at
  0.6, going to try 0.3 first.  Started at 1547.  This run is still going as of
  1730.

--Looking at plot of all runs in a floor plan, can clearly see clusters near
  where we know the objects are.  Need to create a sort of mesh grid over the
  area and then at each point in the mesh, do a gaussian kernel (of what
  variance?) for distance from point to box and then multiply that by the box's
  f-score.  This should give us a function with peaks where the objects are.
  Should also be able to determine the number of peaks (i.e., objects) from
  analyzing the sentences.  Then we can just pick this number of maxima from the
  function to get our object points.

----------------git commit c859642---------------

3 Apr 15

--First impressions of looking at results of small re-run in plan0, plan1, plan2
  of generation dataset shows improved results--more objects detected, but also
  more spurious detections.  However, looking at the scatter plot of detections
  in the xy plane shows a lot more noise--not sure if the correct peaks/clusters
  will be found.  Need to finish writing the function to score the points in
  this plot for peaks.  Also need to modify the current code to export the
  winning box's fscore, as well as testing code to grab these fscores from the
  matlab data.

--Modified run-codetection-with-proposals-similarity to have the fscores of the
  selected boxes as the fourth list in its output.  Shouldn't have any effect on
  previously written code that only uses the xy and pixel locations (lists 2 and
  3) of the boxes.  ANY FUTURE CODE THAT USES SCORES WILL HAVE TO HAVE DATA THAT
  WAS RE-RUN WITH THIS NEW CODE.

--Wrote ensure-scores to put winning box fscores onto results that don't have
  it.

--Modified plot-objects-from-floorplan to have boundaries and grid lines.  Base
  version plots all runs in single color, -multicolor plots each run as a
  different color.

--Built function to pass data back to matlab and currently working on
  score_detections.m in matlab to compute the score for each point in a grid.
  Need to finish testing this.

----------------git commit 0cd6d12---------------

6 Apr 15

--Got plots done on Saturday (in /tmp/sbroniko and an email to Jeff) and they
  look pretty good.  Runtime for doing the scoring function with fscore=0.6 and
  gscore=0.6 was roughly 1 hour.  Still need to do the max-finding on the
  scoring function to find the exact locations of the peaks IOT be able to
  compare them with the ground truth locations.

--Also tried a scoring function run with fscore=0.3.  That run took about 2.25
  hours, which makes sense since there were about 2.25 more detections in that
  run than in the previous run with fscore=0.6.  This is currently saved as
  scores2 in 20150406.mat.  Need to do the plots of this function.

--Need to write up plotting and max finding as functions and then wrap them in
  Scheme calls to Matlab.

18 May 15

--Cleanup commit to account for all AMT stuff in codetection-test.sc.

----------------git commit 9fa4041---------------

28 May 15

--Starting back on codetection stuff.  Found FastPeakFind function from
  http://www.mathworks.com/matlabcentral/fileexchange/37388-fast-2d-peak-finder
  that seems to work pretty well on finding the peaks in my scoring function.
  Using its sub-pixel resolution mode (case 2) does pretty well with finding the
  centroids of peaks.  Probably need to look at making my own version of this
  using the regionprops command.

--Talked with Dan about how to use these locations of my peaks as floorplans.  I
  hadn't realized that the way our acquisition system learns is by using the
  labels (and not just the locations) of the objects (so that 2 objects of the
  same type already start with the same label).  So I can't just give the
  acquisition system xy coordinates, I have to have the objects labeled (with
  the SAME label for objects of the same class).  Dan and I came up with using
  the boxed images that were located in the first step of codetection to build a
  set of images at every found peak location (within some distance of the peak)
  and then run some sort of similarity measure (reuse the old similarity
  measure, or something else?) to determine whether two objects at different
  locations were of the same class.  Do a full every-to-every image similarity
  measure between every pair of points (IN EVERY FLOORPLAN AS WELL) to determine
  an average similarity, and then use a threshold to decide if similar enough to
  be the same class.  Can then label these as OBJECT1...OBJECTN and use as
  floorplan. 

----------------git commit a2c3bae---------------

29 May 15

--Built my own version of FastPeakFind in find_score_peaks.m.  Seems to work
  correctly.  Right now it returns pixel locations (w/sub-pixel accuracy) for
  the centroids of peaks.  Thinking about integrating it into score_detections.m
  so that it can return actual xy locations for peaks.

--Have find_objects.m working.  Returns number of objects found and their xy
  locations. HARDCODED the noise threshold at 0.3--might need to relook this.

----------------git commit 41c0d15---------------

1 Jun 15

--Working on make-test-file-new to add run numbers, frame numbers and pixel
  coordinates to the detection_data structure in MATLAB.

----------------git commit da4ef42---------------

2 Jun 15

--Have find_object_error.m working; this finds the error between detections and
  ground truth for a single floor plan.  Need to automate the extraction of
  ground truth from the *dataset.sc file and also wrap to work over all floor
  plans (probably at the same time as I wrap everything else for all floor
  plans).

--Have get-detection-data-for-floorplan working in codetection-test.sc.  This
  produces the matlab detection_data file as well as saving detection images in
  a separate directory.  Need to look at possibly doing something with the
  clustering to eliminate detection images that are too far away from cluster
  centers (i.e., bad detections).  Also need to look at (ensure-scores) to
  restructure it for faster running--list-refs appear to be slowing it down.

----------------git commit 24a4df6---------------

3 Jun 15

--Wrote (get-ground-truth-from-dataset-file) function to pull ground truth data
  from *dataset.sc files.  Gives list of lists of vectors of the ground truth
  object locations for whole dataset (all floor plans).

--Rewrote (ensure-scores) to skip the time-consuming part if the results file is
  of the new type (4 lists).  Looking more into the time of it, found that the
  slowest part is the reading of the frame data file.  Not sure that I can do
  anything about this, since these are HUGE (~90MB) text files--it's just going
  to take a bit to read them.  But with the new type of results file including
  the winning fscores, this should be avoided on future runs.

--Working on cluster_detections_by_object.m to do clustering.  It seems to
  cluster correctly (NEED TO CHECK THIS MORE).  Still need to implement a
  threshold to reject (mark as invalid) detections that are too far away from a
  cluster center.  This SHOULD get rid of most of the detections that are just
  of open floor and other garbage.  Then I need to write a function (scheme or
  matlab?) that sorts the saved images by cluster--put in separate directories,
  or something else??

----------------git commit 2ae6453---------------

4 Jun 15

--Added thresholding to cluster_detections_by_object.m as input parameter.

--Have sort_by_cluster.m working to sort images by their nearest cluster center
  into tmpN directories. Seems to work pretty well with thresholds of 0.5m and
  0.4m (used in cluster_detections_by_object) to reject non-cluster detections.
  The lower threshold rejects some detections that look like they should be
  good, but there are still a good number in each directory.  Next step is to do
  comparison between each of the tmpN directories to find which items are alike
  and then move into final directories with all like images together.

--Working on sort_clusters_single_floorplan.m.  Seems to work correctly so far.
  Need to figure out a way to determine which temp labels are related to which.
  Right now doing a comparison of each image in tmpM to each image in tmpN.
  Putting those similarity values into a MxN matrix and then playing around with
  how to reduce that matrix to a single score (max of max, max of mean, mean of
  max).  Mean of max produced an interesting result--labels that should have
  matched had an element in their row/col that was HIGHER than their diagonal
  (self-similarity) measure.  I should be able to make this work: if a diagonal
  value is the largest value in the row/col, declare it a unique label; else
  find the item larger than diagonal value and use that label.  Something like
  that--will have to be careful how I write that code (what if the 1,1 element
  is not the largest in its row?)

----------------git commit cc14e25---------------

5 Jun 15

--Have sort_clusters_single_floorplan.m working.  It gives unique labels to
  clusters, and the same label to like clusters (in my test floorplan, 3
  clusters were of a chair, and all 3 got the same label.  Did this by doing an
  every-to-every comparison of the phow features of images in M clusters, putting
  the similarity scores into a matrix, then taking the max of the mean of both
  the rows and columns (two values).  Then put these values into an MxM matrix
  of average similarity scores.  The diagonal elements of this represent
  self-similarity.  Then compare these diagonal elements to the max of its row
  and column.  If the diagonal is the max, then it is a unique label.  If it is
  not, then it gets the unique label from the value that is the max.

--Next need to work on moving images to unique label folders--should be similar
  to sort_by_cluster.m.  Going to do this at the end of
  sort_clusters_single_floorplan.m.  Will also save output (xy_with_label) to
  .mat file for later use.

----------------git commit 5f37a4e---------------

8 Jun 15

--Finished sort_clusters_single_floorplan.m.  It now saves the xy_with_label in
  the file object_xy_with_label.mat (in img_dir) and sorts the like images into
  fplabel* directories (under img_dir).

--Next need to write scheme wrapper around all the matlab stuff for a single
  floorplan.  Also need to write some code to run an entire dataset (all
  floorplans) which will eventually produce the *dataset.sc map file (same one
  that ground-truth comes from).

--Start thinking about OTHER BIG STEPS (below) to add to this--Jeff doesn't
  think what I've got so far is a big enough step to publish.

----------------git commit f22ef4c---------------

9 Jun 15

--Working on (detect-sort-label-objects-single-floorplan), the scheme wrapper to
  all the matlab stuff.  Keep hitting a 'Error: Unexpected MATLAB operator.'
  when calling sort_by_cluster and sort_clusters_single_floorplan.  Need to
  figure out what I'm doing wrong here.

----------------git commit 92fb758---------------

10 Jun 15

--Figured out error was from not having single quotes around a string to be
  passed into a matlab function.  Also added code to write the xy_with_label
  variable out as a scheme vector of vectors.

--Created a backup copy of the MSEE1 dataset in
  /aux/sbroniko/backup-MSEE1-dataset-as-of-10-jun-15/ on buddhi, seulki, and
  rongovosai. 

----------------git commit e8ce1e7---------------

11 Jun 15

--Finished cleanup of MSEE1-dataset directory.  Moved some stuff (judging
  worksheets, minimal-pairs, videos, etc.) to MSEE1-dataset-supplemental-stuff.
  MSEE1-dataset dir is now just auto-drive (comprehension), generation, and
  testing (acquisition).  Also cleaned out all old testing results from each of
  the three subdirs.  End result is that dataset is now 8.3GB (down from
  182GB).  Should significantly speed up the rsync of data for job runs.  Just
  have to remember to keep stuff cleaned up. *THOUGHT*: Do I want to just delete
  the frame-data.sc file after I use it, since it seems to eat up a lot of
  space?

--Started working on new functions to call for doing codetection and
  sorting/labeling objects in single floorplan.  Main codetection function is
  (get-codetection-results-training-or-generation), which is a modified version
  of (results-end-to-end).  Also have (get-object-detections-all-floorplans)
  which, like (get-codetection-results...), spreads the jobs over multiple
  machines/cores.  The wrapper for these two is
  (codetect-sort-templabel-training-or-generation).  I think this is
  correct--will test tomorrow once servers are back online.

--After this, then need to write code that takes the output from all floorplans
  and does comparison to put common labels on each, and also produce floorplan
  file in scheme format.  Probably should make this as parallel as possible
  (most likely matlab parfor), since it will probably be running only on a
  single machine--would be smart to run this on one of the servers, since there
  are more cores there.

----------------git commit eaa3e62---------------

12 Jun 15

--Added saving the phow histograms (fvcell matlab variable) to
  sort_clusters_single_floorplan.m.  After test run, need to look at the size of
  this file.  Could save some time when doing the all-floorplans
  comparison-->can read feature vectors in from file, but will still need to do
  the each-to-each comparisons (PARALLELIZED????)

--Started test run using 3 3g servers (verstand, arivu, perisikan) at 1428.
  Restarted job using only verstand and arivu at 1453.  This left me just shy of
  enough processors  to finish in one iteration.  Had to do it because because
  perisikan kept behaving oddly and killing my jobs.

----------------git commit 5305001---------------

15 Jun 15

--Found typo/bug in (codetect-sort-templabel-training-or-generation).  Corrected
  it and tried re-running on 7 3G/4G servers (all but upplysingaoflun).  Also
  changed all matlab-cpus-per-job values to 4 to account for under-the-hood
  matlab parallelism.

--Found another bug in (get-matlab-data-training-or-generation-improved) where I
  was trying to write the output file to a dir that hadn't yet been created.
  Corrected this and tried re-running same as above.  Run started at 1524.  

--Have another bug, similar error to before.  Need to get it figured out before
  running again.

----------------git commit 8ccb941---------------

16 Jun 15

--Figured out and fixed bugs in (codetect-sort...).  In
  (get-matlab-data-training-or-generation-improved), was missing / between dir
  names.  Also had to write (visualize-results-improved) in order to use new
  directory structure.  Restarting test at 1311.

17 Jun 15

--Job did not complete because the individual jobs that were started on
  perisikan somehow died without finishing.  Jobs on other servers completed
  correctly.  Removed perisikan from server pool and restarting job at 0920.
--Apparently perisikan problem happened because of thrashing causing a reboot.
  Don't know why that didn't happen on any of the other servers.
--Starting work on label_objects_all_floorplans.m, which will take the data
  output from each floorplan and do the following: 1) put common labels on the
  same object across all floorplans, 2) move all similar objects to a single
  dir, 3) output a floorplan file in scheme format (probably need to do this in
  a scheme wrapper).

18 Jun 15

--Job did not complete due to error in (get-object-detections-all-floorplans),
  'output-c is undefined'.  (get-codetection-results-training-or-generation)
  portion of job finished at 0118.
--Fixed code and restarted (get-object-detections-all-floorplans) at 1042.
--Job errored after rsync'ing files due to 'plan0 undefined'--think I'm missing
  quotes somewhere.  Also changed matlab-cpus-per-job in
  get-object-detections... to 12 in order to spread the work out to multiple
  machines. 
--Fixed quotes but looks like jobs errored again.  Error appears to come from
  problem with floorplan-dir-->needs to be appended to full dataset-dir for ls
  command. Fixed and restarting job at 1137.  Job errored again due to bogus
  directories in dataset (plan*/2014-*test20150615/) created a few days ago.
  Removed those dirs and restarted at 1144.
--Run completed on all floorplans at 1152.
--Results look reasonable, but not all objects detected in every floorplan--some
  floorplans had as few as 2 objects detected.  Not really sure why this is,
  maybe just poor proposal scores from those detections--can see objects like
  the shopping bag and the cone in the 'no_cluster' image dirs.

----------------git commit 1f5b458---------------

18 Jun 15

--Started working on label_objects_all_floorplans.m.  Noticed something strange
  with image files returned after rsync.  It was just in plan0 due to leftover
  garbage from testing.  Blew away old stuff on seykhl and copied in new data
  from verstand.  NEED TO FIX/CHANGE the saving location for
  detections...currently in 'detections' dir under plan*, probably should put
  data-output-dir on the end of that to distinguish different runs...then will
  need to add data-output-dir as an argument to label_objects_all_floorplans.m
--Retrying new job (up to individual floorplans stage (stage 3)) using new
  filename convention (above) and more processors per matlab job (8 for stage 1,
  12 for stage 3).  Should have end-to-end result in the morning.  Started run
  at 1705.
--Moved first round of detections into MSEE1-dataset/foo/plan*/detections to
  make sure a mistake in the above doesn't wipe out my testing data.

----------------git commit 3ff44f2---------------

19 Jun 15

--Run completed at 0558 for a total runtime of just under 13 hours.  Preliminary
  look shows that data looks similar to yesterday's run (which, at least for
  plan0, was different from the test runs--not sure why).
--Have label_objects_all_floorplans.m complete to the avg_similarity_matrix.
  Testing using the 20150618 run.  Running simultaneously on seykhl (6 cores)
  and save (48 cores).  MATLAB doesn't seem to be taking much advantage of the
  extra cores on its own--do I need to add explicit parallelism?
--Added explicit parfor on innermost loop of avg_similarity_matrix computation
  (the only place MATLAB let me do it).  Also limited by MATLAB to a max of 8
  cores, even on the 48-core servers.  Restarting simultaneous test at 1620.
--Minor cleanup of label_objects_all_floorplans.m

----------------git commit 7a42d94---------------

20 Jun 15

--Wrote (codetect-sort-templabel-auto-drive),
  (get-codetection-results-auto-drive), (get-matlab-data-auto-drive-improved) to
  run on the auto-drive dataset (no imu-log-with-estimates.txt file, just
  imu-log.txt; also directory structure has NOT changed on this dataset
  (MSEE1)).
--Started run at 1830.

----------------git commit 72c45cc---------------

20 Jun 15

--label_objects_all_floorplans.m run finished on seykhl after ~17 hours.  Same
  job on save still running at 2121 (~29 hours).

21 Jun 15

--Auto-drive jobs started at 1830 on 20 Jun 15 completed at 2314 on 21 Jun 15
  (~29 hours).

22 Jun 15

--label_objects_all_floorplans.m run on save (step 4) finished after ~54 hours.

---------Email to Jeff sent 1212----------- 
Just did a manual kill of my matlabpool workers on seykhl and save (and added
the kill to the end of my code).  I now see 1 matlab process each on seykhl and
save, which are the two interactive windows I have open now.

The comprehension runs for steps 1-3 finished last night around 11:30pm for a
total runtime of ~29 hours.  Not sure why it took so much longer than the
generation runs, but the explanation could be as simple as the comprehension
videos being longer--I haven't looked that closely at it yet, and I'm don't know
if the reason is important enough to spend much time figuring out.

I've started looking at the results from steps 1-3 on the codetection runs, and
they look reasonable.  I did notice a problem on plan0 and plan1 though.  When
Dan and I originally drove those runs, we did the first two plans and then found
an error which forced us to restart and do those two plans again.  I thought I
had gotten rid of the data from those 20 bogus runs, but apparently I hadn't.
So when I ran the comprehension data this weekend, plan0 and plan1 had 20 runs
each (instead of the 10 they were supposed to have).  This means that the step 3
data from those two floorplans is no good (steps 1 and 2 are fine since they
only deal with individual runs--step 3 aggregates all the runs in a floorplan).
I'm setting up a job to re-run just step 3 on just those two floorplans and
should have that started shortly.

The step 4 job on save finished after ~54 hours.  Not entirely sure why it took
over 3x longer for the same code and data to run on save than it did on seykhl
(8x 2.5GHz on save->54 hours vs. 5x 3.46 GHz on seykhl->17 hours).  I think the
next time I run the step 4 code I will ship the job over to one of the
workstations that's not being used interactively (probably chino, since that's
the desk where the rover sits) so that I can use all 6 processors without
bothering anyone.  Once I get the step 3 re-run started I'm going to look at the
step 4 interim results more closely and then finish off the logic to apply the
unique labels across all floorplans.
---------------end email-------------------------

--Re-run of plan0,plan1 on auto-drive started at 1242.  Had to do some
  debugging.  Working restart at 1406.  Finished at 1412.

--Found I needed to re-run plan5,plan8 as well b/c detections dir was not
  there.  Not sure why that happened--maybe rsync failed?

--As of 2101, the rerun of plan5/2014-11-21-02:27:38 and
  plan8/2014-11-21-03:03:46 had completed step 1 and was working on step 2.
  Looks like the step 1 files for both wrote correctly this time.

--Finished manually labeling generation-avg-similarity-matrix.xlsx.  Results
  don't look so good when just taking the max in the row (yellow) and column
  (orange).  Maybe think about something like finding anything within 10% of the
  diagonal value and doing some sort of voting.  Might want to think about
  bringing that back to step 3 as well--but need to think about if it would make
  a difference--observed some within-floorplan groupings that didn't make sense
  (plan8 had a box and a stool grouped as the same item, even though they were
  at different locations).

----------------git commit 83849b1---------------

23 Jun 15

--Steps 1 and 2 on the plan5/ and plan8/ bad runs finished at 0555.  Total
  runtime of ~14.5 hours.  Ran step 3 on those two floorplans and they finished
  in ~8 minutes.

--Step 3 seems to be running quickly--is there a need to change that out from
  Matlab?  Step 3 runs on 2 floorplans in <10 minutes, and on all floorplans in
  51 minutes (auto-drive) and 34 minutes (generation).  Changed number of matlab
  cores for step 3 from 12 to 20 to ensure that only 2 jobs get started on each
  machine at a time--this should bring the runtime down to ~10 minutes.

--Maybe just move step 4 into Scheme.

--Started talking with Dan on how to improve results in step 3
  (misclassification)--came up with new strategy to try--see board.

--Also worked our way back to the top step 3 and how many clusters are found
  from the scoring function.  Figured out that there's probably too much noise
  and things that are only getting seen a few times are getting drowned out in
  the scoring function.  Came up with idea to keep a count of how many times a
  particular winning box's position is visible over a whole floorplan and use
  1/count as a multiplier to the score in the scoring function.  That will
  (hopefully) ensure that things that are seen only a few times, but are only
  possible to see a few times, fare better.  Working on implementing this in
  Scheme and passing the data into Matlab for use in find_objects.  See board.

----------------git commit cab6cfa---------------

24 Jun 15

--Wrote (get-poses-that-match-frames) to take robot poses from IMU log and only
  return the ones that are closest in time to a frame time.

--Have all-poses, left-limits, right-limits and triangles defined in
  (get-detection-data-for-floorplan).  Next need to figure out the formula to
  determine whether a point is within a triangle.

--Wrote (point-in-triangle vec-p list-of-3-vecs-t) that returns 0 if point NOT
  in triangle, 1 if is in.

--Have (get-detection-data-for-floorplan) almost working--having a divide by 0
  when doing the 1/count computation--not sure how that is happening, since that
  means that a box's xy location is not ever within the field of view--should
  probably just handle the error and not worry about it.

--Error in (get-detection-data-for-floorplan) handled and function seems to be
  working correctly--need to remove dtrace?

--Next need to modify find_objects.m so that it uses the frequency data in the
  score computation.

----------------git commit 07672f3---------------

25 Jun 15

--After talking to Dan about the error in (get-detection-data-for-floorplan), he
  told me that there was a multiply by 1000 in (robot-pose-to-camera->world-txf)
  in rover_projection.sc because he took everything from m to mm.  Removed that
  and error appeared to go away.

--Remember that in (get-detection-data-for-floorplan), we set the **HORIZON** to
  pixel 205 (y).

--find_objects.m now seems to be working better and finding peaks that were
  missed before (see plan8).  

--Working with threshold value in that function to adjust when peaks get found
  (had been set at 0.3*max(scores(:))).  0.2 is too high for plan8. 0.18 works
  for plan8, but  when tried on plan0 and plan1 it merges multiple peaks into a
  single peak--no good.  Might just be best to leave the threshold at 0.3 and
  accept the fact that some peaks will be missed.

--Other ideas: 1) set the threshold as a multiple of the mean value of the
  scores function--Dan says that might not be a good idea, but I'm not sure
  why--maybe something with the mean and the std_dev? 2) try starting a
  threshold at the max value and then bringing it down, stopping when previously
  separate peaks merge into one.

--Trying stuff, and saw that using mean + 1.5*std_dev gives a pretty good result
  on plan8--need to look at other plans.

----------------git commit eb967f0---------------

26 Jun 15

--After testing some different settings, decided on using mean+2*std_dev for the
  threshold in find_objects.m.  

--Tried adding some parfors in sort_clusters_single_floorplan.m.

--Parfors seem to speed up sort_clusters_single_floorplan-->running on seykhl,
  each floorplan takes ~2 minutes on average to run all of Step 3 (minus scheme
  part).

--Playing with different methods of determining visual similarity.  First tried
  1a,2a from board, but it gave a self-similarity of 1.  Thought this wouldn't
  work, so went on to try 1b,2a.  This appeared to work at first, but for
  objects with low self-similarity (< ~0.3), it gave me problems--things that
  shouldn't have been merged together were merged.

--Talked to Dan about the above and went back to 1a,2a with self-similarity of
  1.  Looked at a couple of floorplans and saw that things that should be
  re-labeled had a similarity in the relabel column of > 0.4, while things that
  should not have been relabeled did not.  Working on the logic to get the
  correct label selected.  Having a problem with like objects all starting out
  with a zero label--need to think of a way to get the first one a unique label
  without making others bounce--look at column 1 of avg_similarity_matrix0.

----------------git commit 1bad56a---------------

29 Jun 15

--Have Step 3 mostly fixed.  Wrote labels_from_avg_similarity_matrix.m to do
  labeling.  Labels correctly for all but the following: plan5--tmp1 and tmp2
  are boxes, but tmp3 (bag) has a high row score for tmp1, and the tmp2/tmp1
  similarity scores are poor; plan6--similar problem with tmp1(bag),
  tmp3(table), tmp4(cone) all being put together b/c of high column scores for
  tmp3&4 to 1--all should be separate; plan7--tmp1 and tmp4 are both chairs, but
  get labeled separately.

--Not sure how to fix the boxes being labeled separately in plan5, but maybe fix
  bad bag labeling by looking at both (average?) of row and col scores.  Same fix
  might help plan6 also.

--Tried average row/col scores fix, but didn't quite work on all problem
  spots--maybe check the ratio between the numbers.

--Ratio comparison in labels_from_avg_similarity_matrix.m seems to work well.
  No longer getting dissimilar objects with the same label.  Still have similar
  objects getting different labels, but not sure yet how to fix that.  Perhaps
  try something with 2b (from board) in sort_clusters_single_floorplan.m.

--Going to try re-running modified label_objects_all_floorplans.m on both chino
  and save with the generation-testing dataset just to see if the changes to the
  way the parfor is working improves the runtime.

----------------git commit 0642a59---------------

30 Jun 15

--Rerun of step 4 code completed on chino in 502s (~8.37 min) and on save in
  1179s (19.65 min).  Vast improvement over previous runtimes.  Now need to
  check and make sure the avg_similarity_matrix from each is the same, and then
  figure out how to derive labels from that.

--Discovered some sort of error in how feature_vectors is concatenated in
  label_objects_all_floorplans.m--current avg_similarity_matrix values computed
  from this are invalid.  Need to relook at how I'm combining these--maybe move
  the combination of the fvcell data back into step 3.

--Believe I have the feature vector combining bug fixed--re-running on chino to
  test.

--Looks like I need to fix the 'BAD JUJU' part of
  labels_from_avg_similarity_matrix.m 

--Looking at new avg_similarity_matrix in spreadsheet.  First 3 plans are good.
  Errors:
     plan3: fplabel3 is cone, but gets new label instead of previous. Need to
               relook val_ratio_threshold value. 
	    fplabel4 is book bag, but gets new label not previous.  Matching
               columns/rows seem to be stuck in noise, possibly because ~20-30%
               of images are distant shots with stuff in background--maybe look
               at doing some of the 2b stuff???
     plan4: fplabel3 is chair, but row/col ratio might be off, or noise?
            fplabel4 is book bag--row/col ratio change will fix this one
	    ...

----------------git commit 888978e---------------

1 Jul 15

--Continued errors from above:
     plan5: fplabel1 and 2 are both box, but both get different labels (also
               different from true box label)
	    fplabel5 is cone, but gets no label (fixing BAD JUJU should fix
               this)
     plan6: fplabel1 is book bag, but gets no label--this one might be tricky
               b/c of many row elements above threshold.
	    fplabel2 is chair, but gets new label--this one should have matched
               on plan7, fplabel4, but that one didn't get a valid label 
	    fplabel4 is cone, but gets no label (fixing BAD JUJU should fix
               this)
     plan7: fplabel2 is box but gets unique label--this one might not be fixable
               w/o change to how avg_similarity_matrix is computed
	    fplabel4 is chair but gets no label (fixing BAD JUJU should fix
               this)
     plan8: fplabel2 is box but gets different label--this label looks like it
     	       comes from plan5, fplabel2, so fixing that one should fix this
	    fplabel3 is book bag, but gets new label--this could be fixed by
               changing val_ratio_threshold, although this one gets some crappy
               similarity scores vs. things it should do well against--maybe
               changes to asm computation needed here
     plan9: fplabel1 is box but gets different label--looks like it's tied to
               plan8, fplabel2
	    fplabel2 is table, but gets no label (fixing BAD JUJU should fix
               this)
	    fplabel3 is book bag, but gets new label--fixing the 0 label at
               plan6, fplabel1 should fix this 

--From looking at errors, seems like the best first step is to fix the BAD JUJU
  part of the code and see what that fixes.

--Need to think of a way to fix plan3, fplabel3 (i=13)...previous matches do not
  meet threshold, but later matches do...maybe something along the lines of a
  variable to track if everything has had one pass first?...but will that stop
  everything from getting a unique label?...maybe automatically give first
  floorplan unique labels

--Fixed BAD JUJU portion of labels_from_avg_similarity_matrix.m.  Now everything
  gets a label (no more 0s).  Next problem is the labels that are higher than
  the true labels--maybe a pass backwards through the label numbers, looking for
  matches that have a lower label number?

--Working on backwards pass--seems workable.  Gets 38/43 labels correct.

--Modified sort_clusters_single_floorplan.m to implement 1a,2b for individual
  similarity matrices.  Going to test and see what the avg_similarity_matrix
  looks like for each plan, using test_step3new.m

----------------git commit c24cf90---------------

2 Jul 15

--Results of re-run of Step 3 on generation-testing are bad--dissimilar objects
  are getting the same label.  Thinking about changing Step 3 back to the way it
  was and just using the 1a,2b stuff on Step 4.  This will require splitting
  labels_from_avg_similarity_matrix.m into different versions for each step.

--Changed sort_clusters_single_floorplan.m back to using the 1a,2a method of
  finding avg_similarity_matrix elements.  Put 1a,2b into
  label_objects_all_floorplans.m.  Re-running this on chino (also accessing
  files through /net/seykhl instead of rsync--no slowdown observed).

--Hit interesting case in labels_from...step4.m.  Got to other_labels = 2 case
  and had the actual best ratio val come from a label that had already been
  eliminated because it was still 0.  Not sure how to deal with this.

--Got past previous error by just checking a condition and skipping to the next
  pass.  Now have disabled the backwards pass (after playing with the relaxation
  number) and still getting bad labelings.  Need to look at how I'm dealing with
  the multiple labels condition--maybe not doing the right thing in the initial
  loop. 

----------------git commit 896945d---------------

6 Jul 15

--Looking at new version of asm and how things are getting mislabeled.  Maybe
  ratio stuff is screwing it up.  Look at counting matches in BOTH row and
  column, and also maybe changing voting to use asm value for that box as a
  voting value.  Need to think about what to do when matches are to columns/rows
  further on in the matrix that don't have labels yet--maybe some way of keeping
  track of who matches to whom??

--SCRATCH ALL OF THE ABOVE REGARDING LABELING STEP 4 OUTPUT IN MATLAB.  Instead
  formulate the step 4 labeling problem as a graphical model where each detected
  peak is a variable that is given one of k labels (k is an input).  Unary
  scores for each label will be uniform since we don't care which particular
  label a peak gets, just that the peaks corresponding to the same objects get
  the same label.  Then the binary scores between labels will be one of two
  values: the first (a) will come from the average of the two computed scores
  for each pair of peaks in avg_similarity_matrix and will represent the score
  between two peaks when their labels are the same; the second (b) will be a
  constant value (actual number probably doesn't matter much) that will
  represent when two peaks have different labels.  Also need to think about
  implementing an "other" label (similar to dummy proposals) for the weird
  objects that shouldn't have a label.

--Look back at notes starting 11 Mar and code in inference.cpp for how to build
  graphical model solver.

----------------git commit b7f327c---------------

7 Jul 15

--Starting work on bp_label_inference in inference.cpp to do GM solving for peak
  labels.  Should be a (simpler) clone of bp_object_inference already there.
  Main input will be in **g, which will be avg_similarity_matrix from matlab.
  Can manipulate it with (matlab-get-variable ...), (matrix->list-of-lists ...),
  and (easy-ffi:double-to-c 2 ...).  All other input will be constant
  ints/doubles.  Output will be on *labels, similar to *boxes in
  bp_object_inference.  Look at (run-codetection-with-proposals-similarity) in
  codetection-test.sc to see how to manipulate input and output variables.

--Have f score part of bp_label_inference done.  Working on g score part.  Need
  to make sure I'm thinking it through correctly (see comments in code).  Should
  be able to do a single pass through all peaks, adding matrices between that
  peak and all future peaks--correct??

----------------git commit af4e6cd---------------

8 Jul 15

--Finished bp_label_inference and it compiles into dsci.  Working on testing.

--(bp-label-inference) runs, but gives results that are meaningless (usually all
  the same label).  Building a testing function (test-labeling) in
  label-test-setup.sc in order to test quickly.  Need to figure out if the bad
  results are because of the input parameters or because of an error in the GM
  solver code.

--Initial testing shows the GM solver is quite sensitive to the value chosen for
  default-g-value, which I guess makes sense.  Probably need to add some
  debugging statements to inference.cpp to take a look at what the gg matrices
  look like.


**getting -0 when f or default-g are 1...is that causing a problem?  Maybe try
  -1.0*log instead of -log

**STARTING COMMAND**
(define *data-output-dirname* "test20150617")
(codetect-sort-templabel-training-or-generation *data-output-dirname*)


(define *data-directory*
"/aux/sbroniko/vader-rover/logs/MSEE1-dataset/generation/")
(define *output-directory*
"/aux/sbroniko/vader-rover/logs/MSEE1-dataset/results-test20150617")
(define *results-filename* "test20150617/results-0.6-0.6.sc")
(define *frame-data-filename* "test20150617/frame-data-0.6-0.6.sc")
(define *server-list* '("verstand" "arivu" "aruco" "save" "akili" "aql"))
(define *source-machine* "seykhl")
(get-object-detections-all-floorplans
	 data-directory ;; NEED slash on data-dir
	 output-directory ;;NO slash on output-dir--this is a full path
	 results-filename ;;remember to add data-output-dir to this and frame-data...
	 frame-data-filename
	 server-list
	 source-machine ;;just a string, i.e., "seykhl"
	 )

***Need to start thinking about the function to take the individually labeled
   floorplans and do comparisons to put common labels on all objects
   --Have the feature vectors saved in phow_hist_fvcell.mat.  Should use those
     instead of recomputing feature vectors.
   --Will need to take individual matrices in fvcell cells and combine the ones
     that match (look at object_xy_with_label.mat to see fp labels)--also need
     to read in the mat files and rename individually, since all have same
     variable names in them--probably use a 2-d cell array to hold matrices
   --Then do something like the comparison starting at line 45 of
     sort_clusters_single_floorplan.m 


**THEN start figuring out the wrapper to do codetection, sort/label objects in
  single floorplan, and then compare and label objects consistently across all
  floorplans. 
   --probably need some new matlab code to run the similarity comparison
   --have to think about dataflow--detections on each run can be done
     independently, but then will need to have an rsync back to a common dataset
     to do floorplans, then another rsync before doing entire dataset (this will
     probably only be one processor unless I can figure out a way to
     parallelize)
   --should also think about reorganizing output data (like in a 'codetection'
     dir under each run)
   --Do I already have code that ensures track.sc is present?? I think so.


* remember that (run-codetection-with-proposals-similarity ) is the main
  function to generate the results; currently only called in
  (visualize-results), which saves the boxed images as well

--------------------------------------------------------------------------------
***TO JEFF: list of 5-10 bullet points of a publishable increment over tacl2015
   -> schedule of milestones to get a paper out for 15sep15 (ICRA or AAAI).

Bullet points:
1. Detect objects in video frames using off-the-shelf object detector (done).

2. Find real-world locations for detected objects (done).

3. Cluster detection locations and extract N cluster centers to find
detected locations of N objects (done).

4. Measure error as distance between detected and ground-truth locations.

5. Group detection images by cluster and do visual similarity
comparison of clusters--both within and between the various floor
plans--so that like objects have same temporary label.

6. Use observed floor plans with temporary labels as replacement for
hand-coded ground-truth floor plans.  Re-run acquisition with these
observed floor plans.  This should result in learning the object
classes, as was done in the original acquisition runs.

   A. From original Acquisition part of MSEE1-dataset, we only have
3/10 floor plans (75/250 runs) that have observable objects in them,
which is not enough to use for Acquisition.  Can fix this in one of a
couple ways:
     1. Do a manual re-drive of the other 7 floor plans (from original
machine-generated sentences) with physical objects this time.  Traces
should be similar enough to the original traces that we could use the
AMT-generated sentences with these new traces.
     2. Do the same as (1) above, except put the new traces back up on
AMT to get new sentences for each, just to be sure that the traces and
sentences match.
     3. Machine generate 7 new floor plans with 25 new sentences each,
drive them, and get AMT sentences from the traces.

  B. Another option is to repurpose the Generation, Autodrive (MSEE1),
or Autodrive (MSEE2) data for this experiment.  I have been using the
Generation data (manually-driven traces) in the testing of the work
I've done so far.  We don't have human-generated sentences for these
traces, but we do for both the old (MSEE1--10 runs/floor plan) and new
(MSEE2--30 runs/floor plan) Autodrive data sets.  I don't know if it
matters that the Autodrive traces were not driven by hand.  We could
also put the original Generation traces up on AMT to collect
human-created sentences for those 100 traces and then use that for the
new acquisition run.

7. Compare learned object classes to ground truth to measure error.

--------------------------------------------------------------------------------
I think the above should be enough of an increment beyond tacl2015 to
be publishable.  If not, I can look at doing the following additional
tasks:

8. Use detected image data to train class-specific object detectors.

9. Substitute these object detectors for the generic object detector
in (1) above, and re-run steps 1-4 to find out if trained object
detectors give more accurate (i.e., closer to ground truth) detections
than the generic object detector.


Schedule:
1-3: already complete
4: complete by m8jun
5: complete by m15jun
6: Decide on course of action for collecting additional data for new
acquisition run by m15jun.  Collect data and put up on AMT (if
necessary) by f19jun.  Anticipate AMT data back no later than f26jun.
Start running acquisition week of m29jun.
7: Estimate that getting results from new acquisition run will take ~1
week.  If results available m6jul, comparison should be complete by
f10jul.
8&9: Start working on these tasks around m13jul (~2 months to
ICRA/AAAI deadline).
--------------------------------------------------------------------------------


***OTHER BIG STEPS***
--Deal interactively with unknown objects
  --Might need to add rangefinder to reduce processing
  --Might need to rewrite the-force to allow changing of path mid-stream
  (processing done on laptop/workstation)
--Add more prepositions (around, between, past, etc.)
--Change frame of reference: outside observer becomes N,S,E,W, robot frame is
L,R,front, behind.
--

**Now how to collect up all the object images within a certain radius of these
  points? 
  - Have xy points where objects were detected (in each floorplan).
  - results-.-..sc in each run directory has 4 lists: 1st is index of selected
  box in each frame, 2nd is world xy locations of selected box in each frame,
  3rd is pixel coordinates of box in frame (x1 y1 x2 y2), 4th is f-score value
  of winning box.
  - Take list of detections (detection_data in MATLAB) and use first 2 columns
  for xy locations.  Then associate each with one of the object locations.
  - BUT how to go from xy locations back to frame data?  Should I somehow
  restructure my previous code to save that data somewhere?  Or can I match by
  xy location (would have to search in each of the runs, since detection_data is
  aggregated at the floorplan level).
  - Look at putting extra columns into detection_data for run#, frame#,
  proposal#--that should be enough to get back to the actual detection image.

***CONVERSATION with Dan 18 May 15-->Can account for viewing objects from one
   side by taking into account the width of the object--can be as simple as
   using the width of the object to push the object "back" 1/2 object width on
   each detection...need to look at this more systematically to make sure that
   this is really what's happening.
		

****COMMENTS FROM JEFF 6 APR 15: Look at redoing scoring function as some sort
    of Expectation Maximization (k-means, gaussian mixture models, etc.) as a
    faster way of finding clusters.  
**Thoughts from talking with Dan about the above: Can also look at using my
    current scoring function with a larger grid spacing (5cm, 10cm, etc.) to
    find initial points to use with GMM or K-means

******SEGMENT VIDEOS WITH SENTENCES IS NEXT STEP*******
**look at learning.sc in acl2015/code

***In auto-drive dataset, plan0 (and plan1?) have 20 runs, but floorplans are
   different between 1st and 2nd 10***


***BASH SCRIPTS TO LOOK FOR MISSING FOOTER LINES IN CAMERA_FRONT.TXT****
**to list filenames with grep output below 
for i in `ls | grep plan`;
 do 
 for j in `ls $i`;
  do
   echo $i/$j 
   fgrep "logging stopped" $i/$j/camera_front.txt
 done
done

**to do a count by directory
for i in `ls | grep plan`;
 do echo $i 
  fgrep "logging stopped" $i/*/camera_front.txt | wc -l
done


**if we need it, Dan and I worked out the math to compute box world height--on
  paper on my desk.


****MIGHT WANT TO KEY DUMMY BOX SCORE TO ADJACENCY--IF FRAMES ARE NOT ADJACENT,
    DUMMY SCORE IS BAD?????**** Maybe not--talking with Dan convinced me otherwise

**Still need to add binary score factor for temporal coherency / optical flow
  (Scheme->C bindings or in C directly?)

OPEN FILES IN MATLAB (28 Feb 15)
--------------------
~/codetection/source/sentence-codetection/27feb.mat (in codetection-no-git also)
~/codetection/source/robottest.m
~/codetection/source/sentence-codetection/proposals_and_similarity.m
~/codetection/source/sentence-codetection/scott_proposals_similarity2.m
~/codetection/vlfeat-0.9.17/apps/phow_box_match.m (???)



OPEN FILES IN EMACS (28 Feb 15)
-------------------
.   notes.text           28772  Text		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/notes.text
    codetection-test.sc  10504  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/sentence-codetection/codetection-test.sc
    toollib-codetection.: 6850  Scheme		  ~/codetection/source/sentence-codetection/toollib-codetection.sc
    rover-projection.sc   9062  Scheme		  ~/codetection/source/rover-projection.sc
 %  toollib-perspective: 18982  Scheme		  /home/dpbarret/imitate/tool/toollib-perspective-projection.sc
 %  tmp-toollib-misc.s: 104697  Scheme		  /home/dpbarret/imitate/tool/tmp-toollib-misc.sc
    inference.cpp         2776  C++/l		  ~/codetection/source/sentence-codetection/inference.cpp
    codetectionlib-c.h    1953  C++/l		  ~/codetection/source/sentence-codetection/codetectionlib-c.h
    idealib-c.c          40213  C/l		  ~/darpa-collaboration/ideas/idealib-c.c
    codetectionlib-c.c    6720  C/l		  ~/codetection/source/sentence-codetection/codetectionlib-c.c
    makefile|sentence-co: 2620  GNUmakefile	  ~/codetection/source/sentence-codetection/makefile
 %  makefile|ideas       14656  GNUmakefile	  /home/dpbarret/darpa-collaboration/ideas/makefile
    pregexp.sc           22557  Scheme		  ~/codetection/source/sentence-codetection/pregexp.sc
    inference.h             83  C++/l		  ~/codetection/source/sentence-codetection/inference.h

.   prelim-notes.txt       368  Text		  ~/research/prelim-notes.txt
    research.bib         43019  BibTeX		  ~/research/research.bib
    notes.text           29627  Text		  ~/research/1_lit_review/notes.text
    toollib-farneback.sc  5122  Scheme		  ~/darpa-collaboration/ideas/toollib-farneback.sc
    toollib-farneback-c: 12750  C/l		  ~/darpa-collaboration/ideas/toollib-farneback-c.c
    toollib-image-proc: 125308  Scheme		  ~/darpa-collaboration/ideas/toollib-image-processing-scott.sc
    rover-projection.sc   9062  Scheme		  ~/codetection/source/rover-projection.sc
    toollib-image-proc: 124889  Scheme		  ~/darpa-collaboration/ideas/toollib-image-processing.sc
    toollib-perspective: 18982  Scheme		  ~/codetection/source/toollib-perspective-projection.sc
    hmm-def.c            11934  C/l		  /amd/upplysingaoflun/root/aux/home/sbroniko/darpa-collaboration/ideas/hmm-def.c
    hmm-rand.c          306415  C/l		  ~/darpa-collaboration/ideas/hmm-rand.c




* incorporate optical flow into binary score...how???
* box optical flow vs. avg. optical flow might not work--need to look at how the
  geometry works out.

**look at doing single-frame optical flow-->Dan has scheme code that can use C
  bindings--will need to pull original scores out of MATLAB into scheme and then
  add appropriately-weighted optical flow scores in scheme.


******Look at saliency score for image/boxes as a substitute for f (proposal)
      score.  saliency.mit.edu
****Can look at using a comparison between some metric inside the box and
      outside the box (or over the whole frame) to get a better proposal score.
      First thought is to use average color.  Can find average color in each of
      the 3 channels and then treat as a 3-vector and compute distance.  Bigger
      distance = better score.  Could also use some different metric.
****Consider optical flow as well.  Can find optical flow for whole image,
      average it, subtract out the average, and then look at the optical flow
      inside the proposal box.

Looking at using a combination of saliency and average color to build a new
unary score.--Current similarity measure seems to do a good job selecting
consistent boxes, but which track wins depends on unary proposal score, which
doesn't seem very good.

First want to do distance/location measure on each box to reject boxes outside
of boundaries (before sending to similarity measure).  Then can also use
dist/loc as a factor for similarity.

Seem to be getting different boxes when I pull image off of avi file vs. loading
png of frame--not sure why that is.  Make sure the frame is actually the same
one.  Have checked, and isequal(im1,im2) returns 0.  Is this even important?????
