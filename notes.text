Notes file for object co-detection work

27 Dec 14 
--Starting notes file: general idea for this is 3 steps: 
  1) proposals (machine-detected bounding boxes for objects within images)

  2) similarity measure (a measure of the match between boxed objects in
  successive frames)

  3) graphical model solution (formulate boxes with similarity measures as graph
and then find max score path through graph by some method).

--Beginning with tang2014co paper and associated co-localize-v1 code.  Also have
the other 10 papers on co-detection that Haonan posted to Mendeley.  

--Need to look through robot video to find examples of frames with objects in
the field of view.  Right now just need series of frames of same object, not
necessarily from the same video. 

 -----------------git commit 861b30----------------

5 Jan 15 

--Epoxied tilt servo back onto pan servo gears.  Need to let set for 24 hours, 
  then will reattach camera screw.   

--First attempt at running colocalize-v1 code--had to change algorithm parameter
  in solve_qp.m from 'interior-point-convex' to 'interior-point'.  Not very good
  results on example horse pictures.  Tried on cone images from our rover
  training dataset (MSEE1dataset), got equally poor results.

--Noticed in readme for colocalize-v1 code that it is just an implementation of
  the box model from tang2014co paper--does not implement the image model or the
  saliency prior.  This could explain why results on provided test images are
  poor. 

--Got forests_edges_boxes.zip code from Haonan--used for proposal generation.
  Need to start looking at this code.  Also still need to get similarity measure
  code from Haonan.

--Talking with Dan regarding poor results of colocalize-v1 on our cone images,
  came up with the idea to infer a ground plane in the images based on our
  knowledge of the dimensions of the robot (specifically camera height off
  ground and camera tilt angle).  That should allow us to eliminate detections
  that include the floor itself.  Also discussed how to infer a 3D location for
  an object using the height/size of the object in the image with the (known)
  location of the robot.  Can use the distance from the bottom of the image to
  the bottom of the object box to infer a distance from the robot to the
  object.  With that distance, can infer the object's size based on the
  height/width of the box.  Can use this inferred 3D location to limit object
  box comparisons--only need to check similarity of boxes at the same 3D
  location.  Jeff also mentioned using Optical Flow to also aid in codetection,
  since we know the motion of the robot and can apply that to the images.

 -----------------git commit 699a5c5----------------

7 Jan 15

--Got vlfeat.zip from Haonan, which has his similarity measure.  Started working
  on getting this code running and understanding it.

--Added file edgeBoxesOut.m to ./forests_edges_boxes/.  File was missing from
  original zip file.  Also added it to forests_edges_boxes.zip.

--Followed instructions in the readme.txt in ./forests_edges_boxes/ to install
  required dependencies, such as Piotr's Matlab Toolbox and others, detailed in
  section 3 of the readme.

--Got phow_box_match.m from Haonan's vlfeat.zip running on images of traffic
  cones.  His 'scores' variable is the chi-squared distance between the
  phow_hist of boxes in the two images--ultimately a similarity score (lower =
  better).  This replaces the 5th column of the output of edgeBoxesOut, which is
  the box score (higher = better??).  Need to develop a cost function that uses
  both box and similarity score to relate a pair of boxes to each other.  Then
  can optimize this cost function using OpenGM (LOOK UP!!).

-----------------git commit eb7408d----------------

8 Jan 15 

--Used OpenCV camera calibration to get intrinsic matrix for robot front camera.
  Data is saved in camera_calibration/out_camera_data.xml.  Using that intrinsic
  matrix in Dan's  scheme projection code seems to improve the accuracy of the
  projection, but it is still off a bit.

-----------------git commit ccea402----------------

9 Jan 15

--got rover-projection.sc and toollib-perspective-projection.sc from Dan.  Can
  use pixel-and-height->world in rover-projection.sc to go from a pixel location
  to a world location.  Need to look into matlab->scheme interface in
  darpa-collaboration/ideas/toollib-matlab.

-----------------git commit e4cbb6e----------------

14 Jan 15

--Updated notes and added some papers.

-----------------git commit 49760af----------------

15 Jan 15

--Started working on some MATLAB code to approximate what is in
  rover-projection.sc.  The idea is to use some of the various MATLAB-based
  proposal generators along with the ability to map a pixel to a 3-D point (from
  the Scheme code) to help in the eventual cost function for the similarity
  between object boxes.  Have video imported and broken into frame images, and
  sentence imported.  Next need to work on getting odometry and frame timing
  information into MATLAB in order to associate frames with locations.  Then
  need to get efficient method to find location of proposal box (probably center
  of bottom of box), using the assumption that it is on the ground plane.  This
  assumption won't be always valid, but using it should give us bogus (or at
  least unique) locations for bad boxes.  For good boxes we should get a
  relatively consistent 3-D location for the objects.

--Had to tweak ~/Documents/MATLAB/startup.m to ensure that Piotr's Computer
  Vision Matlab Toolbox (http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html)
  works every time MATLAB starts.  Saved a copy in MATLABstartup.m

--Also downloaded EKFMonoSLAM and two related papers by Civera.  Need to get up
  to speed on these, since at first glance they seem to be doing exactly what
  we're doing.

-----------------git commit 891328a----------------

16 Jan 15

--Started working with EKFMonoSLAM code, trying to adapt it to use our video.
  Having a few problems with getting initial point matches on first iteration of
  main loop--I think it is because we aren't getting enough candidate matches.
  I think I need to either read their papers or dig a little deeper into the
  code in order to figure out how they're doing their thing and see if I can
  adapt the code to our images.  Already had to make a few mods, so have virgin
  copy of code in ~/Downloads/EKF...

-----------------git commit 73de280----------------

22 Jan 15

--More work on getting EKFMonoSLAM code to work with our videos.  I think I need
  to get a camera calibration with our camera that is compatible with their
  format.  Still working on this.

-----------------git commit ef63018----------------

23 Jan 15

--Redid OpenCV camera calibration using PPM images instead of JPGs.  Tried
  calibration with a varying number of images to use for calibration.  Saved
  each in camera_calibration/out_camera_data_#ppms.xml.  Ended up getting the
  best results (judged by wall/door lines in background being parallel) from the
  least number of images, 25.  Higher numbers had significant curvature in lines
  that should have been straight.  Next step is to take calibration output and
  figure out how to convert it into the format used by EKFMonoSLAM.
--Figured out the mapping between the OpenCV camera parameters and the
  parameters in the Bouguet model, which is the one that the EKFMonoSLAM code
  can convert from into their model using the code in
  EKF_monoSLAM_1pRANSAC/matlabcalibration2ourcalibration/Bouguet2Tsai.m.
  Original data is saved in matlab_calibration.mat and output saved in
  my_cam_params.m, both in the above directory.
--Got EKFMonoSLAM to start working by doubling the window sizes in
  initialize_a_feature.m (since original code was meant for 320x240 images and
  we have 640x480).  It runs for a few steps and then starts throwing the
  same error it was before--when in select_random_match.m, tries to index an
  element of positions_individually_compatible but can't because
  numel(positions_individually_compatible)=0. I need to figure out why it
  doesn't find any positions that are compatible between frames.

-----------------git commit 6ad0f80----------------


Email to Jeff 27 Jan 15

I'm making progress.  I got the camera calibration and image size (they were
using 320x240 images, and ours are 640x480, so I had to find and change a number
of hard-coded parameters) issues figured out and now I can run their code on our
videos.

The problem I'm running into now is that the code runs for a few frames and then
hits an error.  If I'm interpreting the error message correctly, what's
happening is that it's not finding enough potential matches between the interest
points detected in the two frames.  That is critical to their algorithm because
they are using just the images to localize the camera.  However, since we have
odometry data to work with, that's not as important to us.  Since we can
localize the camera, the rough estimates of where a point is in relation to the
camera can be useful.

What I'm trying to do now is look within the data structures they use to find
the location (presumably with error) estimates for interest points that they
come up with via their algorithm.  Since I'll just be using these to trim the
number of proposals, I don't think I need anything terribly accurate--I just
want to be able to reject proposals that are clearly above or below the ground
plane.


Reply from Jeff 28 Jan 15

   The problem I'm running into now is that the code runs for a few frames and
   then hits an error.  If I'm interpreting the error message correctly, what's
   happening is that it's not finding enough potential matches between the
   interest points detected in the two frames.  That is critical to their
   algorithm because they are using just the images to localize the camera.

I'm not sure I understand how doing what appears below addresses the issue
raised above.

   However, since we have odometry data to work with, that's not as important to
   us.  Since we can localize the camera, the rough estimates of where a point
   is in relation to the camera can be useful.

   What I'm trying to do now is look within the data structures they use to find
   the location (presumably with error) estimates for interest points that they
   come up with via their algorithm.  Since I'll just be using these to trim the
   number of proposals, I don't think I need anything terribly accurate--I just
   want to be able to reject proposals that are clearly above or below the
   ground plane.

Are you saying that

 1. You are not going to run their full algorithm. That their algorithm has two
    parts or stages. The first works and the second doesn't. So you are going to
    discard the second and just use the first because that is all you need?

or

 2. Part of their algorithm fails. And you will replace or augment that part
    with odemtry data and attempt to run their full algorithm with this change?

    Jeff (http://engineering.purdue.edu/~qobi)


My reply to Jeff 28 Jan 15

I was trying to describe something like 1, where I would use parts of their
algorithm to get additional data to use with proposals and similarity measures.

However, after digging deeper into their code and talking with Dan yesterday,
I've decided to put the monocular SLAM aside for now.  Their code is simply too
opaque for me to easily find the data that I'm looking for.  As you said before,
it's starting to become a rabbit hole.

What I'm going to try instead is to use the code that Dan and I came up with a
few weeks ago (where we were projecting the lines and shapes onto the video) to
extract more data from the images in order to build additional similarity
measures.

The general idea is this: (Dan, please jump in if you see anything I missed in
what we discussed yesterday)

1) Get proposal boxes from one of the various proposal mechanisms we have.

2) Use our code, along with the assumption that the bottom of the proposal box
rests on the ground plane, to determine a world location for the box.  I know
that the assumption will be invalid for some boxes, but those boxes will still
get a world location. We can then use the known boundaries of our driving area
to eliminate proposals that are outside our boundaries.  That should do roughly
half of what I was trying to do with the monocular SLAM part.

3) For the boxes that remain after this cut, do 3 different similarity measures:
    1) Visual similarity (what the default similarity measures use) 
    2) Location similarity (from 2 above) 
    3) Object size similarity--like in 2, we can also compute the world height
    and width of a box once we know its distance from the camera.  The thought
    here is that boxes that surround the same object should be more similar in
    size than two random boxes. 

4) These 3 similarity measures should give us a reasonable amount of data to
work with when formulating the graphical model.  Once we have the model
formulated and have boxes grouped into clusters that represent the same object,
we can then use some sort of average of the locations of those boxes to
determine the object location.

We also considered that we will probably get a lot of proposal boxes that detect
the ground right in front of the rover.  These will not get rejected by 2
because they will be in bounds.  While these boxes will probably get high visual
similarity, I doubt that size and location similarity will be very high.  But
should these boxes pass through and get clustered into one or more objects, we
can still look at the area where the box locations are and set some sort of
threshold to reject clusters where the proposal locations are spread over too
wide an area.

This is of course a work in progress.  I'm sure the plan will continue to evolve
as we run into problems or think of things we hadn't thought of before.

28 Jan 15

--Stopped working on EKFMonoSLAM because it started to look like the
  effort/reward ratio for using their code would not be worth it.  I think I can
  get roughly half the effect of their code by using the code Dan and I
  developed to project from pixel location to world
  location. (source/rover-projection.sc) 

--Resuming work on robottest.m to unite the projection stuff in Scheme with the
  proposal and similarity stuff from MATLAB.

--Using the assumption that all proposal boxes are on the ground plane, our code
  can find the distance of a pixel from the cam, and with the odometry can turn
  that into a world location.  We know this assumption is not valid in all
  cases, but it should be the case that proposal boxes that are actually above
  the ground plane will get distances that are farther away from the camera.
  Since we know the boundaries of our driving area, we can exclude boxes that
  have locations outside the boundaries.

--From there, we can establish 3 similarity measures:
       1) Visual similarity (should be the default for current methods).
       2) Location similarity (from location determined as above).
       3) Object size similarity--We can compute the world height and width of a
       box once we know its distance from the camera.  The idea is that boxes
       that surround the same object should have a similar size.

--Use these 3 similarity measures in the graphical model formulation.  This
  should allow us to group boxes in successive frames into clusters that
  represent an object.  We can then use some sort of average (weighted somehow?)
  of the box locations to determine object location.

--May also have a lot of proposal boxes that detect the ground right in front of
  the rover.  These will probably not get rejected based on distance because
  they will be in bounds.  Visual similarity of these might be high, but
  location similarity may be low, and size similarity most likely will be low.
  Should these proposals cluster into an object anyways, we can look at the area
  over which the locations are distributed and set some sort of threshold to
  eliminate clusters spread over too wide an area.

-----------------git commit e46b217----------------

29 Jan 15

--Modified log_to_track.cpp (in vader-rover/position) to have estimates inserted
  with original sensor readings.  Need this so that the timing info in the
  sensors is preserved, so that we can associate a time with a position
  estimate.  Then we can use the times to link a frame to a position.

--***WILL NEED TO USE THIS TO PRODUCE imu-log-with-estimates.txt FOR DATA ON
     MANUALLY DRIVEN RUNS****

--Have the reading of the position estimates (with time) is working from
  imu-log-with-estimates.txt.  Should also work with imu-log.txt produced from
  autodrive (have not tested this yet).  Next step is to match frames to
  positions using the time data.

-----------------git commit 7232960----------------

30 Jan 15

--Have frames matched with position using time data.  For test data, the mean
  difference between frame time and position time is 0.97ms, with a max of
  around 2 ms.  This should be good.

--Working with just 2 images for now (frames 21 and 22) that are temporally
  adjacent and have the cone prominent in them.

--Jeff gave me the idea to use optical flow between frames for reprojecting
  proposals.  See email dated 30Jan15.  Email has C code to do this.  Found some
  MATLAB code (flow_code.zip) that claims to do similar stuff--need to check
  this out more.

--Also need to put camera calibration stuff into MATLAB to do projection of
  boxes into world coordinates.

--Talking with Dan and recognized the need to get at least a very simple dummy
  GM solver built (with MATLAB-scheme-C glue) in order to test out how it all
  will work together.  Look at code in ~/codetection/opengm.

----------------git commit 6484f48----------------

5 Feb 15

--Started working with Haonan's updated sentence-codetection code.  It's
  currently built to be a stand-alone executable, but I think for my purposes it
  would be better to have it built into DSCI so that I can call its functions
  from the interpreter.  Going to look at how to compile it in, similar to how I
  compiled in the stuff for the rover.  The goal of this part is to be able to
  get the results of the GM solver (from C++) back into Scheme in order to use
  Scheme to visualize the boxes.

--Still working on getting codetection.sc (now called toollib-codetection.sc) to
  compile into DSCI.  Getting compilation errors with duplicate
  definitions--need to go through and resolve those.

----------------git commit 77c1855----------------

6 Feb 15

--Got toollib-codetection.sc compiled into DSCI.  Had to comment out most of
  codetectionlib-c.{c,h} because of conflicts with idealib-c.  Seems to be
  working now.  Next step is to figure out how to load up my video (or a subset
  of frames), run the GM solver, and visualize the output.

--Have video loading, frame subset selection, and box drawing and visualization
  working.  Next step is to look at how the GM is formulated and see about
  adding classes.

----------------git commit d94100b----------------

11 Feb 15

--Have box->world projection working. It eliminates both boxes that are outside
  of boundaries as well as those that plot behind the camera (i.e., ones above
  ground plane).  First test has 100 original boxes going down to 39 valid
  boxes.

--Next thought is to look at doing some sort of clustering based on box
  locations.  General idea is to use a triangular or gaussian function with a
  certain radius to build a count for each box based off the distance from that
  box to the others (function value at that distance).  This might be suitable
  for forming a new score for the box.

----------------git commit 1348533----------------

12 Feb 15

--Have unary scoring function based on distance between a box and all other
  boxes.  This distance is used as an input to gaussmf (parameters are sigma=.25
  and mu=0) to get a score (higher=better) for that neighbor.  These are summed
  and then divided by the total number of proposals generated (top_k).  This
  score seems to be a big improvement over old proposal score.  Testing with 2
  frames (1 w/obj, 1 w/o) showed 39 valid boxes with scores max=.1689,
  mean=.0982 for object, 9 valid boxes with scores max=.0382, mean=.0243 for no
  object.  Need to think about how to incorporate--replace old score or do a
  linear combination with some multipliers?  Also, can we use some of this info
  to figure out a way to enable the "dummy" state?

--Need to finish implementing this as a plug-in function to the existing Scheme
  code.  Want to try it as a comparison against the original codetection.  Need
  to figure out how to get frame times/locations synced up with video outside of
  MATLAB-->Dan's code already does this, so need to adapt and use.

--Also want to think about adding to the binary (similarity) score.  The one
  Haonan uses seems pretty good, but I might be able to improve on it.  Two easy
  ones are to compare the world location and world width of boxes.  Can compute
  that easily from the data I have now.  Can't quite figure out how to do the
  world height of the box though, so can't do area so easily.

--Jeff also suggested using other scores, like distance from center of frame
  (unary), optical flow (binary or unary???), and using pb or region contour
  (??) edge detector within the box.  Not sure how to implement the last 2.

--Want to focus on running the full detector on the shortened video in
  training-videos/short (with corresponding video_front.txt and
  imu-log-with-estimates.txt).  This is 18 frames of the cone being in the
  frame.

----------------git commit 470062b----------------
 
13 Feb 15

--Have function in Scheme to align poses with frames.  Need to finish writing
  scott-run-codetection-full-video and corresponding MATLAB function to do
  different computations.  Want to do a linear combination of original proposal
  score, my gaussian distance score, and maybe another gaussian distance score
  based on distance from center of frame.  Later, look at doing a unary optical
  flow score to compare whole frame's average flow to box's avg flow-->look at
  computing using scheme->c bindings and then passing the result into MATLAB.

--Also want to relook binary scores, such as flow between boxes in subsequent
  frames, world distance between boxes, world box width (height is a problem),
  etc.  Jeff also talked about doing binary scores between all frames (not just
  subsequent ones), but can't quite wrap my head around that yet.

----------------git commit 15011c3----------------

16 Feb 15

--Started looking at Multiscale Combinatorial Grouping (arbelaez2014multiscale)
  paper and associated code in source/sentence-codetection/MCG-PreTrained.
  General idea is to have this code output a contour map for each frame and then
  compare box proposals to contour map.  

  Not exactly sure how to do comparison--could we compare area of box to area of
  contour within box?  How to compute this? Also, need a way to prevent tiny
  boxes that are completely inside contour from getting high score.

--Want to use gaussian distance measure as a unary score across multiple
  images.  Have first try at this implemented in
  source/sentence-codetection/scott_proposals_similarity.m.  Had trouble with
  calls to matlabpool() this afternoon--license wasn't available.  Need to try
  again later. 

----------------git commit 26a82bc----------------

17 Feb 15

--matlabpool() call now working--must have been licenses taken up elsewhere on
  campus yesterday afternoon.

--Now having problem within the parfor loop because the number of boxes at each
  iteration is different.  Tried padding out temp_bboxes with zeros, but that
  caused problems with the references to img using bi(n) in the phow_hist()
  call.  Then tried putting an if bi(1)>0 around that call, but got an error
  with imresize.  Next want to try either padding temp_bboxes with ones, or
  doing the parfor loop only from 1:num_nboxes.  If I do the latter, will
  probably need to do something different with the simi(:,:,t-1) assignment.

--Have Matlab code (scott_proposals_similarity.m) working correctly and
  producing matrices of f and g scores.  Now need to test with Scheme.

--Tested with Scheme and visualization and got good results.  Images saved in
  training-videos/short/detection-images-*.png.  Box seems to track the cone
  fairly well throughout, although in the first couple frames it is only on the
  base of the cone, not the body.  The output of libDAI Belief Propagation for
  this run is -2.19992 (not sure what this means--need to ask Haonan)

--Compared these results to the original version run on the same frames.  These
  images are saved in
  test-run-data/codetection-17feb15-selected-frames-cone/detection-images*.png.
  My version seems to work much better--the images from original version seem
  random.  The output of libDAI Belief Propagation for this run is 1.2092.  I'm
  assuming that since we're minimizing here, lower is better...

--I think what I've got working here is probably good enough to use--now need to
  work on getting it to work when there are no objects in the image (i.e., dummy
  state, classes, etc...)

----------------git commit 1f0e61c----------------

27 Feb 15

--Started working with MCG-PreTrained (from arbelaez2014multiscale) as a
  replacement for EdgeBoxes as my proposal mechanism.  Trying to understand the
  input and output of this code in order to integrate it into my framework.


----------------git commit 10b5261..e7ce79d----------------

2 Mar 15

--First tests using MCG look promising.  Getting good results on test
  frames--top-scoring box is surrounding object pretty well.  

--Need to create new penalty term computation.  Thinking about using an
  exponential term to penalize boxes out of bounds, like e^(-xover)*e^(-yover).
  Also need to compute term for boxes that end up projected back behind camera.

--Since each frame takes ~5s to get boxes generated, might want to think about
  doing all box generation in a single parfor loop before doing anything else.

----------------git commit bb86297..fda6e12---------------

3 Mar 15

--Benchmarking MCGboxes (new proposal generation method) and got ~330s run time
  to find all boxes in a 204-frame video (run in parallel on 5 processors).
  That is just for proposal generation--no processing of similarity or anything
  else.

--Re-ran original scott_proposals_similarity.m to compare and ran into some
  issues.  Ended up having to put an 'if (num_nboxes > 0)' around entire binary
  score part.  This shouldn't be a problem in my new method, since the bad boxes
  will just have reduced scores, instead of being eliminated like they were
  here.  Run time for this whole proposals & similarity computation was ~230s.

--Rewriting scott_proposals_similarity2.m to use MCGboxes and different
  scoring.  Have penalty terms for out of bounds and behind camera written.
  Need to compute world width and use in binary score computations.  Need to
  remove unary score piece based on intra-frame distance (marked in comments).
  Also need to fix structure of how the calls to similarity scoring work.  I
  think between-frames distance measure still needs to be in separate loop--but
  can I move the current 1:T loop into the first parfor loop?

----------------git commit c661acb---------------

4 Mar 15

--Think I have scott_proposals_similarity2.m working with MCG boxes and penalty
  terms for unary scores.  Currently it uses Haonan's original adjacent-frame
  similarity for binary scores.  Need to fix that to have binary scores between
  all boxes in all frames.  Look at using pdist2 to do the distance computations
  between box locations--should be able to just vectorize the x,y locations of
  the boxes.  Also do width comparision (simple as abs(b1-b2)).  Then use that
  as a threshold to decide which to send to the visual similarity measure.

----------------git commit b6abd8d---------------

5 Mar 15

--Got good results from MCGboxes unary scores.  System picked up both cone
  and chair in all frames of test video that I expected it to do so.  Now need
  to work on binary scores (world distance & world width & similarity between
  boxes close in world location).

--Have distance (d_score) and width (w_score) measures working.  I think I have
  the visual similarity (s_score) worked out, currently testing.  Also need to
  add a temporal coherency term (based on Optical Flow) to boxes in adjacent
  frames.  Dan says there is Scheme code to do this--need to work with him to
  figure out how to use this.

----------------git commit ff66f12---------------

6 Mar 15

--Original s_score appears to be working, but it takes a LONG time (454s for 3
  frames and top_k = 100)

--Rewrote s_score to be more efficient in computation.  I realized that I could be
  computing the same histogram multiple times the way the loop was originally
  written. Changed to instead to compute and store ALL histograms in the first parfor
  loop (when boxes are found and scored) and then just reference the correct
  histogram when computing the s_score.  Run time now ~20-25s for same data.

--Also discovered that the order of the boxes coming out is not deterministic.
  My s_score had columns in different order from different runs on the same data.

----------------git commit af89418---------------

9 Mar 15

--Tried running a MATLAB test on the full video.  Had to manually kill all
  MATLAB processes after about an hour--froze my system.  Looking at htop, it
  appeared that the parallel workers were eating up all available RAM and
  hanging the system--processors were fine.

--After restarting MATLAB, had same problem as prior with inability to check out
  license for Parallel Computing Toolbox.  No known resolution to this other
  than to wait a while and try again when licenses might be available.

--Working again.  Cleaned up code a bit and working on getting binary score
  output into the format [f1,b1,f2,b2,g] for every link that exists in the giant
  binary scores matrix.

--Trying to run a test of the full code against the full video overnight.

----------------git commit 0fe710e..c487890---------------

10 Mar 15

--Overnight test run completed on full video with top_k=10 in ~409s.  Still need
  to test with larger top_k.

--Have MATLAB finished with output boxes_w_fscore in the same format as original
  (with added world x,y,w values in further columns, and second output gscore as
  a 2-D matrix with rows [f1,b1,f2,b2,g].  All scores coming from MATLAB are for
  actual boxes--will be doing dummy box stuff in C.  Now need to work on changing
  Scheme->C and C interfaces.

--Started on Scheme work.  Have get-matlab-proposals-similarity that gets the
  matlab outputs boxes_w_fscore and gscore back into Scheme.  Also have
  load-data that loads 4 different groups of data into scheme variables.

----------------git commit 978e470---------------

11 Mar 15

--Overnight test of (load-data) resulted in a crash when I came in this
  morning.  Tried each of the four data set loads individually and they all
  worked.  Running test-data-{small,medium,large} (4,40,100 frames) ran in ~7
  minutes.  Running test-data-full by itself worked, but putting all the output
  to my scheme buffer pretty much locked things up.  Had to kill the buffer and
  try again.  Ran (load-data) again (to suppress buffer output) and it ran in
  ~14 minutes.  Seems to work OK.

--Rewrote get-matlab-proposals-similarity to be more general with inputs of
  top-k, box-size, frames poses, and normalized coefficients.  Then wrote
  helper/wrapper functions -by-frame and -full-video to deal with those cases
  (and normalize the coefficients).

--Added function run-codetection-with-proposals-similarity to run codetection
  with the output from the get-matlab-proposals-similarity functions mentioned
  above.  Believe I have the structure set up correctly.

--Next starting work on changing the C++ code for bp_object_inference in
  inference.cpp.  Have to change the structure of the g function and how its
  factors are added.  Also need to add stuff here for dummy state.  Still
  working on this--looking into how OpenGM works and how to add functions,
  specifically trying a sparse function (SparseMarray) for my g function.

----------------git commit 6513816---------------

12 Mar 15

--Continuing to work on OpenGM code in C++.  Discovered that the SparseMarray
  template referenced in the documentation has been replaced by SparseFunction
  with different variable types in the template.  Trying to figure out how to
  use these.  Have an example that compiles, but with compiler warnings (see
  inference-google-groups-post.cpp and
  compiler-warnings-google-groups-post.txt).  Posted a question to the OpenGM
  Google Group, but not very hopeful for an answer (low traffic).  Still
  searching for other documentation or examples.  Found something from Jason
  Corso's course at UMich at https://github.com/wecacuee/opengmdemo, but it
  doesn't do anything with SparseFunction

--Abandoning SparseFunction now and instead trying to do my binary scores as an
  ExplicitFunction.  Not as memory efficient, but should still work.  Saved
  previous work with SparseFunction in
  inference-with-sparsefunction-questionable.cpp. 

--Working on adding binary score functions in a loop through gscore rows.  Once
  this is finished, should be able to run a full test and see if dummy boxes get
  selected in frames with bad boxes.  Also still need to add temporal coherence
  via optical flow as fourth element to binary score (second g-like structure
  where I add its binary value to the binary value coming from g).

----------------git commit 9efaaf3---------------

13 Mar 15

--Got a reply to my google groups post from one of the code authors(Jorg
  Kappes).  He said just to comment out the offending line and not worry about
  it.  Not sure it matters much, since I think it will just be easier to use
  Explicit Function instead of Sparse Function.

--Finished first draft of new OpenGM code.  Throwing a runtime error at this
  point.  Will investigate more.

----------------git commit 7b5a615---------------

14 Mar 15

--Worked on debugging run-codetection-with-proposals-similarity.  Believe I have
  it working correctly now.  It returns the correct list of box numbers for
  test-data-small. 

--Having problems when run against other data sets.  For test-data-medium, it
  never picks a dummy box when it should.  Strangely, changing the values for
  dummy-f and dummy-g seem to have no effect on this.  Need to troubleshoot
  more.

--Found in troubleshooting that when I print out my gg matrices, the columns
  that should have -1.0 for dummy_g are showing up with 0.0 instead.  This could
  be why dummy boxes are not getting selected.  Need to figure out why this is
  happening. 

----------------git commit ccaed7d---------------

16 Mar 15

--Fixed problem with dummy_g not in gg matrices.  Doing different initialization
  on gg fixed the problem.

--Now having problem that running run-codetection-with-proposals-similarity on
  full video causes the following error:
       terminate called after throwing an instance of 'opengm::RuntimeError'
         what():  OpenGM error: libdai Error: Quantity not normalizableQuantity
	   not normalizable 	 
  Not sure why this error occurs.  Initial googling leads me to believe it might
  have to do with an approximate marginal of the factor graph being zero--going
  to try converting all score values to log domain to see if this helps.

--Also seeing the problem that when I run against test-data-medium or
  test-data-large, I tend to get all dummy boxes when I have dummy_g = 1.0.
  Might need to try other combinations of dummy_g and dummy_f.

--Committing before making change to have scores in log domain.

----------------git commit 3d85740---------------

16 Mar 15 (2)

--Still having same error after changing to log space, but now with
  test-data-medium.  Error is happening in the call to bp.infer().  Tried
  changing around the data so that there are no 0's in the gg matrices (i.e.,
  dummy_f=0.99 instead of 1.0).  Still have inf in the gg matrices though--could
  that be the problem?  Is the problem even in the gg matrices?  

--Removed all zeros and infinities from the ff and gg matrices by using
  some_small_number (currently set to 1e-10).  Instead of infinity I use
  -log(some_small_number) and instead of 0 I use -log(1-some_small_number).  Got
  it to run on test-data-medium without crashing (although the result was all
  dummy boxes--that can probably be fixed by adjusting the dummy_f and dummy_g
  parameters).  Next testing on test-data-large and test-data-full.

--Ran without crashing on test-data-large, but still crashed on test-data-full
  on the bp.infer() call.  Not sure how to debug that.  Is the graphical model
  just too damn big?  Do I need to use a different inference algorithm--maybe
  the default BP instead of libDAI BP?

----------------git commit 4f10dce---------------

17 Mar 15

--Saved test data to test-data-*.sc in /home/sbroniko/codetection/testing-data
  to speed up loading of data.  Defined reading of each in codetection-test.sc.

--Still having crash on test-data-full when all ff and gg values set to
  -log(0.5).  Trying a restructuring of the code (suggested by Haonan) so that I 
  can declare a new gg function in every iteration of the loop--basically
  amounts to first finding the frame breaks in the g score matrix and then using
  that index to jump through the g score matrix.  Saving a copy of the old
  inference.cpp to inference-before-restructure.cpp and committing.

----------------git commit 80e7d34---------------

17 Mar 15 (2)

--Tried the restructure that Haonan suggested and still had the same crash when
  running against the full video. 

--Added (~line 130 of inference.cpp) frame_distance_threshold to limit the
  difference between frame numbers when adding a gg function.  The thought was
  to just reduce the number of gg functions/factors added, since it ran fine on
  smaller data sets.  This appears to work.  Got program to run against
  test-data-full with frame_distance_threshold = {50,60}, but it crashed with it
  = {75,100}.  Might this be related to the length of the video?  Probably
  should make this number a parameter to the function.  Also need to switch
  function back to using actual read-in data, rather than testing values.

----------------git commit 60b1a93---------------

18 Mar 15 

--Email conversation with Jeff helped me clarify some of my thoughts as to why I
  had to put the frame_distance_threshold hack into my code and why that hack is
  a bad idea.  Believe I'm hitting some sort of limit on the number of
  variables/labels/factors that can be added, and I think the problem is coming
  from libDAI.  Started conversations on both the OpenGM and libDAI Google
  Groups boards asking whether this limit exists.  If I know the limit, then I
  can try to find a way to stay within it.  Have some other ideas (switch to
  OpenGM BP, use a numerical or top-n limit on the number of binary scores taken
  from MATLAB, etc...) in email to Jeff.

--First trying the switch from libDAI BP to OpenGM BP to see if a crash still
  happens on the full dataset.  With frame_distance_threshold still in place,
  OpenGM BP ran successfully on test-data-full.  Was significantly slower (~5s
  vs. ~0.1s), but still pretty fast.  Ran with frame_distance_threshold removed,
  and it ran without crashing--took ~15s, but that's still not bad.  Committing
  now before I start changing back to using real data (probably in log space).

----------------git commit 33314c5---------------

18 Mar 15

--Wrote matlab-data-to-files in codetection-test.sc to run on a
  server and put matlab output into frame-data.sc files in each directory of
  training data (plan4 thru plan9, since plan0 thru plan3 had no objects in
  video).  Left running overnight on jalitusteabe.

--Next need to revise inference.cpp to use real data (try log space first).
  Then need to write a Scheme wrapper that loads the matlab data from file, does
  the codetection, and then visualizes and saves the result.  Could also try a
  system call that takes the saved frames and puts it into a video and then
  deletes the images.  ***NEED TO APPEND EMPTY LIST TO END OF BOXES, XY BEFORE
  THIS WILL WORK***

----------------git commit a5da3af---------------

19 Mar 15

--Ran into a problem with the overnight run of matlab-data-to-files.  For
  certain runs (only found 1 so far), I get an error num-frames != frame-poses
  from within align-frames-with-poses.  This appears to be happening because
  there are frame times that are later than the latest imu log time.  Need to
  figure out a way to overcome this--perhaps just copy the last pose for all the
  rest of the frames.

--Fix by copying last pose for rest of frames seems to work,
  matlab-data-to-files no longer errors out.  Will have to see if the
  visualization generated from this data is any good
  (.../training/plan4/2014-11-15-22:47:10/frame-data.sc) 

--Found a problem in the data--some of the camera_front.txt files were missing
  one of the footer lines (Camera logging stopped), which was causing
  align-frames-with-poses to error out.  That line was the last one written from
  the rover, so the connection must have been closed before those messages made
  it into the socket on the rover end.  I found the files with the line missing
  and added the line so that the code would work correctly.  Also relaxed the
  condition in align-frames-with-poses so that now as long as there are not more
  frames than frame times, it will run and just ignore the extra frame times at
  the end of the list.

--Changed inference.cpp back to using real data in log space.  Built a
  visualization wrapper around it so that we can visualize the effects of
  changing the parameters (dummy_{f,g}, alpha, beta, gamma, etc.).  Right now
  just playing with dummy_f and dummy_g values to see how that affects when a
  dummy box is selected.  Initial testing on short video segment gave fairly
  good results when dummy_f = dummy_g = 0.6.  Running test on full test video
  overnight to see how that does--timing it with (system "date").  Also need to run on
  other videos (should be able to once the job running on jalitusteabe saves all
  the matlab data).

----------------git commit a137c47---------------

20 Mar 15

--Overnight run completed in ~28 minutes (with precomputed Matlab data, which
  took ~10 minutes itself) for a single 204-frame video.  Results look somewhat
  reasonable--it usually got sensible boxes around the objects it was supposed
  to detect, but there were quite a few spurious boxes chosen (~40% of the time)
  when it should have been choosing the dummy box.  It did choose the dummy box
  ~60% of the time when it should have chosen it, and it never chose the dummy
  box when it should have been choosing an object box.  This run was done with
  the parameters set at dummy-f = dummy-g = 0.6.  Not sure how to change these
  parameters would improve results, or if that is even worth the effort.  It
  might be more beneficial to set up a large run on one of the servers that will
  processs many of the videos and save all the output to a .sc file.  

--Jaliusteabe job to precompute matlab data is still running--plan{4,5,6,7,8} are
  complete, currently working on plan9 (last one).

--Rebuilt visualize-results to take only a path and dummy-f and dummy-g values
  as arguments.  It saves the full results or
  run-codetection-with-proposals-similarity to results-dummy-f-dummy-g.sc, and
  puts image output into the subdirectory /images-dummy-f-dummy-g/.  

--Using this with Dan's synchronous-run-commands-in-parallel-with-queueing from
  toollib-multi-process.sc to run this against all the videos we currently have
  on the 2G servers over the weekend.  New procedure is (run-full-results
  dummy-f dummy-g output-directory).  I don't think this job will end up
  overtaking the matlab precompute job that's currently running on jalitusteabe
  (which is providing the data that run-codetection-with-proposals-similarity
  takes as input), but if it does and the job doesn't finish, we should still
  have usable results from the videos that it does process.

--Got run-full-results going after some debugging and figuring out some issues
  with my .bashrc file.  Job appears to be running and producing output.

--Will check tomorrow on the results of this run.  Will also be starting on
  preparing my prelim document this weekend.

----------------git commit eaf3e6f---------------

21 Mar 15

--Job from last night looks like it completed successfully, with a runtime of
  about 4 hours.  The data from plan9 didn't get processed completely because
  the matlab preprocessing job wasn't complete by the time the results job got
  to those directories.  Starting a new job just to run on plan9, and added a
  line to run-full-results to rsync all the data back to jalitusteabe when it's
  complete.  Will check on this job tomorrow--runtime should be less than an
  hour (only processing 25 runs, while the previous 4 hour job processed 125
  runs). 

----------------git commit 94fab98---------------

23 Mar 15

--Job for processing all videos in training/plan{4,5,6,7,8,9} completed and
  results rsynced back to jalitusteabe.  Will sync these results back to
  seykhl.  Want to think about setting up a job to process the auto-drive and
  generation runs as well.

--Go through processed videos and mark each in 1 of 3 states: out of scope (no
  objects in field of view in video), at least one target object detected
  (good), no target objects detected even though they were in field of view
  (bad).

--Think about adding a constraint on the boxes that they get penalized (or
  rejected) if their height or width is > half the image size--will probably
  have to implement that in MATLAB.

--Implemented size constraint for boxes in MATLAB.  Based the constraint on a
  world width_threshold--boxes that are > than this (currently arbitrarily set
  at 1.5m) have a penalty term of exp(width_threshold-wwidth).  Will have to see
  if it improves results.  Don't think I need to do anything for height, or
  anything based solely on box size within image, since very close objects WILL
  take up a large portion of the field of view.

--Built results-end-to-end that runs matlab results and then c results.  Had to
  make a few changes to scott_proposals_similarity2.m to stop using parallel
  toolbox--had license problems again.  Now just doing images in simple for
  loop, but should still speed up b/c running on servers in parallel.  Running
  this overnight

--Still need to finish going through videos processed over the weekend.

----------------git commit ae69da9---------------

24 Mar 15

--Run on generation dataset complete at 1154. Started at 1828 last night, so
  overall runtime was ~17.5 hours.  Rsync'd data from jalitusteabe back to
  seykhl.  Based on advice from Jeff, going to do the run on the auto-drive
  dataset on the workstations instead of the 2G servers.

--Started run on auto-drive dataset at 1350.  Using 7 workstations (all but
  mine, Dan's, Jeff's, Haonan's), 42 cores total.

--Had to change matlab code to replace pdist function because of issues with
  licenses for the Statistics Toolbox.  The norm function should be a direct
  replacement for computing Euclidean distance.  Testing in matlab seems to
  work. Runtime seems to be about the same.  Restarted job at 1430.

--Noticed problem with done-22, done-28, done-27.  All ran into the error
  'error: num-frames > cam-timing', which should be in align-frames-with-poses.
  Looks like it's the same problem I noticed before, with the 'Camera logging
  stopped' footer line missing in camera_front.txt.  Need to find all instances
  of that and fix, then restart job.

--Fixed bad camera_front.txt files and restarted job at 1544.  Scripts to check
  for footer line are saved below.

--Finished reviewing output of training dataset run.  Out of 150 videos: 82 out
  of scope, 21 bad, 47 good.  For in scope videos, percent good 69.1 (47/68),
  percent bad 30.9 (21/68).

--Need to review output from generation dataset run.

----------------git commit 13274c2---------------

25 Mar 15

--Run on auto-drive dataset complete at 0935 (120 videos).  Started at 1544
  yesterday, so overall runtime was ~17h 51m--nearly the same runtime as the
  generation dataset, but with 20% more videos.  So 7 workstations > 4 2G
  servers. 

--Based on initial observations in output from generation dataset, the penalty
  term on world width didn't seem to have much of an effect.  Decreased the
  width_threshold from 1.5m to 1.0m.  Also added a penalty term to the w_score
  component of the g_score (binary score).  **THIS HAS NOT BEEN TESTED YET**

--Also added a penalty term unary score to penalize boxes that have edges within
  10 pixels (arbitrarily set) of image boundary.

--Re-running job on generation dataset with new changes to matlab code.  Have
  data saving with -new appended to the end of file/dir names.  Will compare
  against first run to see if improvement (although since results/scores have
  some degree of randomness, can't be sure a direct comparison will be all that
  informative).  Basically looking to see if it gets rid of the spurious large
  boxes. Started job at 1445.

----------------git commit c6d16a0---------------

26 Mar 15

--Re-run of generation finished at 2155 last night.  Total runtime was 7h 10m.
  HUGE improvement over running on 2g servers.  Wonder if the changes to the
  matlab code had an effect.

--Going to re-run the auto-drive dataset with new matlab code.  Should probably
  re-run the training dataset as well.  For auto-drive dataset, using 6
  workstations plus the 4 2G servers.  Started job at 0933.  For some reason,
  the rsync of the data to all the machines is taking a LONG time, >2 hours...
  Turned out there was something funky going on with jalitusteabe, so I just
  removed that machine from the list of servers.  Now 6 workstations plus 3 2G
  servers, 60 cores total (51 jobs started in first batch since the job-starting
  code rounds down on the number of available processors).

--Had to restart the above at 1302 because of an error in
  get-matlab-data-auto-drive.  I forgot to append -new on the end of
  frame-data.sc, so it would have overwritten previous data if it had run, and
  also it would have errored when visualize-data tried to find
  frame-data-new.sc.  Also removed Abdullah's workstation from the pool.  Now
  using 5 workstations and 3 2G servers (54 cores).

--Initial look at one generation run with both methods one-by-one showed that
  the new method eliminated almost all of the spurious boxes, but it also
  eliminated many valid boxes as well.  Might need to relax some of the
  constraints there--I think the pixel limitation is still OK, but might need
  width threshold back to 1.5m, and maybe eliminate the width penalty in the
  binary score.  However, looking at the next run in the series didn't show that
  problem.  Maybe a way to evaluate would be to have the matching images
  stitched side-by-side into a single image.

--Finished analyzing first run on generation dataset.  Stats: total videos =
  100, out of scope = 0, good = 80, bad = 20.

----------------git commit 5984fe6---------------

27 Mar 15

--Re-run of auto-drive still going at 1055.  One last job running on
  cuddwybodaeth, a 536-frame video that's been running in the C (OpenGM) portion
  for >13 hours.  Run finished at 1149 (~26 hour runtime with restarts).

--Going to restart another run (suffix -new2) on the generation dataset.  The
  idea is to relax some constraints in matlab, specifically the width penalty in
  the binary score.  Then going to stitch those frames together with the 1x2
  already built so 3 frames will be in a row for comparison.  Going to add
  buddhi, rongovosai, jalitusteabe back to the server pool.

--Started new run at 1111.  Joined images (3-way w/ Original|25Mar run|27Mar
  run) will be in /joined2 when done.  Had to remove wywiad and jalitusteabe
  from the server pool because the rsync kept getting hung up.  Restarted at
  1410.

----------------git commit 55a56f5---------------

28 Mar 15

--Re-run of generation with binary width penalty removed didn't seem to fix the
  problem, based on early observations.  Redoing another run on generation with
  width_threshold set back to 1.5m, so that the only difference in the matlab
  code is the addition of the pixel threshold.  Results will be in -new3 and joined3.


----------------git commit 3f1925e---------------

30 Mar 15

--Saturday's re-run of generation dataset with threshold back to 1.5m still
  didn't produce the results I expected.  On the first run of plan0, around
  frame 60, the cone comes clearly into view and stays there for about 15
  frames.  The original run from 23 March finds the cone in these frames, but
  all later runs do not.  Currently looking at the diff between the current
  matlab code and the matlab code from that date to figure out why.  The only
  substantive changes I could find was the change from pdist to norm for
  computing wwidth, and the conditions to apply the large penalty term for boxes
  too close to the boundary.  I wrote a quick test that confirmed that the
  change from pdist to norm was not a problem--computed wwidth both ways, and
  looked at the sum of the absolute difference between the ways on 15 frames
  with 10 proposals each (150 measurements)-->the sum was 6e-15, which shows
  that there two functions give the same answers.  Next need to look at the
  penalty for boxes too close to image boundary-->perhaps a penalty term
  separate from the locflag penalty (behind camera).

--Trying to do separate flags for bad_r, bad_l, bad_t, bad_b, bad_w, bad_h and
  then only impose the penalty if 2 or more flags are set.

--Started new run with run-my-shit set at:
	  (define (run-my-shit)
	   (results-end-to-end "/aux/sbroniko/vader-rover/logs/MSEE1-dataset/generation/" 10 64 1 1 1 0 0.6 0.6 "/aux/sbroniko/vader-rover/logs/MSEE1-dataset/results20150330")
 	   (join-all-images "/aux/sbroniko/vader-rover/logs/MSEE1-dataset/generation" "joined" "images-0.6-0.6-new4"))
  Output will be in joined4.  Run started at 1715.

----------------git commit 02ce3f4---------------

31 Mar 15

--Last night's run errored out because I forgot to comment out the parfor stuff
  in matlab and I couldn't get enough licenses to run the parallel toolbox.
  Restarting job at 0941.

--Job appeared to run successfully.  At 1740, it is currently working through
  the join-all-images portion.

----------------git commit 24995ed---------------

1 Apr 15

--Run completed at 1856 yesterday.  Total run time of 9h 15m.  Looking at the
  results shows general improvement--most of the spurious boxes are suppressed,
  and while some objects that were detected in the original version are missed
  in the new one, some objects that were missed in the original version are now
  detected.  I think that the current version is good enough to move forward
  with. 

----------------git commit 17c4cce---------------

2 Apr 15

--Starting a run on plans 4,7,8 of training dataset (the only ones with objects
  in the videos) so that we can have at least 3 floorplans where we have 25 runs
  per plan, instead of the 10 per plan in generation and autodrive.

--Sent prelim abstract and check sheet to Matt Golden.

--Run completed in ~4 hours on 75 training videos.  Run errored out at join
  stage because of misconfiguration--fixed and rerunning the join.

--Started looking at plots of box xy locations on floorplan.  Have a function to
  plot the boxes from a single run, need to write another function that plots
  the boxes from all runs on a single floorplan.

--Talking with Dan, decided to do a small re-run over 30 runs (i.e., < # of
  processors available) to look a the effect of changing the values for dummy-f
  and dummy-g.  Going to start with dummy-f first.  Since original value was at
  0.6, going to try 0.3 first.  Started at 1547.  This run is still going as of
  1730.

--Looking at plot of all runs in a floor plan, can clearly see clusters near
  where we know the objects are.  Need to create a sort of mesh grid over the
  area and then at each point in the mesh, do a gaussian kernel (of what
  variance?) for distance from point to box and then multiply that by the box's
  f-score.  This should give us a function with peaks where the objects are.
  Should also be able to determine the number of peaks (i.e., objects) from
  analyzing the sentences.  Then we can just pick this number of maxima from the
  function to get our object points.

----------------git commit c859642---------------

3 Apr 15

--First impressions of looking at results of small re-run in plan0, plan1, plan2
  of generation dataset shows improved results--more objects detected, but also
  more spurious detections.  However, looking at the scatter plot of detections
  in the xy plane shows a lot more noise--not sure if the correct peaks/clusters
  will be found.  Need to finish writing the function to score the points in
  this plot for peaks.  Also need to modify the current code to export the
  winning box's fscore, as well as testing code to grab these fscores from the
  matlab data.

--Modified run-codetection-with-proposals-similarity to have the fscores of the
  selected boxes as the fourth list in its output.  Shouldn't have any effect on
  previously written code that only uses the xy and pixel locations (lists 2 and
  3) of the boxes.  ANY FUTURE CODE THAT USES SCORES WILL HAVE TO HAVE DATA THAT
  WAS RE-RUN WITH THIS NEW CODE.

--Wrote ensure-scores to put winning box fscores onto results that don't have
  it.

--Modified plot-objects-from-floorplan to have boundaries and grid lines.  Base
  version plots all runs in single color, -multicolor plots each run as a
  different color.

--Built function to pass data back to matlab and currently working on
  score_detections.m in matlab to compute the score for each point in a grid.
  Need to finish testing this.

----------------git commit 0cd6d12---------------

6 Apr 15

--Got plots done on Saturday (in /tmp/sbroniko and an email to Jeff) and they
  look pretty good.  Runtime for doing the scoring function with fscore=0.6 and
  gscore=0.6 was roughly 1 hour.  Still need to do the max-finding on the
  scoring function to find the exact locations of the peaks IOT be able to
  compare them with the ground truth locations.

--Also tried a scoring function run with fscore=0.3.  That run took about 2.25
  hours, which makes sense since there were about 2.25 more detections in that
  run than in the previous run with fscore=0.6.  This is currently saved as
  scores2 in 20150406.mat.  Need to do the plots of this function.

--Need to write up plotting and max finding as functions and then wrap them in
  Scheme calls to Matlab.

18 May 15

--Cleanup commit to account for all AMT stuff in codetection-test.sc.

----------------git commit 9fa4041---------------

28 May 15

--Starting back on codetection stuff.  Found FastPeakFind function from
  http://www.mathworks.com/matlabcentral/fileexchange/37388-fast-2d-peak-finder
  that seems to work pretty well on finding the peaks in my scoring function.
  Using its sub-pixel resolution mode (case 2) does pretty well with finding the
  centroids of peaks.  Probably need to look at making my own version of this
  using the regionprops command.

--Talked with Dan about how to use these locations of my peaks as floorplans.  I
  hadn't realized that the way our acquisition system learns is by using the
  labels (and not just the locations) of the objects (so that 2 objects of the
  same type already start with the same label).  So I can't just give the
  acquisition system xy coordinates, I have to have the objects labeled (with
  the SAME label for objects of the same class).  Dan and I came up with using
  the boxed images that were located in the first step of codetection to build a
  set of images at every found peak location (within some distance of the peak)
  and then run some sort of similarity measure (reuse the old similarity
  measure, or something else?) to determine whether two objects at different
  locations were of the same class.  Do a full every-to-every image similarity
  measure between every pair of points (IN EVERY FLOORPLAN AS WELL) to determine
  an average similarity, and then use a threshold to decide if similar enough to
  be the same class.  Can then label these as OBJECT1...OBJECTN and use as
  floorplan. 

----------------git commit a2c3bae---------------

29 May 15

--Built my own version of FastPeakFind in find_score_peaks.m.  Seems to work
  correctly.  Right now it returns pixel locations (w/sub-pixel accuracy) for
  the centroids of peaks.  Thinking about integrating it into score_detections.m
  so that it can return actual xy locations for peaks.

--Have find_objects.m working.  Returns number of objects found and their xy
  locations. HARDCODED the noise threshold at 0.3--might need to relook this.

----------------git commit 41c0d15---------------

1 Jun 15

--Working on make-test-file-new to add run numbers, frame numbers and pixel
  coordinates to the detection_data structure in MATLAB.

----------------git commit da4ef42---------------

2 Jun 15

--Have find_object_error.m working; this finds the error between detections and
  ground truth for a single floor plan.  Need to automate the extraction of
  ground truth from the *dataset.sc file and also wrap to work over all floor
  plans (probably at the same time as I wrap everything else for all floor
  plans).

--Have get-detection-data-for-floorplan working in codetection-test.sc.  This
  produces the matlab detection_data file as well as saving detection images in
  a separate directory.  Need to look at possibly doing something with the
  clustering to eliminate detection images that are too far away from cluster
  centers (i.e., bad detections).  Also need to look at (ensure-scores) to
  restructure it for faster running--list-refs appear to be slowing it down.

----------------git commit 24a4df6---------------

3 Jun 15

--Wrote (get-ground-truth-from-dataset-file) function to pull ground truth data
  from *dataset.sc files.  Gives list of lists of vectors of the ground truth
  object locations for whole dataset (all floor plans).

--Rewrote (ensure-scores) to skip the time-consuming part if the results file is
  of the new type (4 lists).  Looking more into the time of it, found that the
  slowest part is the reading of the frame data file.  Not sure that I can do
  anything about this, since these are HUGE (~90MB) text files--it's just going
  to take a bit to read them.  But with the new type of results file including
  the winning fscores, this should be avoided on future runs.

--Working on cluster_detections_by_object.m to do clustering.  It seems to
  cluster correctly (NEED TO CHECK THIS MORE).  Still need to implement a
  threshold to reject (mark as invalid) detections that are too far away from a
  cluster center.  This SHOULD get rid of most of the detections that are just
  of open floor and other garbage.  Then I need to write a function (scheme or
  matlab?) that sorts the saved images by cluster--put in separate directories,
  or something else??

----------------git commit 2ae6453---------------

4 Jun 15

--Added thresholding to cluster_detections_by_object.m as input parameter.

--Have sort_by_cluster.m working to sort images by their nearest cluster center
  into tmpN directories. Seems to work pretty well with thresholds of 0.5m and
  0.4m (used in cluster_detections_by_object) to reject non-cluster detections.
  The lower threshold rejects some detections that look like they should be
  good, but there are still a good number in each directory.  Next step is to do
  comparison between each of the tmpN directories to find which items are alike
  and then move into final directories with all like images together.

--Working on sort_clusters_single_floorplan.m.  Seems to work correctly so far.
  Need to figure out a way to determine which temp labels are related to which.
  Right now doing a comparison of each image in tmpM to each image in tmpN.
  Putting those similarity values into a MxN matrix and then playing around with
  how to reduce that matrix to a single score (max of max, max of mean, mean of
  max).  Mean of max produced an interesting result--labels that should have
  matched had an element in their row/col that was HIGHER than their diagonal
  (self-similarity) measure.  I should be able to make this work: if a diagonal
  value is the largest value in the row/col, declare it a unique label; else
  find the item larger than diagonal value and use that label.  Something like
  that--will have to be careful how I write that code (what if the 1,1 element
  is not the largest in its row?)

----------------git commit cc14e25---------------

5 Jun 15

--Have sort_clusters_single_floorplan.m working.  It gives unique labels to
  clusters, and the same label to like clusters (in my test floorplan, 3
  clusters were of a chair, and all 3 got the same label.  Did this by doing an
  every-to-every comparison of the phow features of images in M clusters, putting
  the similarity scores into a matrix, then taking the max of the mean of both
  the rows and columns (two values).  Then put these values into an MxM matrix
  of average similarity scores.  The diagonal elements of this represent
  self-similarity.  Then compare these diagonal elements to the max of its row
  and column.  If the diagonal is the max, then it is a unique label.  If it is
  not, then it gets the unique label from the value that is the max.

--Next need to work on moving images to unique label folders--should be similar
  to sort_by_cluster.m.  Going to do this at the end of
  sort_clusters_single_floorplan.m.  Will also save output (xy_with_label) to
  .mat file for later use.

----------------git commit 5f37a4e---------------

8 Jun 15

--Finished sort_clusters_single_floorplan.m.  It now saves the xy_with_label in
  the file object_xy_with_label.mat (in img_dir) and sorts the like images into
  fplabel* directories (under img_dir).

--Next need to write scheme wrapper around all the matlab stuff for a single
  floorplan.  Also need to write some code to run an entire dataset (all
  floorplans) which will eventually produce the *dataset.sc map file (same one
  that ground-truth comes from).

--Start thinking about OTHER BIG STEPS (below) to add to this--Jeff doesn't
  think what I've got so far is a big enough step to publish.

----------------git commit f22ef4c---------------

9 Jun 15

--Working on (detect-sort-label-objects-single-floorplan), the scheme wrapper to
  all the matlab stuff.  Keep hitting a 'Error: Unexpected MATLAB operator.'
  when calling sort_by_cluster and sort_clusters_single_floorplan.  Need to
  figure out what I'm doing wrong here.

----------------git commit 92fb758---------------

10 Jun 15

--Figured out error was from not having single quotes around a string to be
  passed into a matlab function.  Also added code to write the xy_with_label
  variable out as a scheme vector of vectors.

--Created a backup copy of the MSEE1 dataset in
  /aux/sbroniko/backup-MSEE1-dataset-as-of-10-jun-15/ on buddhi, seulki, and
  rongovosai. 

----------------git commit e8ce1e7---------------

11 Jun 15

--Finished cleanup of MSEE1-dataset directory.  Moved some stuff (judging
  worksheets, minimal-pairs, videos, etc.) to MSEE1-dataset-supplemental-stuff.
  MSEE1-dataset dir is now just auto-drive (comprehension), generation, and
  testing (acquisition).  Also cleaned out all old testing results from each of
  the three subdirs.  End result is that dataset is now 8.3GB (down from
  182GB).  Should significantly speed up the rsync of data for job runs.  Just
  have to remember to keep stuff cleaned up. *THOUGHT*: Do I want to just delete
  the frame-data.sc file after I use it, since it seems to eat up a lot of
  space?

--Started working on new functions to call for doing codetection and
  sorting/labeling objects in single floorplan.  Main codetection function is
  (get-codetection-results-training-or-generation), which is a modified version
  of (results-end-to-end).  Also have (get-object-detections-all-floorplans)
  which, like (get-codetection-results...), spreads the jobs over multiple
  machines/cores.  The wrapper for these two is
  (codetect-sort-templabel-training-or-generation).  I think this is
  correct--will test tomorrow once servers are back online.

--After this, then need to write code that takes the output from all floorplans
  and does comparison to put common labels on each, and also produce floorplan
  file in scheme format.  Probably should make this as parallel as possible
  (most likely matlab parfor), since it will probably be running only on a
  single machine--would be smart to run this on one of the servers, since there
  are more cores there.

----------------git commit eaa3e62---------------

12 Jun 15

--Added saving the phow histograms (fvcell matlab variable) to
  sort_clusters_single_floorplan.m.  After test run, need to look at the size of
  this file.  Could save some time when doing the all-floorplans
  comparison-->can read feature vectors in from file, but will still need to do
  the each-to-each comparisons (PARALLELIZED????)

--Started test run using 3 3g servers (verstand, arivu, perisikan) at 1428.
  Restarted job using only verstand and arivu at 1453.  This left me just shy of
  enough processors  to finish in one iteration.  Had to do it because because
  perisikan kept behaving oddly and killing my jobs.

----------------git commit 5305001---------------

15 Jun 15

--Found typo/bug in (codetect-sort-templabel-training-or-generation).  Corrected
  it and tried re-running on 7 3G/4G servers (all but upplysingaoflun).  Also
  changed all matlab-cpus-per-job values to 4 to account for under-the-hood
  matlab parallelism.

--Found another bug in (get-matlab-data-training-or-generation-improved) where I
  was trying to write the output file to a dir that hadn't yet been created.
  Corrected this and tried re-running same as above.  Run started at 1524.  

--Have another bug, similar error to before.  Need to get it figured out before
  running again.

----------------git commit 8ccb941---------------

16 Jun 15

--Figured out and fixed bugs in (codetect-sort...).  In
  (get-matlab-data-training-or-generation-improved), was missing / between dir
  names.  Also had to write (visualize-results-improved) in order to use new
  directory structure.  Restarting test at 1311.

17 Jun 15

--Job did not complete because the individual jobs that were started on
  perisikan somehow died without finishing.  Jobs on other servers completed
  correctly.  Removed perisikan from server pool and restarting job at 0920.
--Apparently perisikan problem happened because of thrashing causing a reboot.
  Don't know why that didn't happen on any of the other servers.
--Starting work on label_objects_all_floorplans.m, which will take the data
  output from each floorplan and do the following: 1) put common labels on the
  same object across all floorplans, 2) move all similar objects to a single
  dir, 3) output a floorplan file in scheme format (probably need to do this in
  a scheme wrapper).

18 Jun 15

--Job did not complete due to error in (get-object-detections-all-floorplans),
  'output-c is undefined'.  (get-codetection-results-training-or-generation)
  portion of job finished at 0118.
--Fixed code and restarted (get-object-detections-all-floorplans) at 1042.
--Job errored after rsync'ing files due to 'plan0 undefined'--think I'm missing
  quotes somewhere.  Also changed matlab-cpus-per-job in
  get-object-detections... to 12 in order to spread the work out to multiple
  machines. 
--Fixed quotes but looks like jobs errored again.  Error appears to come from
  problem with floorplan-dir-->needs to be appended to full dataset-dir for ls
  command. Fixed and restarting job at 1137.  Job errored again due to bogus
  directories in dataset (plan*/2014-*test20150615/) created a few days ago.
  Removed those dirs and restarted at 1144.
--Run completed on all floorplans at 1152.
--Results look reasonable, but not all objects detected in every floorplan--some
  floorplans had as few as 2 objects detected.  Not really sure why this is,
  maybe just poor proposal scores from those detections--can see objects like
  the shopping bag and the cone in the 'no_cluster' image dirs.

----------------git commit 1f5b458---------------

18 Jun 15

--Started working on label_objects_all_floorplans.m.  Noticed something strange
  with image files returned after rsync.  It was just in plan0 due to leftover
  garbage from testing.  Blew away old stuff on seykhl and copied in new data
  from verstand.  NEED TO FIX/CHANGE the saving location for
  detections...currently in 'detections' dir under plan*, probably should put
  data-output-dir on the end of that to distinguish different runs...then will
  need to add data-output-dir as an argument to label_objects_all_floorplans.m
--Retrying new job (up to individual floorplans stage (stage 3)) using new
  filename convention (above) and more processors per matlab job (8 for stage 1,
  12 for stage 3).  Should have end-to-end result in the morning.  Started run
  at 1705.
--Moved first round of detections into MSEE1-dataset/foo/plan*/detections to
  make sure a mistake in the above doesn't wipe out my testing data.

----------------git commit 3ff44f2---------------

19 Jun 15

--Run completed at 0558 for a total runtime of just under 13 hours.  Preliminary
  look shows that data looks similar to yesterday's run (which, at least for
  plan0, was different from the test runs--not sure why).
--Have label_objects_all_floorplans.m complete to the avg_similarity_matrix.
  Testing using the 20150618 run.  Running simultaneously on seykhl (6 cores)
  and save (48 cores).  MATLAB doesn't seem to be taking much advantage of the
  extra cores on its own--do I need to add explicit parallelism?
--Added explicit parfor on innermost loop of avg_similarity_matrix computation
  (the only place MATLAB let me do it).  Also limited by MATLAB to a max of 8
  cores, even on the 48-core servers.  Restarting simultaneous test at 1620.
--Minor cleanup of label_objects_all_floorplans.m

----------------git commit 7a42d94---------------

20 Jun 15

--Wrote (codetect-sort-templabel-auto-drive),
  (get-codetection-results-auto-drive), (get-matlab-data-auto-drive-improved) to
  run on the auto-drive dataset (no imu-log-with-estimates.txt file, just
  imu-log.txt; also directory structure has NOT changed on this dataset
  (MSEE1)).
--Started run at 1830.

----------------git commit 72c45cc---------------

20 Jun 15

--label_objects_all_floorplans.m run finished on seykhl after ~17 hours.  Same
  job on save still running at 2121 (~29 hours).

21 Jun 15

--Auto-drive jobs started at 1830 on 20 Jun 15 completed at 2314 on 21 Jun 15
  (~29 hours).

22 Jun 15

--label_objects_all_floorplans.m run on save (step 4) finished after ~54 hours.

---------Email to Jeff sent 1212----------- 
Just did a manual kill of my matlabpool workers on seykhl and save (and added
the kill to the end of my code).  I now see 1 matlab process each on seykhl and
save, which are the two interactive windows I have open now.

The comprehension runs for steps 1-3 finished last night around 11:30pm for a
total runtime of ~29 hours.  Not sure why it took so much longer than the
generation runs, but the explanation could be as simple as the comprehension
videos being longer--I haven't looked that closely at it yet, and I'm don't know
if the reason is important enough to spend much time figuring out.

I've started looking at the results from steps 1-3 on the codetection runs, and
they look reasonable.  I did notice a problem on plan0 and plan1 though.  When
Dan and I originally drove those runs, we did the first two plans and then found
an error which forced us to restart and do those two plans again.  I thought I
had gotten rid of the data from those 20 bogus runs, but apparently I hadn't.
So when I ran the comprehension data this weekend, plan0 and plan1 had 20 runs
each (instead of the 10 they were supposed to have).  This means that the step 3
data from those two floorplans is no good (steps 1 and 2 are fine since they
only deal with individual runs--step 3 aggregates all the runs in a floorplan).
I'm setting up a job to re-run just step 3 on just those two floorplans and
should have that started shortly.

The step 4 job on save finished after ~54 hours.  Not entirely sure why it took
over 3x longer for the same code and data to run on save than it did on seykhl
(8x 2.5GHz on save->54 hours vs. 5x 3.46 GHz on seykhl->17 hours).  I think the
next time I run the step 4 code I will ship the job over to one of the
workstations that's not being used interactively (probably chino, since that's
the desk where the rover sits) so that I can use all 6 processors without
bothering anyone.  Once I get the step 3 re-run started I'm going to look at the
step 4 interim results more closely and then finish off the logic to apply the
unique labels across all floorplans.
---------------end email-------------------------

--Re-run of plan0,plan1 on auto-drive started at 1242.  Had to do some
  debugging.  Working restart at 1406.  Finished at 1412.

--Found I needed to re-run plan5,plan8 as well b/c detections dir was not
  there.  Not sure why that happened--maybe rsync failed?

--As of 2101, the rerun of plan5/2014-11-21-02:27:38 and
  plan8/2014-11-21-03:03:46 had completed step 1 and was working on step 2.
  Looks like the step 1 files for both wrote correctly this time.

--Finished manually labeling generation-avg-similarity-matrix.xlsx.  Results
  don't look so good when just taking the max in the row (yellow) and column
  (orange).  Maybe think about something like finding anything within 10% of the
  diagonal value and doing some sort of voting.  Might want to think about
  bringing that back to step 3 as well--but need to think about if it would make
  a difference--observed some within-floorplan groupings that didn't make sense
  (plan8 had a box and a stool grouped as the same item, even though they were
  at different locations).

----------------git commit 83849b1---------------

23 Jun 15

--Steps 1 and 2 on the plan5/ and plan8/ bad runs finished at 0555.  Total
  runtime of ~14.5 hours.  Ran step 3 on those two floorplans and they finished
  in ~8 minutes.

--Step 3 seems to be running quickly--is there a need to change that out from
  Matlab?  Step 3 runs on 2 floorplans in <10 minutes, and on all floorplans in
  51 minutes (auto-drive) and 34 minutes (generation).  Changed number of matlab
  cores for step 3 from 12 to 20 to ensure that only 2 jobs get started on each
  machine at a time--this should bring the runtime down to ~10 minutes.

--Maybe just move step 4 into Scheme.

--Started talking with Dan on how to improve results in step 3
  (misclassification)--came up with new strategy to try--see board.

--Also worked our way back to the top step 3 and how many clusters are found
  from the scoring function.  Figured out that there's probably too much noise
  and things that are only getting seen a few times are getting drowned out in
  the scoring function.  Came up with idea to keep a count of how many times a
  particular winning box's position is visible over a whole floorplan and use
  1/count as a multiplier to the score in the scoring function.  That will
  (hopefully) ensure that things that are seen only a few times, but are only
  possible to see a few times, fare better.  Working on implementing this in
  Scheme and passing the data into Matlab for use in find_objects.  See board.

----------------git commit cab6cfa---------------

24 Jun 15

--Wrote (get-poses-that-match-frames) to take robot poses from IMU log and only
  return the ones that are closest in time to a frame time.

--Have all-poses, left-limits, right-limits and triangles defined in
  (get-detection-data-for-floorplan).  Next need to figure out the formula to
  determine whether a point is within a triangle.

--Wrote (point-in-triangle vec-p list-of-3-vecs-t) that returns 0 if point NOT
  in triangle, 1 if is in.

--Have (get-detection-data-for-floorplan) almost working--having a divide by 0
  when doing the 1/count computation--not sure how that is happening, since that
  means that a box's xy location is not ever within the field of view--should
  probably just handle the error and not worry about it.

--Error in (get-detection-data-for-floorplan) handled and function seems to be
  working correctly--need to remove dtrace?

--Next need to modify find_objects.m so that it uses the frequency data in the
  score computation.

----------------git commit 07672f3---------------

25 Jun 15

--After talking to Dan about the error in (get-detection-data-for-floorplan), he
  told me that there was a multiply by 1000 in (robot-pose-to-camera->world-txf)
  in rover_projection.sc because he took everything from m to mm.  Removed that
  and error appeared to go away.

--Remember that in (get-detection-data-for-floorplan), we set the **HORIZON** to
  pixel 205 (y).

--find_objects.m now seems to be working better and finding peaks that were
  missed before (see plan8).  

--Working with threshold value in that function to adjust when peaks get found
  (had been set at 0.3*max(scores(:))).  0.2 is too high for plan8. 0.18 works
  for plan8, but  when tried on plan0 and plan1 it merges multiple peaks into a
  single peak--no good.  Might just be best to leave the threshold at 0.3 and
  accept the fact that some peaks will be missed.

--Other ideas: 1) set the threshold as a multiple of the mean value of the
  scores function--Dan says that might not be a good idea, but I'm not sure
  why--maybe something with the mean and the std_dev? 2) try starting a
  threshold at the max value and then bringing it down, stopping when previously
  separate peaks merge into one.

--Trying stuff, and saw that using mean + 1.5*std_dev gives a pretty good result
  on plan8--need to look at other plans.

----------------git commit eb967f0---------------

26 Jun 15

--After testing some different settings, decided on using mean+2*std_dev for the
  threshold in find_objects.m.  

--Tried adding some parfors in sort_clusters_single_floorplan.m.

--Parfors seem to speed up sort_clusters_single_floorplan-->running on seykhl,
  each floorplan takes ~2 minutes on average to run all of Step 3 (minus scheme
  part).

--Playing with different methods of determining visual similarity.  First tried
  1a,2a from board, but it gave a self-similarity of 1.  Thought this wouldn't
  work, so went on to try 1b,2a.  This appeared to work at first, but for
  objects with low self-similarity (< ~0.3), it gave me problems--things that
  shouldn't have been merged together were merged.

--Talked to Dan about the above and went back to 1a,2a with self-similarity of
  1.  Looked at a couple of floorplans and saw that things that should be
  re-labeled had a similarity in the relabel column of > 0.4, while things that
  should not have been relabeled did not.  Working on the logic to get the
  correct label selected.  Having a problem with like objects all starting out
  with a zero label--need to think of a way to get the first one a unique label
  without making others bounce--look at column 1 of avg_similarity_matrix0.

----------------git commit 1bad56a---------------

29 Jun 15

--Have Step 3 mostly fixed.  Wrote labels_from_avg_similarity_matrix.m to do
  labeling.  Labels correctly for all but the following: plan5--tmp1 and tmp2
  are boxes, but tmp3 (bag) has a high row score for tmp1, and the tmp2/tmp1
  similarity scores are poor; plan6--similar problem with tmp1(bag),
  tmp3(table), tmp4(cone) all being put together b/c of high column scores for
  tmp3&4 to 1--all should be separate; plan7--tmp1 and tmp4 are both chairs, but
  get labeled separately.

--Not sure how to fix the boxes being labeled separately in plan5, but maybe fix
  bad bag labeling by looking at both (average?) of row and col scores.  Same fix
  might help plan6 also.

--Tried average row/col scores fix, but didn't quite work on all problem
  spots--maybe check the ratio between the numbers.

--Ratio comparison in labels_from_avg_similarity_matrix.m seems to work well.
  No longer getting dissimilar objects with the same label.  Still have similar
  objects getting different labels, but not sure yet how to fix that.  Perhaps
  try something with 2b (from board) in sort_clusters_single_floorplan.m.

--Going to try re-running modified label_objects_all_floorplans.m on both chino
  and save with the generation-testing dataset just to see if the changes to the
  way the parfor is working improves the runtime.

----------------git commit 0642a59---------------

30 Jun 15

--Rerun of step 4 code completed on chino in 502s (~8.37 min) and on save in
  1179s (19.65 min).  Vast improvement over previous runtimes.  Now need to
  check and make sure the avg_similarity_matrix from each is the same, and then
  figure out how to derive labels from that.

--Discovered some sort of error in how feature_vectors is concatenated in
  label_objects_all_floorplans.m--current avg_similarity_matrix values computed
  from this are invalid.  Need to relook at how I'm combining these--maybe move
  the combination of the fvcell data back into step 3.

--Believe I have the feature vector combining bug fixed--re-running on chino to
  test.

--Looks like I need to fix the 'BAD JUJU' part of
  labels_from_avg_similarity_matrix.m 

--Looking at new avg_similarity_matrix in spreadsheet.  First 3 plans are good.
  Errors:
     plan3: fplabel3 is cone, but gets new label instead of previous. Need to
               relook val_ratio_threshold value. 
	    fplabel4 is book bag, but gets new label not previous.  Matching
               columns/rows seem to be stuck in noise, possibly because ~20-30%
               of images are distant shots with stuff in background--maybe look
               at doing some of the 2b stuff???
     plan4: fplabel3 is chair, but row/col ratio might be off, or noise?
            fplabel4 is book bag--row/col ratio change will fix this one
	    ...

----------------git commit 888978e---------------

1 Jul 15

--Continued errors from above:
     plan5: fplabel1 and 2 are both box, but both get different labels (also
               different from true box label)
	    fplabel5 is cone, but gets no label (fixing BAD JUJU should fix
               this)
     plan6: fplabel1 is book bag, but gets no label--this one might be tricky
               b/c of many row elements above threshold.
	    fplabel2 is chair, but gets new label--this one should have matched
               on plan7, fplabel4, but that one didn't get a valid label 
	    fplabel4 is cone, but gets no label (fixing BAD JUJU should fix
               this)
     plan7: fplabel2 is box but gets unique label--this one might not be fixable
               w/o change to how avg_similarity_matrix is computed
	    fplabel4 is chair but gets no label (fixing BAD JUJU should fix
               this)
     plan8: fplabel2 is box but gets different label--this label looks like it
     	       comes from plan5, fplabel2, so fixing that one should fix this
	    fplabel3 is book bag, but gets new label--this could be fixed by
               changing val_ratio_threshold, although this one gets some crappy
               similarity scores vs. things it should do well against--maybe
               changes to asm computation needed here
     plan9: fplabel1 is box but gets different label--looks like it's tied to
               plan8, fplabel2
	    fplabel2 is table, but gets no label (fixing BAD JUJU should fix
               this)
	    fplabel3 is book bag, but gets new label--fixing the 0 label at
               plan6, fplabel1 should fix this 

--From looking at errors, seems like the best first step is to fix the BAD JUJU
  part of the code and see what that fixes.

--Need to think of a way to fix plan3, fplabel3 (i=13)...previous matches do not
  meet threshold, but later matches do...maybe something along the lines of a
  variable to track if everything has had one pass first?...but will that stop
  everything from getting a unique label?...maybe automatically give first
  floorplan unique labels

--Fixed BAD JUJU portion of labels_from_avg_similarity_matrix.m.  Now everything
  gets a label (no more 0s).  Next problem is the labels that are higher than
  the true labels--maybe a pass backwards through the label numbers, looking for
  matches that have a lower label number?

--Working on backwards pass--seems workable.  Gets 38/43 labels correct.

--Modified sort_clusters_single_floorplan.m to implement 1a,2b for individual
  similarity matrices.  Going to test and see what the avg_similarity_matrix
  looks like for each plan, using test_step3new.m

----------------git commit c24cf90---------------

2 Jul 15

--Results of re-run of Step 3 on generation-testing are bad--dissimilar objects
  are getting the same label.  Thinking about changing Step 3 back to the way it
  was and just using the 1a,2b stuff on Step 4.  This will require splitting
  labels_from_avg_similarity_matrix.m into different versions for each step.

--Changed sort_clusters_single_floorplan.m back to using the 1a,2a method of
  finding avg_similarity_matrix elements.  Put 1a,2b into
  label_objects_all_floorplans.m.  Re-running this on chino (also accessing
  files through /net/seykhl instead of rsync--no slowdown observed).

--Hit interesting case in labels_from...step4.m.  Got to other_labels = 2 case
  and had the actual best ratio val come from a label that had already been
  eliminated because it was still 0.  Not sure how to deal with this.

--Got past previous error by just checking a condition and skipping to the next
  pass.  Now have disabled the backwards pass (after playing with the relaxation
  number) and still getting bad labelings.  Need to look at how I'm dealing with
  the multiple labels condition--maybe not doing the right thing in the initial
  loop. 

----------------git commit 896945d---------------

6 Jul 15

--Looking at new version of asm and how things are getting mislabeled.  Maybe
  ratio stuff is screwing it up.  Look at counting matches in BOTH row and
  column, and also maybe changing voting to use asm value for that box as a
  voting value.  Need to think about what to do when matches are to columns/rows
  further on in the matrix that don't have labels yet--maybe some way of keeping
  track of who matches to whom??

--SCRATCH ALL OF THE ABOVE REGARDING LABELING STEP 4 OUTPUT IN MATLAB.  Instead
  formulate the step 4 labeling problem as a graphical model where each detected
  peak is a variable that is given one of k labels (k is an input).  Unary
  scores for each label will be uniform since we don't care which particular
  label a peak gets, just that the peaks corresponding to the same objects get
  the same label.  Then the binary scores between labels will be one of two
  values: the first (a) will come from the average of the two computed scores
  for each pair of peaks in avg_similarity_matrix and will represent the score
  between two peaks when their labels are the same; the second (b) will be a
  constant value (actual number probably doesn't matter much) that will
  represent when two peaks have different labels.  Also need to think about
  implementing an "other" label (similar to dummy proposals) for the weird
  objects that shouldn't have a label.

--Look back at notes starting 11 Mar and code in inference.cpp for how to build
  graphical model solver.

----------------git commit b7f327c---------------

7 Jul 15

--Starting work on bp_label_inference in inference.cpp to do GM solving for peak
  labels.  Should be a (simpler) clone of bp_object_inference already there.
  Main input will be in **g, which will be avg_similarity_matrix from matlab.
  Can manipulate it with (matlab-get-variable ...), (matrix->list-of-lists ...),
  and (easy-ffi:double-to-c 2 ...).  All other input will be constant
  ints/doubles.  Output will be on *labels, similar to *boxes in
  bp_object_inference.  Look at (run-codetection-with-proposals-similarity) in
  codetection-test.sc to see how to manipulate input and output variables.

--Have f score part of bp_label_inference done.  Working on g score part.  Need
  to make sure I'm thinking it through correctly (see comments in code).  Should
  be able to do a single pass through all peaks, adding matrices between that
  peak and all future peaks--correct??

----------------git commit af4e6cd---------------

8 Jul 15

--Finished bp_label_inference and it compiles into dsci.  Working on testing.

--(bp-label-inference) runs, but gives results that are meaningless (usually all
  the same label).  Building a testing function (test-labeling) in
  label-test-setup.sc in order to test quickly.  Need to figure out if the bad
  results are because of the input parameters or because of an error in the GM
  solver code.

--Initial testing shows the GM solver is quite sensitive to the value chosen for
  default-g-value, which I guess makes sense.  Probably need to add some
  debugging statements to inference.cpp to take a look at what the gg matrices
  look like.

----------------git commit e292daa---------------

9 Jul 15

Email from Jeff and my reply:
--------------------------------------------------------------------------------
On Wed, Jul 8, 2015 at 7:28 PM, Jeffrey Mark Siskind <qobi@purdue.edu> wrote:
Scott - Several debugging suggestions:

 1. Print the values of some of the vertex and edge weights to make sure that
    the values are qualitatively what you expect them to be. Numbers that you
    expect to be big should be big and those you expect to be small should be
    small.

​I did this yesterday​ ​and things looked as I expected them to look.  So I don't
think the problem is a simple error in the code.​
 
 2. Make sure that you have the polarity of the cost function correct.
    You want disssimilarity of proposals associated with peaks should penalize
    them from having the same label. This is different than having similarity
    of proposals associated with peaks reward them when having the same label.
    The latter can be trivially satisfied when all peaks have the same label.


​I'm beginning to think that this is exactly where my problem is.  Right now I
have all my vertex weights (unary scores) set uniformly, which I think is
reasonable because the particular label value that a set of similar peaks gets
does not matter.  For my edge weights (binary scores), I am using a similarity
score between peaks.​ It is computed as follows:

1.  For a pair of peaks (X & Y), I take the image descriptors of all images
associated with each and compute the chi-squared distance between every pair of
images (actually 1-chisq distance so that higher=better).  This gives a numX by
numY matrix of similarity scores between individual images.  

2.  I then take the row and column maxes of this matrix, which represent the
best matches for each image.

3.  I then take these row and column max vectors and take the mean of the top N
percent (currently set at 50%, but that was arbitrary) of each vector.  This
gives me 2 numbers that represent how well X matches to Y and how well Y matches
to X.  ​

4.  For the final similarity score between X and Y, I average the two numbers in
3.

For the matrix of edge weights between X and Y, I put the similarity score
described above on the diagonal, so that it represents the score for X and Y
having the same label.  For the off-diagonal elements, I just use a default
score (representing dissimilarity), which should in theory be a worse score.

I've found in testing that using a default dissimilarity score is probably not
good enough.  No matter what value I set for the default score, the trivial
solution (all labels the same) keeps popping up.  So I'm thinking that I
probably need to figure out and compute some sort of dissimilarity measure.  It
could be as simple as replicating the above steps with no change in 1 or 4, then
changing 2 to a min (vice max) and 3 to a mean of the bottom N percent.  I need
to think this through a bit more, but I think this is probably a good place to
start.
-----------------end email------------------------------------------------------
Jeff's reply and my reply to it:

On Thu, Jul 9, 2015 at 10:52 AM, Jeffrey Mark Siskind <qobi@purdue.edu> wrote:
  The similarity/dissimilarity score should be symetric. I.e. sim(X,Y)=sim(Y,X)
  and dissim(X,Y)=dissim(Y,X). If not, you are doing something wrong.

​Understood.​ I'm working on rethinking how to find a uniform measurement for
similarity between groups of images--i.e., how to reduce the numX by numY matrix
in 1 to a single value.  Determinants won't work because the matrices aren't
square.  The idea I'm playing with now is to take the SVD of the numX by numY
matrix of individual image comparisons, and then take the mean of the singular
values.  Looking at the results of this, it seems reasonable.  I still get
self-similarity of 1, and the highest values in a row/column usually match to a
similar object.

  And you definitely want a dissimilarity penalty, not a similarity reward.
  Using a similarity reward will *necessarily* assign all peaks to the same
  class.  It can't possibly do anything else. Think of it this way. If you have
  an attractive force between N particles, they will all be drawn together, no
  matter how weak the forces are and no matter how different the forces are. If
  you have a repulsive force between N particles, and a limit on the size of the
  bounding sphere they will cluster into points on the surface of the sphere
  where the clusters minimize the repulsive force within a cluster (and the
  cluster positions are maximally spaced).

​Agreed.  I'm trying to think of a way to measure dissimilarity.  Could it be as simple as 1/sim(X,Y) or (1-sim(X,Y))?​
--------------------------------------------------------------------------------

--Based on the above reply, thinking about how to do both a symmetric similarity
  measure, and some sort of dissimilarity measure.  Thought about determinants
  for the X to Y matrix, but since matrices are not square, det is not defined.
  Then looked at SVD for the X to Y matrices.  Currently taking mean of singular
  values and using that in avg_similarity_matrix.  Seems to be a decent
  measure--self-similarity is 1, and things that should match often have a value
  > 0.75, but not always.  But the largest value in the columns usually matches
  to a correct object.

--Could do a dissimilarity measure as simple as 1/sim or 1-sim...

--After talking with Dan, not sure what the mean of singular values means.  Also
  looked at only taking the mean of nonzero (or > eps?) singular values (tested
  this in label_objects_test.m and found that no singular value was < 1e-10).
  Also remembered that singular value is square root of eigenvalue, so thought
  about squaring mean singular value...

--Going to try to do a rewrite of bp_label_inference to use either 1/sim or
  1-sim as dissimilarity measure.  Also going to get rid of dummy_g to make
  things simpler.  Jeff seems to believe that using this would result in a
  no-op, but Dan and I aren't so sure about that.

--looked at maxIterations and convergenceBound--maybe those are set wrong.  Need
  to finish working on inference.cpp.

----------------git commit d3eddea---------------

10 Jul 15

--Looked at interim optimizer output with visitor function and saw that
  regardless of the number of iterations set, the optimizer was exiting after 1
  step because convergence bound was at -infinity.  Tried setting convergence
  bound to -infinity and saw that through thousands of iterations, the function
  value and bound never changed, so it looks like it can't find a way out of the
  initial guess.  Going to try building a smaller subset of the data (probably
  first 2 floorplans, 8 peaks with 6 unique labels) to see what happens then.
  Probably need to try to solve the smaller model by hand to see if the output
  is what I expect.  Might also think about moving from log space to linear
  space to make things simpler to see.  What to set for dissimilarity?

--Playing around and with tiny (4 peaks) subset and noticed that the values set
  for the first peak seem to overwhelm all the others.  This might be because of
  the way I set up the graph, since the first peak is connected to all
  subsequent peaks explicitly.  I assumed that the links were bidirectional, but
  maybe they're not.  Going to try explicitly connecting all peaks.

--Tried the above and got 'OpenGM Error: variable indices of a factor must be
  sorted' so apparently doing that won't work.  Does my graph have loops?  That
  could cause a problem with standard BP--actually it looks like standard BP is
  ok with loops.

--Tried using Bruteforce algorithm and got correct solutions for tiny (4x4) and
  small (8x6) sets.  This tells me that the cost function is set up correctly,
  and the problem is with BP.  Need to look at OpenGM docs and figure out what
  I'm doing wrong (or a better algorithm to use).

----------email traffic with Jeff this afternoon--------------------------------

​On Fri, Jul 10, 2015 at 3:52 PM, Jeffrey Mark Siskind <jeffrey.mark.siskind@gmail.com> wrote:
Did you check the cost function? Did you find why you don't get a good solution?

    Jeff (http://engineering.purdue.edu/~qobi)

As far as Dan and I can tell, the cost function is set up correctly and the
problem is somewhere with the Belief Propagation (BP) inference algorithm.

I was trying smaller subsets using BP, first a tiny subset with 4 peaks that
were all different and then a small subset with 8 peaks that had 2 pairs of
matching peaks (6 unique labels).  With BP, it would always label the first peak
0 and the rest of the peaks 1, and its objective function value was worse than
what I computed by hand.  From looking at the interim output via the visitor
function, I could see that it was starting with that suboptimal value and never
changing it.

Dan suggested that I try the Brute Force algorithm on these smaller sets, and
that worked.  The 4 peak 4 label tiny set (16 possible combinations) returned in
< 1 ms, and the 8 peak 6 label small set (6^8 or ~1.7M possible combinations)
returned in ~11.5s.  Unfortunately I don't think I can run brute force on the
full set of 43 peaks with 8 possible labels (8^43 or 6.8e38 possible
combinations).  But the fact that brute force did work tells me that I have the
problem set up correctly, and that the errors I'm getting stem from how I'm
trying to use the BP algorithm.

For the problem setup, I used the similarity score I described in my first email
to you yesterday morning (averaged best matches, for lack of a better name), and
for the dissimilarity I used 1-sim.  I realize that this sets a threshold at
0.5, but it seems to work for now.  I moved all the values back into linear
space, and I'm doing a maximization of a summation of the scores.

I think I need to spend some time with the OpenGM documentation to read up on
the different algorithms they have implemented--either I'll find that there's an
algorithm other than BP that's better suited to this problem, or I'll figure out
what I'm doing wrong when I set up BP.

-----------------------------end email------------------------------------------

----------------git commit c06a0b4---------------

13 Jul 15

--Changed inference.cpp main function for label inference (bp_label_inference)
  to use libDAI BP instead of OpenGM BP/Bruteforce.  Kept working Bruteforce
  version in bruteforce_label_inference.  Not working quite right yet--getting
  (0 1 0 1) or (1 0 0 1) for tiny.

--Trying to change everything back to log space with minimizer, since the
  default inference type with libDAI is maxprod, and it should be sumprod, but I
  can't figure out how to change that parameter.  Got Bruteforce version changed
  and the answers look correct.

--Change back to log space in bp_label_inference looks like it yields the same
  results.  Not sure what other parameters to change.  Tried changing updateRule
  from PARALL to SEQFIX; no change.  

Output of (test-labeling-tiny 4 0) 

Starting BP[tol=-inf,maxiter=10000,maxtime=inf,verbose=2,logdomain=0,updates=SEQFIX,damping=0,inference=MAXPROD]...BP::run:  WARNING: not converged after 10000 passes (0.182613 seconds)...final maxdiff:0
180 ms
LibDAI Belief Propagation 924.702
(0 1 0 1)

Correct: OpenGM Brute Force 186.587 (3 2 1 0)

Not sure where the problem is.  Thinking that logdomain=0, inference=MAXPROD are
problems, but not sure how to change those parameters.  Is the interface in the
libDAI library or in OpenGM??

----------------git commit 9d78659 ---------------

14 Jul 15

--Remembered that I should have been calling the (test-labeling-*) functions
  with a f-value of 1 (vice 0) after changing to log space.  New output is:
Incorrect: (test-labeling-tiny 4 1)
	   Starting BP[tol=-inf,maxiter=10000,maxtime=inf,verbose=2,logdomain=0,updates=SEQMAX,damping=0,inference=MAXPROD]...BP::run:  WARNING: not converged after 10000 passes (0.350979 seconds)...final maxdiff:0
	   350 ms
	   LibDAI Belief Propagation 3.66832
	   (0 1 0 1)
Correct: (test-labeling-tiny-brute 4 1) 
	 0 ms 
	 OpenGM Brute Force 2.38058 
	 (1 0 3 2)

--Also tried running libdai with all 4 possibilities for UpdateRule, same
  results on all.

--Tried changing DiscreteSpace to SimpleDiscreteSpace, but no change; brute is
  still correct, libdai is still wrong.

----------------git commit b2f740e---------------

15 Jul 15

--After email exchange with Haonan, going to try adding some random noise to the
  off-diagonal elements.  Need to think about whether to add the noise before or
  after going to log space, as well as how big the noise should be.  Don't want
  it to be so big that it changes a good score to a bad one or vice versa.

--Trying adding noise in log space.  With noise limited to [-0.01,0.01], get
  almost correct results.  But also getting the 'not converged' warning with
  tolerance=1e-10.  Wonder if I should add more iterations to see if it will get
  better. Although it is surprising that the tiny set doesn't converge after
  1000 iterations, since there are only 4^4=256 possible combinations.

-----Email traffic with Jeff and Haonan-----------------------------------------
​From: Jeffrey Mark Siskind <qobi@purdue.edu>
Date: Wed, Jul 15, 2015 at 1:46 PM
Subject: did you try adding noise?
To: sbroniko@purdue.edu
Cc: dpbarret@purdue.edu


Did you try adding noise?

    Jeff (http://engineering.purdue.edu/~qobi)

​From: Scott Bronikowski <sbroniko@purdue.edu>
Date: Wed, Jul 15, 2015 at 3:26 PM
Subject: Re: did you try adding noise?
To: Jeffrey Mark Siskind <qobi@purdue.edu>
Cc: Dan Barrett <dpbarret@purdue.edu>, Haonan Yu <haonan@haonanyu.com>


I'm working on it now.  It seems like the size of the noise might have an effect
on the correctness.   

I'm running the brute force and the libdaiBP on the same functions/factors so
that I can get an apples-to-apples comparison on them. With the noise set to the
range [-0.01,0.01], brute force still gets the right answer, but libdaiBP is
still wrong, though much less wrong than without noise.   

For the tiny (4 peaks, 4 labels) case:
OpenGM Brute Force 2.36102
(0 2 3 1)
LibDAI Belief Propagation 2.80476
(3 1 3 2)

For the small (8 peaks, 6 labels) case:
OpenGM Brute Force 9.82801
(2 3 5 4 5 0 1 4)
LibDAI Belief Propagation 11.305
(2 1 5 4 5 0 1 4)

So as you can see, in both cases libdaiBP's objective function score is higher
than it should be, and it also has one label that is different.  This is a
marked improvement over the noiseless version I was working with yesterday. 

Another interesting thing I noticed is that in both the tiny and small cases,
libdaiBP gave me the 'WARNING: not converged' warning with tolerance = 1e-10.  I
have maxIterations=1000 now, but I wonder if it will do better if I bump up the
iterations. 

I'm going to try increasing the size of the noise to see if it helps.  I'll also
try the different UpdateRule values--the above was done with PARALL.

​From: haonan <haonan@haonanyu.com>
Date: Wed, Jul 15, 2015 at 3:32 PM
Subject: Re: did you try adding noise?
To: Scott Bronikowski <sbroniko@purdue.edu>
Cc: Jeffrey Mark Siskind <qobi@purdue.edu>, Dan Barrett <dpbarret@purdue.edu>


You can try setting the tolerance to a bigger value like 1e-6 (1e-10 is too
small). Then it should be easy for BP to converge. When BP is trying to
converge, the cost is not strictly increasing/decreasing. It may jump back to a
worse solution at your max iteration 1000. You should be able to print out the
solution at each iteration to see whether this is true. 
 
Haonan

​From: Scott Bronikowski <sbroniko@purdue.edu>
Date: Wed, Jul 15, 2015 at 3:51 PM
Subject: Re: did you try adding noise?
To: haonan <haonan@haonanyu.com>
Cc: Scott Bronikowski <sbroniko@purdue.edu>, Jeffrey Mark Siskind
<qobi@purdue.edu>, Dan Barrett <dpbarret@purdue.edu> 



You should be able to print out the solution at each iteration to see whether
this is true. 


​Do you know how to do this?  I currently have verboseLevel=2, but that is just a
guess.  I haven't been able to find​ what the different verboseLevel values do. 


​From: Jeffrey Mark Siskind <qobi@purdue.edu>
Date: Wed, Jul 15, 2015 at 3:33 PM
Subject: Re: did you try adding noise?
To: Scott Bronikowski <sbroniko@purdue.edu>
Cc: dpbarret@purdue.edu, haonan@haonanyu.com


   So as you can see, in both cases libdaiBP's objective function score is
   higher than it should be

Again, you should check for both brute force and libdai, that the value of the
objective function is correct for the labeling that it gives. You should have
a simple Scheme function that checks this so that you can do it without
effort.

I presume that you are trying to minimize the objective functions, correct?

Is this still in log space? You should probably stay in log space.

   Another interesting thing I noticed is that in both the tiny and small
   cases, libdaiBP gave me the 'WARNING: not converged' warning with tolerance
   = 1e-10.

Is this for both tiny (4 peaks, 4 labels) and small (8 peaks, 6 labels)? If
so, then a good strategy would be to change the parameters that influence the
number of iterations until this message goes away.

    Jeff (http://engineering.purdue.edu/~qobi)

​From: Scott Bronikowski <sbroniko@purdue.edu>
Date: Wed, Jul 15, 2015 at 3:52 PM
Subject: Re: did you try adding noise?
To: Jeffrey Mark Siskind <qobi@purdue.edu>
Cc: Scott Bronikowski <sbroniko@purdue.edu>, Dan Barrett <dpbarret@purdue.edu>,
Haonan Yu <haonan@haonanyu.com> 


I presume that you are trying to minimize the objective functions, correct?

​Yes
​ 
Is this still in log space? You should probably stay in log space.

​Yes it is in log space.
​ 

   Another interesting thing I noticed is that in both the tiny and small
   cases, libdaiBP gave me the 'WARNING: not converged' warning with tolerance
   = 1e-10.

Is this for both tiny (4 peaks, 4 labels) and small (8 peaks, 6 labels)? If
so, then a good strategy would be to change the parameters that influence the
number of iterations until this message goes away.


​Will do.​ 

​From: haonan <haonan@haonanyu.com>
Date: Wed, Jul 15, 2015 at 3:56 PM
Subject: Re: did you try adding noise?
To: Scott Bronikowski <sbroniko@purdue.edu>
Cc: Jeffrey Mark Siskind <qobi@purdue.edu>, Dan Barrett <dpbarret@purdue.edu>


There is a visitor class in openGM. Check out the section 3 in openGM manual and
search "visitor". Pass a visitor to infer() function will print out useful
messages. 

Haonan


​From: Scott Bronikowski <sbroniko@purdue.edu>
Date: Wed, Jul 15, 2015 at 4:25 PM
Subject: Re: did you try adding noise?
To: haonan <haonan@haonanyu.com>
Cc: Scott Bronikowski <sbroniko@purdue.edu>, Jeffrey Mark Siskind
<qobi@purdue.edu>, Dan Barrett <dpbarret@purdue.edu> 


I tried using the visitor on infer(), and all it did was print out the initial
and final values of the objective function, but nothing in between. 


​From: haonan <haonan@haonanyu.com>
Date: Wed, Jul 15, 2015 at 4:30 PM
Subject: Re: did you try adding noise?
To: Scott Bronikowski <sbroniko@purdue.edu>
Cc: Jeffrey Mark Siskind <qobi@purdue.edu>, Dan Barrett <dpbarret@purdue.edu>


I remember in the manual that you can self define a visitor class to do stuff
you want. You can try doing that. There might be also some parameters you need
to pass to the visitor object to tell it what to print. You can find examples in
the manual. 

Also note that currently you have one redundant variable (label) in your
formulation. If you don't care which specific labels the peaks have, you can
first assign label 0 to peak 0, and let BP decide the rest. To do that, set the
unary cost for peak 0 as 0 if label=0, otherwise inf. Or you can rewrite the
framework so that the graph doesn't include peak 0 explicitly. 

Haonan


​From: Scott Bronikowski <sbroniko@purdue.edu>
Date: Wed, Jul 15, 2015 at 5:00 PM
Subject: Re: did you try adding noise?
To: haonan <haonan@haonanyu.com>
Cc: Scott Bronikowski <sbroniko@purdue.edu>, Jeffrey Mark Siskind
<qobi@purdue.edu>, Dan Barrett <dpbarret@purdue.edu> 


I might look into the visitor class stuff if I think it's worth the effort, but
for right now I'm getting decent results by playing around with the values for
maxIterations, noiseLimit, and tolerance. 

Setting peak 0 to label 0 is a really good idea.  I'm going to give that a try.
I'm actually seeing cases now where I get the 'not converged' warning after
1000000 iterations on the tiny set, but the output labels are an equally correct
permutation of the optimum label set found by brute force.  If I start all
possible solutions with 0 as the first label, that should drastically reduce the
total possible permutations. 

-------------------End email traffic--------------------------------------------

--Getting good results with tolerance in the range of 1e-4 to 1e-5 and
  iterations over 100000.  Sometimes still doesn't converge, and some of these
  non-convergences are valid permutations of the brute force optimum.  Going to
  implement Haonan's idea of fixing all peak 0 labels to 0 to reduce the
  permutations.  Another thought--should the tolerance be somehow linked to the
  noiseLimit? 

--Fixed peak 0 to label 0.  Also wrote (test-labeling-noise) in
  label-test-setup.sc in order to run the whole dataset.  Left that running as
  of 1805.

----------------git commit a3ddbce---------------

16 Jul 15

----------Email to group ----------
From: Scott Bronikowski <sbroniko@purdue.edu>
Date: Thu, Jul 16, 2015 at 9:52 AM
Subject: Re: did you try adding noise?
To: Scott Bronikowski <sbroniko@purdue.edu>
Cc: haonan <haonan@haonanyu.com>, Jeffrey Mark Siskind <qobi@purdue.edu>, Dan
Barrett <dpbarret@purdue.edu> 


I let the labeling function run on the entire 43-peak, 8-label
generation dataset last night, and I saw the following results this
morning
--------------------------------------------------
> (test-labeling-noise 8 1 1e5 0.1)
in label_inference_with_noise
num_peaks = 43, num_labels = 8
Unary functions added
Binary functions added
before bp call
after bp call
before bp.infer
begin: value 429.207 bound -inf
Starting BP[tol=0.0001,maxiter=100000,maxtime=inf,verbose=2,logdomain=0,updates=PARALL,damping=0,inference=MAXPROD]...BP::run:
 WARNING: not converged after 100000 passes (529.215 seconds)...final
maxdiff:0.737506
value 507.664 bound -inf
after bp.infer
LibDAI Belief Propagation 528550 ms
LibDAI Belief Propagation 507.664
(0 5 4 7 4 1 5 7 0 7 4 5 7 1 1 5 1 5 1 5 1 5 1 5 5 7 6 5 4 7 5 1 1 5 1
1 1 5 1 5 4 1 5)
-------------------------------------------------------

The good news is that for 100000 iterations, it runs in < 9 minutes.
The bad news is that the results are pretty crappy.  See the labels
paired with the ground truth objects below.  And I also see that the
final value of the objective function (507.664) is worse than the
beginning value (429.207).  That tells me that when the solver hits
max iterations it just spits out whatever solution it has at that
iteration and doesn't keep any memory of the best solution it's seen
so far, as Dan and I were discussing yesterday.

I think I've got tolerance set to a pretty reasonable value (1e-4).
I'm going to try playing with the max iterations and the noise value.
I'll try the noise value first, since I know 100k iterations will only
take about 10 minutes to run.  I think I may have set the noise value
too high on this one--looking back on what I ran yesterday, the small
and tiny sets had good results with the noise set at the 0.01 to 0.05
range.

---------labels with ground truth-------
0 chair
5 bag-book
4 table
7 cone
4 table
1 stool
5 box
7 cone
0 box
7 cone
4 table
5 bag-shop
7 cone
1 bag-book
1 chair
5 unknown
1 unknown
5 chair (?)
1 bag-book
5 cone
1 unknown
5 box
1 box
5 bag-book
5 table
7 cone
6 bag-book
5 chair
4 table
7 cone
5 chair
1 box
1 bag-book
5 chair
1 stool
1 stool
1 box
5 bag-book
1 chair
5 box
4 table
1 bag-book
5 stool
-------End email to group----------------

--Trying with different values for noise limit.  Still seeing that final value
  is worse than starting value for NL=0.01,...  Maybe need to increase
  iterations?

--Also had the thought that it might be worthwhile to have a function that runs
  the data multiple times (for different random noise each time) and then finds
  the run with the best score.  But how can I programmatically find the begin
  value to determine if the end value is less (better), or to see if the run
  converged or not?

--Do I want to write a script/function that does an exploration of the parameter
  space? Make tolerance a parameter and include that.  Fix f_value since it will
  always be 1.

----------------------------Emails from Haonan----------------------------------
From: haonan <haonan@haonanyu.com>
Date: Thu, Jul 16, 2015 at 2:07 PM
Subject: Re: did you try adding noise?
To: Scott Bronikowski <sbroniko@purdue.edu>
Cc: Jeffrey Mark Siskind <qobi@purdue.edu>, Dan Barrett <dpbarret@purdue.edu>


Did you try fixing the the first peak as 0? It looks like you did from the
result shown here, but I just want to make sure. 

100000 iterations are TOO many. In my case of 60 variables with each variable
having 250 labels, it only takes about less than 30 seconds to
converge. Whenever the iteration number is too large, it is a sign that the
framework has some problem. With 10 minutes, I believe a good search strategy
with pruning or a greedy algorithm can find good solution, then BP does not have
any advantage. 

I don't think directly adding noise is a principled solution. It can only be
used as a way to validate the assumption that BP doesn't perform well when the
costs are symmetric. If there is no way to specify binary costs that are
discriminative for your problem, then maybe this problem is not well suited for
BP. I would try other algorithms like A* search or linear programming (after
conversion). 

Haonan

From: haonan <haonan@haonanyu.com>
Date: Thu, Jul 16, 2015 at 2:19 PM
Subject: Re: did you try adding noise?
To: Scott Bronikowski <sbroniko@purdue.edu>
Cc: Jeffrey Mark Siskind <qobi@purdue.edu>, Dan Barrett <dpbarret@purdue.edu>


For linear programming, with each pair of peaks, you have 8*8 variables a(i,j)
that are in [0,1]. When a(i,j) = 1, it means that you pick label i for one peak
and j for another. Also you have a constraint so that \sum a(i,j) = 1. In total
there will be 43*42/2*8*8 = 57792 variables, which is pretty common for linear
programming problems and I am pretty sure it can find *global* optimum in less
than 10 minutes. 

Haonan
--------------------------------end emails-------------------------------------

--Based on the above and conversation with Jeff and Dan in group meeting, going
  to put graphical model approach on hold for now and try a branch-and-bound
  algorithm.  Will probably end up coding the final version up in C++, but have
  a rudimentary scheme version worked out with Dan on the whiteboard--will
  implement and test on tiny and small sets to ensure that it works.  That will
  give me something to compare my C code against.  Also need to read up on
  branch and bound and other possible algorithms.

----------------git commit 4aac693---------------

17 Jul 15

--Starting work on labeling.sc, where I will put the new stuff Dan and I talked
  about yesterday.

--Have (find-labels) working, I think.  Not sure if the algorithm is
  correct--don't see it abandoning any bad baths on the tiny or small datasets.
  It does find correct answers though.  Going to try running on the full dataset
  over the weekend to see what it comes up with and how long it takes.  Might
  need to rethink how we're estimating best possible and deciding--might have a
  bug in the code.

--Also might want to think about moving the labeling.sc stuff into a file that
  compiles into dsci in order to speed things up.

----------------git commit 8b0b5a6--------------- 

20 Jul 15

--On Friday before leaving, Dan and I fixed the logic issue and confirmed that
  the code was abandoning bad branches in the tree.  Tried running against the
  full dataset.  Started at 1842 on Friday; as of 1052 on Monday morning (>64
  hours) , was still running.  Going to kill the run and try compiling the code
  into dsci to see if it will run faster.  Should probably also try the smaller
  (half) dataset first.

--Made smaller runs and got the following
> (run-4)
0.01s (2.380576052665758 (0 1 2 3))
> (run-8)
0.05s (9.924843526466908 (0 1 2 3 2 4 5 3))
> (run-12)
2.49s (21.47483629609758 (0 1 2 3 2 4 5 3 5 3 2 6))
> (run-16)
329.35s (39.83369894630844 (0 1 2 3 2 4 5 3 5 3 2 6 3 1 0 7))
(run-21) ran for over and hour with no result.  

--Thinking about doing some iterative runs where the next iteration is seeded
  with the results (either the actual label list or just a score based on the
  previous labels plus filler as the best).

--Need to think more about the above.  Seeding future runs with previous results
  can lead to problems if there is bad data early, but we can mitigate this by
  doing multiple different orderings of the peaks and comparing results.  Will
  need to put results back into original order and the compare different results
  by looking at which peaks are labeled alike--if peaks 1,3,5 are the same, all
  we care about is that they get the same label.

--Did some logic speedup in (find-labels)--now in toollib-codetection.sc.  Now
  getting (run-21) done in ~19 minutes with perfect correctness
> (run-4)
0.01s (2.380576052665758 (0 1 2 3))
> (run-8)
0.02s (9.924843526466908 (0 1 2 3 2 4 5 3))
> (run-12)
0.09s (21.47483629609758 (0 1 2 3 2 4 5 3 5 3 2 6))
> (run-16)
0.69s (39.83369894630844 (0 1 2 3 2 4 5 3 5 3 2 6 3 1 0 7))
> (run-21)
1136.01s (74.84826991034235 (0 1 2 3 2 4 5 3 5 3 2 6 3 6 0 4 7 0 1 1 7))

----------------git commit 8e9fefa--------------- 

21 Jul 15

--Did some more logic improvements (no longer need to search through small table
  to find best-possible, just have it saved in big table) and got faster runs
  with the same results.
> (run-4)
0.01s (2.380576052665758 (0 1 2 3))
> (run-8)
0.02s (9.924843526466908 (0 1 2 3 2 4 5 3))
> (run-12)
0.07s (21.47483629609758 (0 1 2 3 2 4 5 3 5 3 2 6))
> (run-16)
0.60s (39.83369894630844 (0 1 2 3 2 4 5 3 5 3 2 6 3 1 0 7))
> (run-21)
1051.44s (74.84826991034235 (0 1 2 3 2 4 5 3 5 3 2 6 3 6 0 4 7 0 1 1 7))

--Noticed there are errors in run-21, including ones that changed from run-16.
  Don't know if a larger run would fix this because we haven't tried it.  Run-43
  did not finish overnight.  Restarted it this morning to try it with latest
  improvements.  

--Need to think about logic in Jeff's email from today (21 Jul 15) to see if we
  are doing the best possible thing we can be doing.

--Also need to think more about seeding future runs and doing multiple order
  permutations (see bullets 3 & 4 from yesterday).

--Changed pruning to use a sort of best-possible to investigate best branch
  first.  Results improved dramatically, but still didn't run fast enough for
  run-25. (still have errors as well)
> (run-4)
0.01s (2.380576052665758 (0 1 2 3))
> (run-8)
0.01s (9.924843526466908 (0 1 2 3 2 4 5 3))
> (run-12)
0.02s (21.47483629609758 (0 1 2 3 2 4 5 3 5 3 2 6))
> (run-16)
0.09s (39.83369894630844 (0 1 2 3 2 4 5 3 5 3 2 6 3 1 0 7))
> (run-21)
288.22s (74.84826991034235 (0 1 2 3 2 4 5 3 5 3 2 6 3 6 0 4 7 0 1 1 7))

--Tried some more seeded runs (see results in generation-working-list.ods).  Got
  faster results on bigger data, but had the problem that errors made in earlier
  iterations (i.e., what is used to seed future runs) never get fixed.  Think
  this might be a dead end.

--Then tried -a runs that have twice the number of possible labels--those run
  MUCH faster (still haven't gotten 35a and 43a to return yet).  NO ERRORS, but
  have objects that have multiple labels--singletons in here might be a problem
  for noun-learning system.
> (run-4a)
0.02s (2.380576052665758 (0 1 2 3))
> (run-8a)
0.02s (9.924843526466908 (0 1 2 3 2 4 5 3))
> (run-12a)
0.02s (21.47483629609758 (0 1 2 3 2 4 5 3 5 3 2 6))
> (run-16a)
0.03s (39.22651273787593 (0 1 2 3 2 4 5 3 5 3 2 6 3 7 0 8))
> (run-21a)
0.08s (70.23061173453181 (0 1 2 3 2 4 5 3 5 3 2 6 3 7 0 8 9 10 11 12 13))
> (run-25a)
0.91s (103.3741797061463 (0 1 2 3 2 4 5 3 5 3 2 6 3 7 0 8 9 10 11 12 13 14 15 1 2))
> (run-30a)
97.83s (158.038122553138 (0 1 2 3 2 4 5 3 5 3 2 6 3 7 0 8 9 10 1 11 12 13 14 1 2
  3 1 15 2 3))           
 
--Running a test on the far-right emacs window with the tables redefined so that
  dissim is now -log((1-sim)/2).  Seems to be running faster.

----------------git commit 68097e0--------------- 

22 Jul 15

--Far-right test with dissim=-log((1-sim)/2) runs faster, but gets pretty crappy
  results (bad labeling).  Think this might be a dead end, unless we can come up
  with a better way to change the dissim score.

--(run-35a) completed--results look good, except for that both box and chair
each get label 13 once, all other labels unique to an object
> (run-35a)
4751.28s (218.0955581668972 (0 1 2 3 2 4 5 3 5 3 2 6 3 7 0 8 9 10 1 11 12 13 14
1 2 3 1 15 2 3 13 14 7 0 4))

--Going to try doing a run-30a and run-30b where b starts at 12 and ends at 42
  so that a and b cover all of the dataset.  Then will try to sync the label
  sets to each other and find a common labeling--wondering if this might help
  with multiple labels.

--Trying run-30b with the back 30 elements of the matrix and it's been running 
  all day, while run-30a finished in < 2 minutes.  Not sure what's going
  on here, and not sure what I need to look at to try to fix it.  Is there
  something bogus going on in the data somewhere between 31 and 43?  How would I
  find that?  Left this running overnight to see if it eventually comes back.

--Need to think about how to restructure best-possible to run faster...

--Trying a seeded run of 43 (seeded from run-30a).  Got an answer after ~3.3
  hours.  Results saved in spreadsheet--still a few errors.

--Rewrote find-labels, score, new-score, new-best-possible,
  compute-max-score-vec, best-possible-i to speed up the computation.  Now
  getting the following results:
> (run-4)
0.03s (2.380576052665758 (0 1 2 3))
> (run-8)
0.03s (9.924843526466908 (0 1 2 3 2 4 5 3))
> (run-12)
0.03s (21.47483629609758 (0 1 2 3 2 4 5 3 5 3 2 6))
> (run-16)
0.03s (39.83369894630844 (0 1 2 3 2 4 5 3 5 3 2 6 3 1 0 7))
> (run-21)
2.81s (74.84826991034232 (0 1 2 3 2 4 5 3 5 3 2 6 3 6 0 4 7 0 1 1 7))

> (run-4a)
0.03s (2.380576052665758 (0 1 2 3))
> (run-8a)
0.03s (9.924843526466908 (0 1 2 3 2 4 5 3))
> (run-12a)
0.03s (21.47483629609758 (0 1 2 3 2 4 5 3 5 3 2 6))
> (run-16a)
0.05s (39.22651273787594 (0 1 2 3 2 4 5 3 5 3 2 6 3 7 0 8))
> (run-21a)
0.04s (70.23061173453178 (0 1 2 3 2 4 5 3 5 3 2 6 3 7 0 8 9 10 11 12 13))
> (run-25a)
0.04s (103.3741797061461 (0 1 2 3 2 4 5 3 5 3 2 6 3 7 0 8 9 10 11 12 13 14 15 1
2)) 
> (run-30a)
0.63s (158.0381225531381 (0 1 2 3 2 4 5 3 5 3 2 6 3 7 0 8 9 10 1 11 12 13 14 1 2
3 1 15 2 3)) 
> (run-35a)
19.32s (218.0955581668972 (0 1 2 3 2 4 5 3 5 3 2 6 3 7 0 8 9 10 1 11 12 13 14 1
2 3 1 15 2 3 13 14 7 0 4))

The above represents a ~100x increase in speed for both regular -a runs. Left
(run-25) and (run-43a) going overnight. 

----------------git commit dd9e025--------------- 

23 Jul 15

--(run-25) from last night finished
> (run-25)
1300.34s (111.6538437921642 (0 1 2 3 2 4 5 3 5 3 2 6 3 1 4 7 0 4 1 3 0 6 7 1 2))

--(run-30b) from yesterday finished (this was started before the revamp of the
  code yesterday).
> (run-30b)
70828.21s (176.8273516672281 (0 1 2 3 4 5 6 7 8 9 5 10 6 5 11 10 6 12 9 0 1 13
  13 14 5 12 14 10 15 13))

--(run-43a) still running as of 1615.  Guess the estimate was a bit off.

--Re-ran (run-30b) with updated code and got the same results MUCH faster
> (run-30b)
395.57s (176.8273516672281 (0 1 2 3 4 5 6 7 8 9 5 10 6 5 11 10 6 12 9 0 1 13 13 14 5 12 14 10 15 13))

--Looked at (run-30a) and (run-30b) and saw that there was no overlap on stool.
  Trying 35a&b--is there a way to ensure overlap?  Maybe shift by one and have
  many overlapping answers.  (run-35a) returns after ~20 seconds, but (run-35b)
  still running after multiple hours.  Can only guess that the reason that runs
  so much longer is because the data in that part of the matrix doesn't allow it
  to prune as effectively as the earlier data.

--Made the changes that Dan and I talked about yesterday in find-labels where we
  are running new-score twice--changes didn't have much of an effect (actually
  slowed down by a small amount).  Changing back, but left changes in the file,
  commented out. Actually figured out that it was something weird going on with
  seykhl; both ways of the code ran at roughly the same speed, but 25% slower
  than yesterday.  Ran the same code on chino and got yesterday's speeds.

--Trying a rolling overlap with the 30-peak sets (since that is the largest I've
  gotten the -b version to return with so far).  Called it rolling-overlap-30.
  When it finishes, need to look at the output and figure out how to sync the
  results back together.

----------------git commit b7a277f--------------- 

24 Jul 15

--(run-35b) returned overnight after ~15 hours of running.
> (run-35b)
53285.22s (231.0130697714877 (0 1 2 3 1 4 5 6 7 5 8 9 10 11 12 8 2 1 8 9 2 1 13
12 7 5 14 14 15 8 13 15 2 4 14))

--Talked with Dan about doing seeded full runs (probably seeded from 30 or 35)
  with wraparound overlap, then resorting results to original order and doing a
  binary decision on each pair in each run to determine if two peaks are of the
  same label or not.  Can then use these decisions as votes to determine an
  overall labeling (must have an odd number of runs)

--Re-ran a run to 43 seeded from 30-a and it returned in 42s.  I think the above
  idea might be feasible.3

--After talking with Jeff in meeting, decided to abandon the idea of multiple
  overlapping seeded runs.  Instead going to look at using color histograms as
  an additional measure of similarity in order to enhance similarity measure.

--Started work on step_4_build_similarity_matrix.m to output similarity matrix
  based on dense SIFT and color histograms.  Downloaded some color histogram
  code from
  http://www.mathworks.com/matlabcentral/fileexchange/43630-color-histogram-of-an-rgb-image
  to try out.

--Have first draft of step_4_build_similarity_matrix running--need to check/debug.

----------------git commit d373c5f--------------- 

27 Jul 15

--(run-43a) returned after ~89 hours
> (run-43a)
320073.34s (347.2546002514632 (0 1 2 3 2 4 5 3 5 3 2 6 3 7 0 8 9 10 1 11 12 13 14 1 2 3 1 11 2 3 13 14 9 0 4 4 15 1 10 15 2 7 4))

--Looking into using earth mover distance (emd option to phist2) instead of
  chi-squared distance on advice of Jeff.  Could probably use this directly as a
  dissimilarity measure, since it is bigger the greater the difference, and it
  is NOT limited to [0,1] range.  But then what to use for similarity measure?
  Maybe keep similarity as same chisq based dsift measure as before, and use emd
  with color histograms for dissimilarity...

--Run started on Friday errored because of typo in code.  Fixed and reran.
  After ~3 hours, still running--think it might be because of new color
  histograms.  Used 64 for nBins parameter, which gives 64^3-long histogram.
  Perhaps need to try with nBins at 16 (or 8).

--Got new similarity matrix to return after putting color histogram nBins to
  16.  Running find-labels on this gives bad (but FAST!) results on both regular
  and a runs. 
> (run-4)
0.03s (3.588659437122793 (0 0 0 1))
> (run-8)
0.04s (15.70878123818661 (0 0 0 1 0 0 2 1))
> (run-16)
0.03s (66.67347196219464 (0 0 0 1 0 0 2 1 2 1 0 0 1 0 0 3))
> (run-25)
0.13s (174.1102771188035 (0 0 0 1 0 0 2 1 2 1 0 0 1 0 0 3 0 0 0 1 4 0 0 0 0))
> (run-30)
0.42s (249.9530239584043 (0 0 0 1 0 0 2 1 2 1 0 0 1 0 0 3 0 0 0 1 4 0 0 0 0 1 0
  0 0 1)) 
> (run-35)
5.55s (344.0566675056085 (0 0 0 1 0 0 2 1 2 1 0 0 1 0 0 3 0 0 0 1 4 0 0 0 0 1 0
  0 0 1 0 0 0 0 0))
> (run-43)
2953.34s (521.6963659512988 (0 0 0 1 0 0 2 1 2 1 0 0 1 0 0 3 0 0 0 1 4 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0))
> (run-4a)
0.04s (3.588659437122793 (0 0 0 1))
> (run-8a)
0.04s (15.70878123818661 (0 0 0 1 0 0 2 1))
> (run-12a)
0.04s (35.44489266279697 (0 0 0 1 0 0 2 1 2 1 0 0))
> (run-16a)
0.04s (66.67347196219464 (0 0 0 1 0 0 2 1 2 1 0 0 1 0 0 3))
> (run-21a)
0.04s (120.3894684983797 (0 0 0 1 0 0 2 1 2 1 0 0 1 0 0 3 0 0 0 1 4))
> (run-25a)
0.06s (174.1102771188035 (0 0 0 1 0 0 2 1 2 1 0 0 1 0 0 3 0 0 0 1 4 0 0 0 0))
> (run-30a)
0.43s (249.9530239584043 (0 0 0 1 0 0 2 1 2 1 0 0 1 0 0 3 0 0 0 1 4 0 0 0 0 1 0
  0 0 1)) 
> (run-35a)
5.46s (344.0566675056085 (0 0 0 1 0 0 2 1 2 1 0 0 1 0 0 3 0 0 0 1 4 0 0 0 0 1 0
  0 0 1 0 0 0 0 0))
> (run-43a)
2873.03s (521.6963659512988 (0 0 0 1 0 0 2 1 2 1 0 0 1 0 0 3 0 0 0 1 4 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0) )

--Need to think about better/other ways to determine sim/dissim.  Looking at new
  asm in spreadsheet (asm_20150727a.mat), can see that the column max is almost
  always the right answer.  Dan suggested using a sigmoid function with a
  threshold, but must find threshold by plotting likes and unlikes to find if
  there is a gap.

--Trying sigmoid threshold (playing with values around 0.58, 0.65, 0.8) in
  build-tables, but getting really odd results--things that should be different
  are getting the same label.  As far as Dan and I can tell, we've got the right
  data in the tables.  But we still get the same answers as before implementing
  the sigmoid.  Tried new sigmoid functions with original similarity data and
  got the same results as before--don't think the sigmoid function is having the
  effect we think it is.

--Going to try a re-run of step_4_build_similarity_matrix that outputs the color
  histogram and dsift matrices individually and computed with both chi-squared
  and emd metrics.

--Think we have sigmoid issue figured out--now need to adjust the threshold.

----------------git commit 2c37dcd--------------- 

28 Jul 15

--Re-run of step_4_... finished after ~17 minutes.  Fixed a problem that the emd
  computations should have been using mins and ascending scores but were still
  using the maxes and descending scores that chisq computations used.  Matrix
  outputs make more sense now.

--Going to try to play with the m_factor parameter in step_4_...  This controls
  what percentage of the matches are used in the averaging.  asm_20150728a.mat
  has the output when m_factor is 0.5.  Going to try runs that vary that
  parameter both higher and lower to see the effect.  Have m_factor of 1, 0.75,
  0.5, 0.25 saved in asm_20150728b.mat.  Have m_factor of 0.2, 0.15, 0.1, 0.05
  saved in asm_20150728c.mat.  Need to look at these and see if there is a
  threshold between good and bad.

----------------git commit 608d1f5--------------- 

31 Jul 15

--Looked at histograms of good and bad matches with various descriptors and
  various measures.  Images stored in good-bad-comparison/.  Email traffic,
  notes: 
  Me to Jeff:
The images are in upplysingaoflun:/tmp/good-bad-comparison

Files are named in x-y-z.png format, where x is {chist, dsift} for the
type of descriptor (color histogram or dense SIFT), y is {chisq,emd}
for the distance metric (chi squared distance (truly it is 1-chisq, so
higher is better there) or earth mover distance (lower is better)),
and z is {100percent, top75percent, top50percent, top25percent,
top20percent, top15percent, top10percent, top05percent}, which
represents what percentage of the sorted image-to-image comparison
scores were used to compute the average group-to-group score.

Correct matches are plotted in green, bad matches are in blue, and the
overlap is shown in the white outline.  I just used equal number of
bins between both histograms (rather than equally-sized bins), but I
think these are still adequate to show the general idea.

  Jeff reply:
On Fri, Jul 31, 2015 at 5:07 PM, Jeffrey Mark Siskind <qobi@purdue.edu> wrote:
> dsift-chisq-15  0.52
> dsift-chisq-20  0.55
> dsift-chisq-25  0.48
> dsift-chisq-50  0.4
> dsift-chisq-75  0.36
> dsift-chisq-100 0.35

  Notes:
  --Look at color histogram finding tool--might be bogus
  --Visualize the color histograms of standard images (i.e., orange cone
    vs. black backpack) and should be able to see a difference 

--Get some help from Haonan with dsift/chisq since my code is based on his.
  Send him a simple data set and the code that does dsift and have him look to
  see if something looks bogus.

----------------git commit f222a46--------------- 

3 Aug 15

--Tried running color histograms with smaller numbers of bins (4^3 = 64), but
  results from both chist and emd metrics were bad.  Thinking that it might be
  because the histograms are cubic (bins on each axis of rgb 3-space) and then
  are vectorized before doing the metric--moves that should be high-cost are
  actually low-cost because of vectorization.
--Thinking that it might be better to write my own color histogram code that
  keeps each color channel (whether rgb, hsv, or Lab) separate, compares like
  channels, and then takes that vector of 3 differences as a point in 3-space.
  Points closer to the origin are evidence of more similar images.  Started
  work on Lab_histogram.m to implement this--looks like it works correctly.
  Compared 4 cone images (orange) to 4 bag images (black) and got histograms
  that looked like each other for each group and had significant differences
  between the groups.
--Also downloaded RGB2Lab and Lab2RGB from
  http://www.mathworks.com/matlabcentral/fileexchange/24009-rgb2lab.  Seems
  legit, author is from Berkeley--need it because our version of Matlab does not
  have it built in like newer versions.

----------------git commit aa2f7e5--------------- 

4 Aug 15

--Looked at colorAutoCorrelogram.m downloaded from
  http://www.mathworks.com/matlabcentral/fileexchange/42008-content-based-image-retrieval/content/ImageRetrieval/colorAutoCorrelogram.m.
  Not sure it will work--the output it gives for similar images is very
  different.

--Looking at Lab_histogram.m that I wrote yesterday and I think there might be
  something funky with it--all x values (bin centers) are between 0 and 1, while
  in Lab space, L is in [0,100] and a&b are in [-128,127] (??).  Also found
  makecform('srgb2lab') in matlab, which appears to do the same thing as
  RGB2Lab, but gives slightly different numbers on the same image (????) and
  runs 4x slower.  But might be more correct.

--Thinking that I need to write my own histogramming code--imhist appears to
  mess things up, but I think I can use hist and just specify my bin centers.

--Have own histogram code in Lab_histogram.m (fixed from yesterday) and
  histogram_distance.m.  Seems to be working correctly--running a new
  test_step_4 on chino, will have results in asm_20150804.mat by morning.

----------------git commit eb7190d--------------- 

5 Aug 15

--Plotted new good-bad output and saved files in
  good-bad-output/20150805-2nd-attempt.  Some look like crap, some look OK.  I
  think I'll be able to take some of this output from color histograms and
  sigmoid it to establish a threshold, then possibly log and apply branch and
  bound.  Could also do a separate sigmoid on the dense SIFT output (with
  separate threshold) and then do a weighted average of the sigmoid outputs from
  each to get the score that goes to B&B.

----------------git commit 91948d1--------------- 

11 Aug 15

--Retesting test_step_4 using 8 (vice 16) bins on color histogram to see if
  that's better than what I have.

--Need to play with sigmoid/thresholding to see if chist sim matrix will work.

12 Aug 15

--Have yesterday's test saved in asm_20150811.mat. Currently playing with
  sim_chist_chisq_3d_100_20150804.sc (from a week ago--yesterday's test didn't
  look much better).

--Looking at sigmoid function...need to use 1-sig (or negative slope parameter)
  to get values below threshold to have high score.

--Running another test_step_4 using 7x7 patches at the center of the images for
  the color histogram, instead of the full image.

--Found old sigmoids in toollib-codetection.sc and had to change them back to
  the old way since I'm now doing the sigmoid directly in labeling.sc.

--Tried runs using chist_chisq_3d_100 with *sigmoid-threshold* at 0.115 and
  *sigmoid-slope* at -10.  Got the following results:
> (run-4)
0.03s (2.056020340795548 (0 1 2 3))
> (run-8)
0.03s (6.821283434053237 (0 1 2 3 2 1 4 3))
(run-16)
0.04s (33.62578965133648 (0 1 2 3 4 1 5 6 5 6 2 7 6 1 0 7))
> (run-21)
0.40s (60.16798524830126 (0 1 2 3 2 1 4 5 4 5 2 4 5 1 0 4 6 0 1 3 7))
> (run-25)
40.07s (91.07636035728069 (0 0 1 2 1 3 4 5 4 5 1 4 5 0 6 4 6 3 0 2 7 4 4 0 3))
> (run-30)
265.85s (127.1473575161836 (0 0 1 2 1 3 4 5 4 5 1 4 5 0 6 4 6 3 0 2 7 4 4 0 3 5
0 3 1 2))
> (run-35)
4716.41s (190.7967865639224 (0 0 1 2 1 3 4 5 4 5 1 4 5 0 6 4 7 3 0 2 7 4 4 0 3 5
0 6 1 2 6 4 0 6 3)) 
> (run-4a)
0.04s (2.056020340795548 (0 1 2 3))
> (run-8a)
0.04s (6.821283434053237 (0 1 2 3 2 1 4 3))
> (run-12a)
0.04s (16.95972445924803 (0 1 2 3 4 1 5 6 5 6 2 5))
> (run-16a)
0.04s (33.62578965133648 (0 1 2 3 4 1 5 6 5 6 2 7 6 1 0 7))
> (run-21a)
0.19s (59.41375367317812 (0 1 2 3 4 1 5 6 5 6 2 7 6 1 8 7 9 0 1 10 11))
> (run-25a)
66.24s (89.6074815445682 (0 0 1 2 3 4 5 6 5 6 1 7 6 0 8 7 9 4 0 10 11 5 7 0 4))
> (run-30a)
837.72s (125.1368213834221 (0 0 1 2 3 4 5 6 5 6 1 7 6 0 8 7 9 10 0 11 12 5 7 0
10 6 0 10 1 2)) 
  Thinking about changing the threshold and/or the
  slope to see if that makes things run faster and/or more accurately.

--After talking with Dan, I think the slope might be OK.  After looking at
  good-bad plot for chist_chisq_3d_100, saw that threshold might be way too
  high. Trying with threshold set at 0.07; seems to be slowing things down quite
  a bit on regular runs but speeding them up on -a runs, and results don't look
  any better 
> (run-4)
0.04s (1.430428594060378 (0 1 2 3))
> (run-8)
0.04s (5.541400846345208 (0 1 2 3 2 1 4 5))
> (run-12)
0.04s (13.6042161870225 (0 1 2 3 4 1 5 6 5 6 2 0))
> (run-16)
0.06s (26.93780537792364 (0 1 2 3 4 1 5 6 5 6 2 7 6 1 0 7))
> (run-21)
44.17s (49.61792579036732 (0 1 2 3 2 1 4 5 4 5 2 6 5 1 0 6 7 0 1 3 7))
> (run-25)
2165.51s (76.08336475145053 (0 0 1 2 1 3 4 5 4 5 1 6 5 0 1 6 7 3 0 2 7 4 6 0 3))
> (run-4a)
0.07s (1.430428594060378 (0 1 2 3))
> (run-8a)
0.04s (4.969154522102971 (0 1 2 3 4 5 6 7))
> (run-12a)
0.04s (12.62891599410311 (0 1 2 3 4 5 6 7 6 7 8 9))
> (run-16a)
0.05s (25.14751140899375 (0 1 2 3 4 5 6 7 6 7 8 9 10 1 11 9))
> (run-21a)
0.05s (43.429241725945 (0 1 2 3 4 5 6 7 6 7 8 9 10 1 11 9 12 13 0 14 15))
> (run-25a)
0.48s (67.00190790901938 (0 1 2 3 4 5 6 7 6 7 8 9 10 1 11 9 12 5 0 13 14 6 15 1
  2)) 
> (run-30a)
87.47s (94.66108008825046 (0 1 2 3 4 1 5 6 5 6 7 8 9 1 0 8 10 11 12 13 14 5 8 15
  11 6 15 12 2 3)) 

--Things to try:
	 1. Histograms of center patch of images (asm's computing on chino now)
	 2. Some sort of linear combination between chist and dsift data (should
	 be relatively straightforward to do)
	 3. Image segmentation (ala 637 Lab 3).  
	    -Need to do some thinking and coding on this--637 lab used grayscale
     	     images, so these are different--maybe segment them in grayscale and
	     then use those pixel locations for a color histogram??  
	    -637 code is in C, not sure how to call it from MATLAB, or if I
 	     should  have some separate C code that does segmentation and then
 	     outputs a masked color image
	    -Background subtraction?  Would that work?  Not in the traditional
	 sense b/c I'm not working with a stream of images with a fixed
	 background.  But could do some sort of segmentation and then look for
	 the largest area...

----------------git commit dcbf763--------------- 

13 Aug 15

--First look at new plots from chino is BAD.  Not sure if I did them right or
  not.

--Working on a filtering of the images at the end of step 3.  Main file is
  filter_image_test.m.  Basic idea is to take all images that were clustered
  together at the end of step 3, find the histograms (either dsift or chist) of
  each, find the mean histogram, find the distance (either chisq or emd) between
  each image histogram and the mean histogram, and then reject images which are
  above a certain threshold (currently using the mean+0.5*std of the
  DISTANCES).  Looks like it might do some good.  Rough script to run it is
  run_filter_post_step_3.m.  Think it is working now.

----------------git commit 3bc6f96--------------- 

14 Aug 15

--Did good/bad plots of old test_step_4 run on the newly filtered images.  Saved
  it in good-bad-comparison/20150814-8bin_chist-center_patch-self_filtering.
  Nothing looks any better than previous stuff.  Need to try something
  different.

--Talking with Jeff--after testing the color histogram on sets of randomly
  chosen images from each class, look at how the groups are compared. 
  Currently making matrix of image-to-image comparisons between each class and
  then doing things with row/column means/maxes/mins/etc.  Instead, forget the
  matrix and just rank order all the image-to-image distance scores and take
  the top X percent, and average that as the similarity between the two groups.

--Ran 15 iterations of the test of color histogram distance (measured both with
  chi squared and earth mover distance, and both using the full image and a 7x7
  patch in the center) for 3 random images from each of the 8 classes. (file is
  histogram_testing.m) 

--Results are in source/sentence-codetection/color-histogram-test-results.  The
  leading number of the filename is the iteration number, and the rest of it
  should be self-explanatory.  I also saved the random images themselves: by
  row, they are: chair, bag-book, table, cone, stool, box, bag-shop, junk. 

--I think what the results show is that the quality of the match within or the
  difference between classes is highly dependent upon which images are randomly
  chosen.  What I'm not entirely sure of is how this will affect the new method
  of finding similarity between groups of images that may or may not be of the
  same class (mentioned above).

--It also looks like using the small patch in the center leads to higher
  variance among the scores--which makes sense since a small patch is more
  likely to contain fewer colors.  But what I see looking at the images is that
  the primary object isn't always centered in the proposal image, so sometimes
  we end up getting a background color instead.  I've thought about sampling the
  image from more than just the center, for example taking a small set of
  patches (perhaps 5 in a 'x' or '+' shaped pattern), but I'm not sure if that
  will cause more harm than good.  

----------------git commit 9b97adb--------------- 

16 Aug 15

--Implemented idea of using top N percent (currently 30) of color histogram
  distance matches between groups as similarity measure in
  step_4_chist_similarity_matrix.m.  Runs fast, ~2 min.  Did good-bad plots and
  they don't look particularly good.  But looking at the matrices for
  chisq3d_patch and chisq3d_full in sim_chisq3d_patch_20150816.ods and
  sim_chisq3d_full_20150816.ods, it looks like the lowest score in the col/row
  is usually the right one--thinking about taking the lowest score
  (non-diagonal) in each row/col and using the mean+1stddev to set the
  threshold.  Could I put the diagonal element as inf in matlab?

--Ran labeling on sim_chisq3d_patch_20150816.sc and got results rather quickly,
  although they weren't very accurate:
> (run-4)
0.04s (0.03859569172637077 (0 0 1 2))
> (run-8)
0.04s (2.767485488513134 (0 0 1 2 3 0 4 5))
> (run-16)
0.16s (22.08477957617687 (0 0 1 2 3 4 5 2 5 6 7 4 7 5 5 5))
> (run-21)
0.28s (30.46407991549173 (0 0 1 2 3 4 5 2 5 6 1 4 7 5 5 5 5 7 0 2 5))
> (run-25)
2.04s (40.58235334697235 (0 0 1 2 3 0 4 2 4 5 1 6 7 6 4 4 4 7 0 2 4 4 0 6 6))
> (run-4a)
0.04s (0.03859569172637077 (0 0 1 2))
> (run-8a)
0.05s (0.6644189522770908 (0 0 1 2 3 4 5 6))
> (run-12a)
0.05s (2.653916115874833 (0 0 1 2 3 4 5 6 5 7 8 4))
> (run-16a)
0.05s (8.543723506302884 (0 0 1 2 3 4 5 6 5 7 8 4 9 5 5 5))
> (run-21a)
0.05s (18.61595573604413 (0 0 1 2 3 4 5 6 5 7 8 4 9 5 5 5 5 9 0 6 5))
> (run-25a)
0.21s (27.34618418803512 (0 0 1 2 3 4 5 6 5 7 8 9 10 9 5 5 5 10 0 6 5 5 0 9 9))
> (run-30a)
0.95s (38.81863686360955 (0 0 1 2 3 4 5 6 5 7 8 9 10 9 5 5 5 10 0 2 5 5 0 9 9 6
  0 4 11 2)) 
> (run-35a)
154.31s (65.52725106135911 (0 0 1 2 3 4 5 6 5 7 8 9 10 9 5 5 5 10 0 2 5 5 0 9 9
  6 0 4 11 2 0 0 10 10 0))

--Above labels a lot of stuff as 0, probably because the things getting labeled
  zero often have similar/same color (i.e., floor) at center.  Maybe try
  something different with patches-->do a sampling of patches (maybe a 3x3 box
  of patches) and consolidate that into a sampled image--might have better luck
  at finding things that are similarly shaped by that method.

--Also, still need to try labeling against the sim_chisq3d_full_20150816.sc
  data.

----------------git commit 6c941a4---------------

17 Aug 15

--Implemented grid of patches in step_4_chist_similarity_matrix.m.  Results
  don't look very good--everything has low distance when it shouldn't.

--Also thinking about trying some color-based segmentation (see
  http://www.mathworks.com/help/images/examples/color-based-segmentation-using-k-means-clustering.html
  and http://www.vlfeat.org/matlab/vl_kmeans.html), but haven't been able to try
  this yet.

--Jeff suggests redoing the dataset with a better color balance on the camera,
  in an environment where we can control lighting conditions, and using objects
  (like blue trashcan and yellow Lego bag) that have more color variation.  Need
  to investigate this further after RAL/ICRA paper done.

--LOOK at Learning OpenCV, Chapter 7, Histograms and Matching.

--Stopping new experimentation to write up current progress as a partial result
  for RAL/ICRA.

--Changed labeling.sc back to using asm_20150702.sc for data and trying (run-25)
  to see if results are same as in notes above.

----------------git commit d79d91e---------------

21 Aug 15

--For ralicra2016, created new dataset in
  /aux/sbroniko/vader-rover/logs/MSEE1-dataset/ralicra2016 with 6 floorplans (30
  objects, should be 25 unique instances if step 3 is perfect)

--Made a few mods in the source code and re-ran step 3; got 27/30 objects detected.

----------------git commit 8a33324---------------

22 Aug 15

--For ralicra2016, ran new step 4 and labeling.  Got decent results from
  (run-26a), still waiting for (run-26) to finish.
> (run-4a)
0.03s (2.348462312758173 (0 1 2 3))
> (run-9a)
0.03s (14.18919756335641 (0 1 2 3 4 5 1 2 6))
> (run-12a)
0.03s (27.59512694270645 (0 1 2 3 4 5 1 2 6 1 7 2))
> (run-16a)
0.03s (48.331306478022 (0 1 2 3 4 5 1 2 3 1 6 2 3 7 8 9))
> (run-20a)
0.11s (78.34933026745459 (0 1 2 3 4 5 1 2 3 1 6 2 3 7 8 9 0 10 11 5))
> (run-26a)
1.06s (139.541932634983 (0 1 2 3 4 5 1 2 3 1 6 2 3 7 5 8 0 9 9 10 1 11 10 2 8 9))
> (run-4)
0.05s (2.348462312758173 (0 1 2 3))
> (run-9)
0.05s (12.66365265366496 (0 1 2 3 4 2 1 2))
> (run-12)
0.05s (29.09886766872226 (0 1 2 3 4 4 1 2 3 1 0 2))
> (run-16)
0.50s (52.88228269272211 (0 0 1 2 3 4 0 1 2 0 3 1 2 3 4 1))
> (run-20)
8.32s (83.7273487405632 (0 1 2 3 4 5 1 2 3 1 0 2 3 4 5 4 0 5 1 1))

----------------git commit 3fb62a8---------------

23 Aug 15

--(run-26) finished with relatively good results after ~2 hours
> (run-26)
7486.17s (149.3931060362158 (0 1 2 3 4 5 1 2 3 1 0 2 3 4 5 4 0 5 5 1 1 0 1 2 2
5)) 

--Also adding all .mat files that are small enough to github.

----------------git commit 507451f---------------

23 Aug 15 (2)

--Found error I made in the collection of the ground truth data yesterday, so
  had to redo all localization error calculations--much better now.  Resaved
  data in .sc and .mat files.  Also put data from /aux/.../ralicra2016 into
  source/sentence-codetection/ralicra2016-data 

----------------git commit 02a2d17---------------

24 Aug 15

--Commit to catch any last changes on the ralicra2016 stuff.

----------------git commit d285512---------------

14 Sep 15

--Restarting work because ralicra2016 and tro2015 are submitted.

--Brainstorming ideas for how to collect a better dataset to try using color
  histograms on.

--Biggest problem seemed to be variability of lighting--need a location that has
  consistent lighting--thinking about using the gym at Burnett Creek (no
  windows, basketball/volleball/etc courts laid out on floor to use as markers,
  probably has some decent props available to us--cones, balls, etc.)

--Dan and I previously had an idea to program the robot to do an autonomous
  search for objects--probably only have to input the robot's starting location
  and the boundaries, then can drive a search grid computed from that.  Could we
  incorporate this into the new dataset eventually?

--First will need to do some small trial runs at BCE to see if the video comes
  out OK there--also might need to recheck the white balance of the camera
  (believe that is adjusted in the FlyCap API, so will need to get into that)

--To use the rover off campus, will need to change the network infrastructure
  since PAL3.0 does not exist off campus.  Should be able to use a cheap 802.11n
  router (open encryption?) to get the data from the rover back to the laptop.
  Can probably plug the laptop into the router wired so that the rover is the
  only thing using wifi.  Do we have a router available here?

30 Sep 15

--Catching changes made after 24 Aug 15...not sure all of what they are

----------------git commit 39dddc6---------------

16 Oct 15

--Starting back on next increment for research.  Need to think of something that
  either unifies tro2015 and ralicra2016 or goes beyond them.

--3 off-the-cuff ideas
    1. Automatically train class-specific object detectors using images
       collected from drives (original idea from prelim)
  	    -Create a corpus of labeled images from the automatically labeled
             images from the previous section. 
	    -Use this corpus to train a pre-existing object detector without
             human intervention. 
	    -Then use this purpose-trained object detector as a replacement for
             the generic object detector (MCG). 
    2. Can we use color histograms to learn color words?  Conceptually, pretty
       much the same as how we learned nouns and prepositions for tro2015.
    3. Can the robot automatically discover its environment?
       	    -Easy to program the robot to drive a discovery pattern in an
             unknown environment. 
	    -Hard part is doing obstacle avoidance in an on-line manner.

--Have test video in BCE gym with different-colored objects (i.e., red cone,
  blue cone, etc.).  Data is in
  /aux/sbroniko/vader-rover/logs/BCE-test-run-30sep15.  Has controlled lighting,
  which is good.  Run is >9min long, so need to look at how much the
  localization drifts over such a long run.

--Dan had idea of using the regular pattern on the ceiling to help with
  localization--probably would just be another input to the Kalman Filter.
  Would probably need to calibrate the pano cam in order to try this.  Is it
  worth the effort?

--A little more fleshing out of idea 1 above:
    a. Start with randomly generated floor plans (on new basketball court?) and
    either randomly or AMT generated sentences describing paths on those floor
    plans.
    b. Manually drive those paths to collect data.
    c. Run data through ralicra2016 system to get automatically-detected floor
    plans. Also use this system to get cropped images of detected objects.
    	   -*THOUGHT* Should we add color histograms to this system b/c of new
	   objects? 
    d. Use auto-floorplans along with sentences and traces as input to acquisition
    system from tro2015.  This gives us object names to go with images.
    	   -*THOUGHT* Could we combine ideas 1 and 2 above by using color
	   histograms here to learn color words?  Would that be interesting?
    e. Could also do generation and comprehension from the new models learned in
    acquisition...not entirely sure what we would gain by doing this.
    f. At this point, branch off into something new-->use object images from c
    and object names from d to train some off-the-shelf object detector (which
    one??).  This is all automatic from b onward.
    g. This is pretty much as far as I've thought it through.  Some problems I
    see right now:
    	-What would we do with this automatically trained object detector?  
	-How could we test its performance?  
	-If we used color histograms to integrate color, would we be able to
         deal with colors in a reasonable way (i.e., would blue cone and red
         cone be 2 completely different object models, or would we have
         independent structure and color models?)  A lot will depend on the
         off-the-shelf object detector we choose.

--Some fleshing out of idea 3 above:
    a. Could we get around the obstacle avoidance problem by letting the rover
    just bump into objects and then react to the bump switches differently?
    That's pretty much what a Roomba does.  One problem I see is that most of
    the objects are light enough that they would move when bumped by the rover.
    Some (buckets, boxes) would be easy to weight down, but others (balls,
    cones, stools) wouldn't.

----------------git commit ab6af0b---------------

26 Oct 15

--Have rejection of ralicra2016 paper from RAL.  ICRA decision still pending.
  Need to go through RAL reviews carefully and figure out what needs to be done
  to address the issues.  Then we can estimate whether or not we can fit the
  changes into the page limit for RSS (28 Jan, 8 pages + refs) or IROS (1 Mar,
  length limit not yet announced).  If not, target to journal.

--Need to fix sync of cameras to sensors on robot using pthread barriers.  I
  think the infrastructure is already there, just need to put a time limit on
  the camera thread so that we get images at EXACTLY 10 fps/camera.  Then the
  sensor data coming in at 50 Hz should be an exact factor of 5 more frequent,
  so downsampling will be simple.  Need to get this working before any other
  potential experiments.  

--LIDAR idea (email to Jeff, 22 Oct) on hold for now until we figure out what
  needs to be done to fix the ralicra paper.  
  ---------------text of email to Jeff, 22 Oct--------------------------
  Dan and I were talking before he left about the best way to do range finding,
  and we agree that trying to mount the range finder on top of the camera is
  probably a bad idea, since the scanning we'd have to do with the range finder
  would lead to motion blur on the images.

  What we came up with instead is mounting the range finder on its own servo on
  a probe that would stick out from the front of the rover, probably bolted to
  the undercarriage.  We should be able to get everything we need for this
  off-the-shelf, with the exception of the metal probe which will mount the
  range finder to the rover.  But this will be simple enough (probably a 1/4"
  thick aluminum bar with holes for the servo and bolts drilled/cut) that I
  think the machine shop downstairs can make it easily.  And until we have the
  servo and sensor mated, it will be hard to judge how far out from the front of
  the rover it needs to be.

  As I was researching the ultrasonic sensors, I found out that most of them in
  the $20-50 price range (including the one we have in the lab) use an RS485
  data bus for communication.  The reviews and tutorials I've read say that
  RS485 can be a pain to work with.  That, combined with the fact that most
  ultrasonics have a beam width of around 60 degrees, led me to start looking at
  laser range finders instead.

  I found a laser range finder on Robot Shop that was only $115 and had good
  reviews.  It can interface digitally to a Teensy and they already have some
  sample code to do this, so getting it up and running should be
  straightforward.  I have extra Teensy boards on hand, so my thought was to add
  a second Teensy to the rover to control the range finder and its servo, and
  then the Teensy will report the data back to the Gumstix via USB.  The
  attached spreadsheet has the items I'd like to order, along with the links.
  The grand total is $370.40, but that includes spares.  If we need to trim the
  price, we can ditch the spares.

  Please let me know what you think.
  ---------------end text of email to Jeff, 22 Oct--------------------------

27 Oct 15

--Starting work on putting barriers around camera task on rover.  Have found
  where to change code in the_force and that should be easy.  However, must also
  put barriers into manual mode (emperor) which will be harder since there is no
  existing barrier code there.

--Talking to Dan about automatic search mode (new mode--name idea: Padawan) and
  he came up with a good idea.  I had thought that the mode would get a single
  point from the laptop and then drive that point until the laptop sent another
  point (or the rover got to the first point).  However, Dan pointed out that
  that could be bad, since a disconnection will keep the rover going even if
  stuff is in the way.  So Dan's idea is to have the laptop keep sending points
  to the rover at regular intervals and have the rover stop if it stops getting
  points. 

5 Nov 15

--**THOUGHT** if we use different scenes, I will need to go back into the
    codetection code (proposals_and_similarity?) to remove the hard-coding of
    the boundaries--probably should do this as an input to the function

----------------git commit 816d0ec---------------

6 Nov 15

--***LOOK FOR odometry calibration procedure.  Need to calibrate for
  carpet.-->FOUND by talking to Dan: Odometry calibration procedure is to
  manually drive to known measured points on floor, marking them in the log file
  (should be a button in driver-gui, if not look at old versions in git).  Then
  Dan has code that optimizes the parameters (rover width, R and L ticks/cm) so
  that estimated locations match measured locations.  Should be able to do this
  on each floor surface we use.

10 Nov 15

--Thinking about what changes will need to be made to codetection code to remove
  hacks/penalties.  
    -Should be able to remove all modifications to the unary score and then just
     use the proposal score (maybe keep the on-the-ground assumption, and
     explain better in paper).   
    -Want to KEEP the size/location criteria for the binary scores, since those
     seem well-principled.  
    -**INTEGRATE Jeff's idea of using the location match as a multiplier to the
     SIFT and size matches

--Could we make it run faster by limiting the connectivity of the first
  GM...maybe only have a frame have connections to the 5-10 frames on either
  side of it?   Or is the slowdown in the proposal generation?  Need to look at
  this. 

--Need to get a better measurement of localization drift and report it.  Dan may
  have data from previous measurement somewhere, but might be easier/better to
  do new measurement.

--Some ideas Dan and I had while brainstorming ~5 Nov for ralicra revision:
  --Find a different image feature detector that outputs a
    fixed-length vector which we can use to replace or compare against the
    SIFT features that we currently use
  --Find some new objects that are both NOT rotationally symmetric and
    NOT toys.  Some ideas we have:
     --Dummy tank round I have at home (not rotationally symmetric if
       we lay it on its side)
     --Black trash bag with box inside (this and the tank round are
       very good simulators for IEDs)
     --Large trash can (I have one at home that is semicircular and
       thus not rotationally symmetric)
     --Cars
  --Different scenes:
     --school gym
     --parking garage or lot (this is where we would have cars as
       objects)--Dan and I had talked about using the parking garage here on 
       campus, but I also just realized that the parking lot at Kara's school 
       would be pretty deserted on a weekend, which would give us more
       control over where objects are located
     --MSEE (??--not sure if we want to use this one again or not)
     --others
  --Collect ~2 different floor plans (~5 objects and ~25 runs on each)
    in each of ~3 different scenes.  This will give us ~30 object
    instances (peaks), which we already know is near the limit of what we
    can label in a reasonable amount of time using branch and bound.
    Could possibly extend this number by having more instances of an
    object class within a floor plan, but would have to make sure that the
    codetection step that finds the like objects within a single floor
    plan can catch these in order to keep the number of objects to label
    across floor plans down.
  --Collect training and testing data sets on each scene.  This would
    be useful if we were to take the collected images to train an object
    detector, and then test that object detector against unseen data from
    the different scenes.
  --(not necessarily for this ralicra revision, but could be if we go
    to a longer format) Collect a data set with both differently colored
    objects (i.e., blue cone and green cone) as well as different objects
    of same color (i.e., blue cone and blue ball).  This data would let us
    try to learn adjectives (colors) along with prepositions and nouns.

--Put in a citation to Gaussian Membership Functions in ralicra--popular in
  fuzzy logic literature. (Neuro-Fuzzy and Soft Computing book or Semantic
  constraints for membership function optimization paper)

--**THOUGHTS** on changes for ralicra
  --Try doing binary scores with NO feature detector (just location + width) as
    a comparison of PHOW SIFT stuff.
  --For judging correctness: put all cropped images that the system groups
    together on AMT; have users select which images are different from the
    others (all at once or in smaller groups?); take error as #/% of images
    selected. 

--Created ~/codetection/source/new-sentence-codetection as workspace for changes
  to codetection code.  Deleted a lot of old data (which is still saved in
  ../sentence-codetection) and generally cleaned up.  Will start modifying this
  for new ideas so that an old working version still exists.

--More discussion with Dan on new experiments.  Came up with the following:
  -Quantitative comparisons
   -train object detector (Felzenswalb or Dalal & Triggs, have code for both)
    with automatic output; compare against detector trained with hand boxes
    (Haonan has AMT code for this) and/or detector trained with hand
    categorization to compare automatic to human performance
   -Get AMT judgments on proposal labeling (second bullet of **THOUGHTS** above) 
  -Could also try using color histograms in place of SIFT for subsystem
   breakdown (1st bullet of **THOUGHTS**)

----------------git commit 8dcfaf6---------------

11 Nov 15

--Email traffic w/Jeff and group about new experiments:
On Wed, Nov 11, 2015 at 8:50 AM, Jeffrey Mark Siskind <qobi@purdue.edu> wrote:
> This is just a thought. I'm trying to think of what would make our experiments
> for robotic codetection come across as more realistic. Our current framework
> of placing arbitrary objects in an open area is inherently unrealistic. And I
> think it might subconsciously bother the reviewers even if they don't
> explicitly mention it. It would be more natural, if our robot drove around
> some non-open area and encountered the objects that naturally occurred, or at
> least could naturally occur, in that setting.
>
> For example, we could drive around the living room/dining room area of several
> different homes. Or the bedroom/bathroom area of several different homes. That
> would be compelling because it would imply a story, even if we didn't mention
> it explicitly, that our method could eventually be useful for home robotics.
>
> For another example, we could drive around several different office
> environments. Beside EE and MSEE, we might be able to get access to other
> buildings and offices at Purdue. We might ask if we could gather data while
> driving around elementary school hallways and classrooms after hours, in
> return for offering to give a guest science-class demo during school hours.
>
> There are office parks where people can rent office space on a short-term
> basis. There used to be such a small office building in West Lafayette across
> the street from Einstein's Bagels, next to the West Lafayette Library. the
> building is now frontier.com so I don't know if there still is rentable office
> space there. But I am sure that there is such in Indy or Chicago. And we could
> probably cut a deal to rent several adjacent offices for one day for perhaps a
> few hundred or maybe a thousand dollars. There are probably many such in Indy
> or Chicago so we could get a variety. Just driving around different desks,
> chairs, file cabinets, conference tables, and the like, in different offices
> with different doors and different hallways.
>
> Generally, this would be far more compelling. Even if there were only a small
> number of object classes, like chairs, desks, tables, file cabinets, lamps,
> kitchen cabinets, garbage pails, doors, radiators, and electrical outlets, we
> could get a wide variety of instances that exhibit natural variation.
>
> We would have to make sure that our method worked before we expended the
> effort of gathering the data. So we would do a small pilot first, maybe even a
> succession of incrementally larger pilots.
>
> I think that if we had a dozen different such environments (say six home and
> six office), where there was just a single 'floorplan' in each such
> environment, that is the natural arrangement of objects in that environment,
> and we drove a few dozen paths in each such environment, we would have a far
> more compelling experiment.
>
>     Jeff (http://engineering.purdue.edu/~qobi)

On Wed, Nov 11, 2015 at 9:01 AM, Jeffrey Mark Siskind
<jeffrey.mark.siskind@gmail.com> wrote:
> One thing that is easy to do is rent a night at a dozen different hotels. We
> could drive around the hotel room. We might, if we were suitably politic, be
> able to drive around the lobby, hallways, gym, pool, breakfast area, etc.
>
> I write this as I am sitting in a hotel room and realize that it would be
> ideal. A dozen hotel rooms at about $100/night each.
>
>     Jeff (http://engineering.purdue.edu/~qobi)

On Wed, Nov 11, 2015 at 9:47 AM, Jeffrey Mark Siskind
<jeffrey.mark.siskind@gmail.com> wrote:
> Also, it may be possible to rent 'function rooms' at hotels, conference
> centers, community centers, churches, libraries, etc.
>
>     Jeff (http://engineering.purdue.edu/~qobi)

Yes to all of this!  I think the hotel rooms are probably our best
bet.  There we will get a variety of instances of household objects
that are of the same class but have different appearance.  I think the
only prior knowledge we would need would be the total number of object
classes (which we already need as input) to have similar objects
grouped together for labeling on the basis of size and shape rather
than exact appearance.  It might require some changes to the second
GM, but it might not.

One thought that comes to mind is that we should probably try to use
suites or handicapped-accessible rooms (or even better,
handicapped-accessible suites) to give the rover more room to maneuver
and get decent looks at the objects.  Some regular rooms could be so
tightly packed that the rover wouldn't have room to drive or see much
at all.  The ADA-compliant rooms should even have a bathroom big
enough for us to drive into.

On Wed, Nov 11, 2015 at 10:55 AM, Jeffrey Mark Siskind <qobi@purdue.edu> wrote:
> For a sample, I took a video at the Hilton Garden Inn room I was staying in in
> Boston on my cell phone. I'll forward it later. I think it was
> handicapped accessible, but not a suite. It was definitely big enough to drive
> around in, even in the bathroom.
>
> There are lots of hotel chains that specialize in suites. Homewood Suites,
> Marriott Courtyard, Fairfield Inn & Suites, Residence Inn, Extended Stay
> America, ... Some have multiple bedrooms in addition to a living room.
>
>     Jeff (http://engineering.purdue.edu/~qobi)

Yeah, a room that size should work.  Looks like there's enough room to
get the rover around and get good looks at the furniture.  The desk
and wheeled chair might cause problems because there's not a lot of
substance to them at ground level, but we already knew about that
issue.  All of the other furniture looks like good candidates to be
picked up by the proposal generator.

One thought--would we want to have a few objects that we bring
ourselves so that they are identical in each room?  Two obvious
candidates that pop to mind are a trashcan and a suitcase.  I'm not
sure if doing this would help, hurt, or not make a difference.

On Wed, Nov 11, 2015 at 1:35 PM, Daniel Barrett <dpbarret@purdue.edu> wrote:
> I've been checking tcsvt daily or more. It still says
>
> I got ratslam to run on our video and odometry from Scott's school gym test
> and got complete garbage. I'm checking to make sure I didn't screw up the
> input format somehow, but it doesn't look promising. 
>
> Before I switched to ratslam I was looking into how I would insert our
> odometry into lsdslam, and since it works by adding a bunch of pairwise
> constraints between camera poses to a graphical model solver, I should be able 
> to add my own constraints according to the odometry, or if we decided to go
> that way, according to language somehow. Another thing about lsdslam is that
> the version they released is from 2014, and uses a pinhole camera model along
> with radial distortion correction to try to handle fisheye cameras, but that
> isn't good enough to handle a 180 degree FOV. They have a more recent paper
> from 2015 which addresses this with a different camera model. The videos
> showing results from the newer paper are even more impressive. I was thinking
> of either asking them if they could give me the new code or possibly trying to
> make the camera model change in the released code. Or I could just crop our
> pano cam images so that they only include the middle portion which can be
> reasonably corrected, since it sort of works as is from the pano cam, minus
> the occasional loss of tracking, which the odometry should account for. 
>
> If we decide to use hotel rooms, this is probably all unnecessary, although if
> we want to navigate multiple rooms or hallways it might still be.


Open files in Emacs 11Nov15
. * *shell*<3>          483161  Shell:run	  ~/codetection/
    notes.text          193092  Text		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/notes.text
    codetection-test.sc  66263  Scheme		  ~/codetection/source/sentence-codetection/codetection-test.sc
  * *shell*<2>          463653  Shell:run	  ~/Dropbox/grad school stuff/Purdue/research/1_SLAM/
    today.text          120740  Text		  ~/vader-rover/today.text
  * *shell*             424584  Shell:run	  /etc/
    output.text          11074  Text		  /tmp/output.text
    scott_proposals_onl: 16552  MATLAB		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/sentence-codetection/scott_proposals_only.m
    proposals_and_simila: 3956  MATLAB		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/sentence-codetection/proposals_and_similarity.m
    out_camera_data_25p: 44039  nXML Valid	  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/camera_calibration/out_camera_data_25ppms.xml
    toollib-codetection: 18999  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/sentence-codetection/toollib-codetection.sc
    ralicra2016-labeling: 7261  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/sentence-codetection/ralicra2016-labeling.sc
    toollib-misc.sc      44479  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/darpa-collaboration/ideas/toollib-misc.sc
    table-test.sc          898  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/sentence-codetection/table-test.sc
    toollib-matlab.sc    12100  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/darpa-collaboration/ideas/toollib-matlab.sc
    QobiScheme-AD.sc    295402  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/QobiScheme/source/QobiScheme-AD.sc

***REMEMBER to change symlink for toollib-codetection.sc in
   ~/darpa-collaboration/ideas-->change to new toollib-codetection.sc

13 Nov 15

--Collected 34 runs in 4 different rooms (dining, kitchen, living, master bed)
  in my house yesterday.  Saved on seykhl in
  /aux/sbroniko/vader-rover/logs/house-test-12nov15. 

--Next step is to see if runs are accurate enough to use.
  -Reconstruct trace from odometry and look at it to see if it makes sense
  -Do this for all runs, and then stitch front, pano, and trace together into
   single video
  -Watch these to see if the traces make sense with the videos

--Only after this shows good results is it worthwhile to start trying the (1st
  round) graphical model on the videos.


14 Nov 15

--Continuing to work on automatic stitching together of videos with traces.

--Automatic stitching code completed in rendering-code.sc.  Have written
  partial-wrapper to run the joining of images and then the making of images
  into videos.  Seems to be taking a while--probably something screwy with my
  code, but seems to be running correctly.  Need to check videos once
  partial-wrapper completes.

----------------git commit aa233b7---------------

16 Nov 15

--Looking at plots, it seems to be that the x and y axes are not scaled
  equally.  X axis appears to have a wider range than the y axis.  Need to fix
  this in future plotting runs--both axes should be scaled to the larger of the
  two ranges. **Think this is fixed in animate-house-tracks**

--Finished fixes in rendering-code.sc for plotting.  Appears to work now.
  Redoing the run of stitching videos together to get a better look.

***STILL NEED TO THINK ABOUT restructuring the above stuff so that it instead
   does all steps on a single video in one shot, then doing a wrapper that sends
   it multiple videos to do sequentially.  Don't forget to clean up as well.

----------------git commit 6456a07---------------

16 Nov 15 (2)

--Found bug in partial-wrapper, should be fixed.

----------------git commit 1c09c9c---------------

17 Nov 15

--Copied house-test-12nov15/ to all workstations (minus mohio) and servers in
  preparation for running stage 1 of the codetection.

--Finished re-run of stitched files.  Need to do a more thorough look, but it
  appears that traces are good as long as the path stays short.  Longer paths
  have drift issues (which we already knew), but slipping when trying to turn on
  carpet does not appear to be an issue-->the plot turns in sync with the video.

--Starting work on changing code in new-sentence-codetection to run new house
  runs.  Have all new functions below a dated comment line in
  codetection-test.sc. Also commented out all f-score penalties (except for
  behind camera/not on floor penalty) in scott_propsals_similarity2.m.

--Running test of single video on aruco to make sure it works, then will run all
  videos in parallel on multiple servers.

--Found bug in MATLAB startup.m (~/Documents/MATLAB) and fixed it so that old
  sentence-codetection was not added to matlab path at startup.

----------------git commit 798af84---------------

21 Nov 15

--have visualize-proposals working to draw all proposals on frames.

--working on new-animate-tracks-with-boxes to draw trace with proposal/result
  boxes

--main wrapper to do all such for a single path/run is make-4-by-video-house-one-run

23 Nov 15

--Fixed visualize-proposals so that the colors of the boxes are scaled to the
  range of the run's mean score +/- 1 std-dev.  Looks better now

--Need to think more about how to encode the different data we have as part of
  the binary score between two proposals--see email from Jeff, 22 Nov.

--Working on new-animate-tracks-with-boxes to make images of traces with all and
  selected proposals at the same time

----------------git commit 9cb145c---------------

24 Nov 15

--Finished (make-all-house-test-videos) which makes 4-by videos for all house
  runs.  Running overnight to test.

--Talking with Jeff--reformulate graphical model to use ONLY world distance as
  binary cost, and figure out how to take into account temporal proximity in
  frames.

----------------git commit 4c7b61a---------------

25 Nov 15

--Problems running (make-all-house-test-videos) on seykhl--ran overnight and
  only completed 4 videos.  Looked like it was thrashing/crushing the RAM.
  Moved everything over to aruco and re-running.  Running in a screen session.

--Have plotting software running and saving {contour,scatter,surface}.png for
  each run on seykhl (screen).

--Redid proposal and binary score code to save proposals separately from binary
  scores.  Running (single-run-test) which calls the new functions in scheme and
  matlab.  Should see results in floorplan-0-sentence-1/20151125-iter1.  Running
  on aql in screen.

----------------git commit c1b141f---------------

25 Nov 15 (2)

--Changed new_binary_scores.m around line 289 to ensure that only the NEXT FRAME
  is connected to each frame (temporally adjacent).

--Also modified rendering-code.sc in (make-all-house-plots) to look for
  detection_data.mat before running (get-house-detection-data-one-run path
  testdir matlab-filename) 

----------------git commit 7000813---------------

26 Nov 15

--Fixed bug in codetection-test.sc get-matlab-proposals-similarity-new that
  stopped proposal_data.mat from loading; also fixed conditional that stops a
  bunch of unnecessary processing if proposal_data.mat already exists.

--Changed top_k to be a parameter to single-run-test

--Added floorplan boundaries (read from
  /aux/sbroniko/vader-rover/logs/house-test-12nov15/house-x-y.sc) to matlab
  proposal generation (new_proposals_and_dsift.m) to penalize proposals outside
  of floorplan.

--20151125-iter1 completed, results look more reasonable than previous

--Starting 2 new runs:
  20151125-iter2--still 10 proposals/frame, but now with floorplan boundaries
  20151126-iter3--changing to 40 proposals/frame 

----------------git commit 450d857---------------

29 Nov 15

--The 2 runs started on 26 Nov 15 look like they have completed.  Based on
  completion times, iter3 w/ 40 proposals/frame only took ~40 minutes longer (~5
  hours) than iter2 w/ 10 proposals/frame--all other parameters held the same
  between these runs. Unfortunately, I did not have enough scrollback in screen
  to see the split of that time between proposal generation and graphical model
  inference.

--Wrote (make-videos-and-plots) to do the visualizations for these runs. Started
  at 1722.

----------------git commit dce7199---------------

30 Nov 15

--Fixed bugs in make-house-plots-one-run-new, seems to work now

--Now working on getting a short, no-turn segment of video to test.  Will use
  this with new code that does the following:
  1. gets rid of (DOES NOT set to 0) unary scores of boxes behind camera 
  2. gets rid of (DOES NOT set to 0) unary scores of boxes outside boundary
  3. DISCOUNTS all other unary scores by some factor
  4. adds box pixel distance for binary score
  5. gets rid of world distance binary score 

--Have short segment in house-test-12nov15/test-segment.  Had to manually modify
  log/track files to match video.

--Wrote get-and-save-proposal-boxes to do a large number of proposals from each
  video--seems to work on its own, but has problems when running inside of
  for-each in get-and-save-proposal-boxes-all-videos...seems to be working OK now

--In new binary score function, keep both pixel distance and world distance as
  measures.  Have a binary flag for each to turn on and off, with a parameter
  that controls the weighting of their sum.  Can keep these as constants at the
  top of the program for now.

--Wrote new_score_saved_proposals_with_dsift.m to do 1-3 from above.

--Wrote new_binary_scores_world_and_pixel to do 4-5 from above (and the stuff 2
  comments up).

--Wrote new scheme wrappers for all, starting with single-run-test-new.

--Started simple-run-and-plot at 1835.  

**NEED TO REMEMBER that rendering-code.sc loads codetection-test.sc, so can't
  load rendering-code from codetection-test 

----------------git commit 9340bf7---------------

1 Dec 15

--Bug somewhere in simple-run-and-plot that is causing
  make-quad-videos-and-plots-one-run to crash.  Looks like there's a missing
  .mat file (detection_data.mat?)

--Fixed bug in new_binary_scores_world_and_pixel.m that had computed image
  centers incorrectly.

--Fixed bug where I failed to scale the dummy box unary score (dummy-f) by the
  discount factor.

--Noticed bug in frame-data-0.6-0.6.sc from 3 runs yesterday that every proposal
  has the same pixel coordinates--figured out that I wasn't incrementing j
  properly in new_score_saved_proposals_with_dsift--DUH!  Fixed.

--Fixed bugs in rendering-code.sc and make_house_plots_new.m that caused
  make-quad-video-and-plots-one-run to crash when the detection data was null
  (i.e., all dummy boxes)

--Re-running same test as last night with date changed to 1201.  Not all dummy
  boxes this time, but still too many.  Doing some investigation into the scores
  to see why--maybe the dummy-f and dummy-g are too high?

--Looking on aql run to precompute proposals, saw that floorplan-0-sentence-3
  and floorplan-0-sentence-4 seemed to get the error that I saw yesterday when
  there weren't enough proposals--re-running these with a smaller number of
  proposals...will probably also have to look for others that error like this.

----------------git commit 47e45fb---------------

1 Dec 15 (2)

--Looking at why we are getting dummy box on test-segment so often.

--Found logic error in new_binary_scores_world_and_pixel.m where I'm selecting
  the scores from G to put into g_score.  Was using j+1, which I thought was
  limiting it to next frame, but was really limiting it to next proposal.
  Restructured this to check all proposals in only next frame.

--Output of fix above looks promising--went from outputting only ~70 total
  binary scores to outputting ~500000 binary scores for the k=100 test.

--Confirmed that 6 floorplan-sentence dirs that errored out in the first run all
  had 'error: num-frames > cam-timing' pop up in align-frames-with-poses.  Looks
  like this happens because camera_front.txt does not have the correct footer
  lines, so read-camera-timing-new cuts off timing lines and thus
  align-frames-with-poses doesn't have enough data.  Could fix this by doing
  some parsing on camera_front.txt in read-camera-timing-new, instead of just
  chopping off final lines.

--Reran same test runs in 20151201a and got better results.  No more dummy
  boxes, but box still jumps around a bit.  Need to probably look at the test
  from Jeff's email (Dec 1, 1:08pm), but instead of looking at dummy box frames,
  look at frames that select 'wrong' box.

--Going to try to re-run test, but use world distance metric as comparison.
  Probably could do third test with a mixture/weighted sum of both metrics.  To
  speed things up, I commented out phow_hist call in
  new_score_saved_proposals_with_dsift.m --> now runs in under 11 sec for up to
  k=100!

--20151201b dirs are world distance metric test; not so good.

--20151201c dirs are pixel and world with a=0.5; not very good either.

----------------git commit 35cf765---------------

2 Dec 15

--REMOVED binary_score_threshold in new_binary_scores_world_and_pixel.m-->may
  need to re-add if using a more-connected graph.

--Proposal computing run on aql had errors on 1-2 and 1-5; re-running with
  num-proposals at 250.  This worked on 0-3 and 0-4 from yesterday.

--Going to run simplest possible experiment with top-k=1 and dummy-f=dummy-g=0
  so there is only 1 possible solution for OpenGM to find.  Should be able to
  compute by hand to see if output is the same.  Then will try top-k=2.  This
  should give me an indication of whether or not the graphical model is built
  correctly. 

--Did manual computation for simplest experiment and got same output value as
  OpenGM--see note below for commands.  Noticed in inference.cpp code that my GM
  is set up to minimize the sum of the -log of the scores.  This seems wrong and
  hackish.  Going to write new bp_object_inference function that simply
  maximizes the sum of the scores themselves.

--Wrote bp_object_inference_new that gets rid of logs and maximizes.  Seems to
  work.  Re-run of simplest experiment for k=1 got exact same result as OpenGM
  value.  Working on manually computing k=2 score (OpenGM value = 59.44)

--Ran 20151202d_top_k_{10,50,100} as 50/50 world/pixel distance split--results
  not very good.

--Ran 20151202e_top_k_{10,50,100,200} using only pixel distance--still have
  problems.

--Maybe try with more iterations in OpenGM?  Changed this from 100 to 10000 and
  then retried k=2 test (20151202c_1_top_k_2).  Also added visitor to bp.infer
  call. Changed binary score back to world/pixel distance split so that
  comparison to previous 20151202c run will be identical.-->Tried, same output
  result; looks like it stops after ~80 iterations.

--Maybe try with a smaller convergence bound?  Was set at 1e-7.  Changed to
  1e-10.  -->Tried as 20151202c_2_top_k_2, identical output.

--Trying re-run of 20151202d_* runs as 20151202f to get output with visitor and
  more iterations-->have screen output redirected into ~/screenlog.0

----------------git commit ad06815---------------

3 Dec 15

--Wrote gm-checking.sc with a couple functions to check output of OpenGM with
  scores.  Have saved in the comments there a bunch of commands used in building
  it. 

--Thinking about changing around the membership functions I use for the world
  distance.  Had been using gaussmf before, but changed to sigmf.  Also look at
  zmf.  

--Looking at viterbi code Jeff sent--trying to integrate it into inference.cpp
  in order to get viterbi score to compare against OpenGM score.

--Random thought-->should proposal score have more influence than it does?  Or
  are my metrics just bad??  How could I use better binary metrics?

--Or is my pixel distance sigmoid too steep?  With the center cross-over at
  80px and a slope of -0.1, distances of up to 40px get a score above 0.98.

--Trying a test with JUST pixel distance and the sigmoid parameters set to
  [-0.075,40] using the standard set of k values.  Seeing if this affects
  output.  This is 20151203a_top_k_{10,50,100,200}.

--**FOUND** arithmetic error in calculation of box centers for pixel
    distance-->computed (x2-x1)/2 (1/2 width) and (y2-y1)/2 (1/2 height), but
    forgot to add x1 and y1 to those to find the center of the images.

--Rerunning previous test as 20151203b*.

--From Jeff: There is a much simpler solution. You are measuring image
             distance. Since all proposals presumably have their edges bounded
             by the image edges, their centers must lie within the image. The
             minimum is 0 and the maximum is the image diagonal.Just scale
             linearly in this range to [0,1].
  Will try linear membership function once b test completes.

--Proposal computation on aql errored on 5-0, 4-5, 4-4, 3-4, 3-3; also re-runs
  of 1-2 and 1-5 from yesterday don't seem to have saved-->re-running all

--20151203b* tests looked MUCH better--a few dummy boxes and a few boxes that
  were the whole frame, but much more temporal coherence than before.  Never
  tried linear membership function.

--Thinking about changing pixel distance measure to IoU--looking for fast ways
  to do it in matlab.

----------------git commit d9b25eb---------------

4 Dec 15

--Talking with Jeff on my way out yesterday:  Need to incorporate world distance
  between proposals-->should help with the problem of the full-frame proposal in
  frames where the robot is moving.  For frames where the robot is stationary,
  this won't help.  Need to 'carry forward' the selected proposal in frames
  where the robot is not moving/turning (can determine this from odometry).
  With current setup in OpenGM, don't know if there's a way to copy new
  proposals into subsequent frames-->maybe the simplest solution is to copy
  forward ALL proposals between stationary frames when building bboxes from raw
  frames in matlab.  **Won't be able to run this in parallel anymore, at least
  not for stationary frames.**

--Email from Jeff this morning seems to suggest a new approach using Viterbi
  instead of OpenGM.

--Implemented viterbi and have single-run-test-viterbi working now.  Need to
  rewrite the plotting functions in rendering-code.sc.

--Experimenting with a new distance-from-the-robot metric as a unary score so
  that the model prefers things further away from the robot.

--Email to group: 

  Currently running tests in
  seykhl:/aux/sbroniko/vader-rover/logs/house-test-12nov15/test-segment/20151204f_ww_1_df_{0.1,0.5}_top_k_{10,50,100,200}.

  Output images will be in ./images/ under each directory.  

  The ww represents world weight (i.e., these are using 100% world distance and
  0% pixel distance) and df represents discount factor.  I'm testing with
  discount factors of both 0.1 and 0.5 to see what effect the new
  distance-from-the-robot based unary score has.

  I still need to finish fixing the code that does the plot/video creation--will
  work on that from home this weekend.

----------------git commit 9f0b881---------------

7 Dec 15

--**THINK ABOUT: new pixel distance measurement function-->previously talked
    about using a 4-D measurement (all 4 corners), but won't a 2-D measurement
    of opposite corners be equivalent, since all boxes are rectangular?

--Have make-quad-video-and-plots-viterbi written to visualize new runs.

--Do I want to compile viterbi.sc into dsci?

--Changed new unary score in new_score_saved_proposals_with_dsift2.m to be a
  modification (multiplication) of the original fscore rather than a replacement
  of it.

--Discovered that MCG was not matching hand-drawn ground truth proposals on a
  significant number of frames.  Tried using edgeBoxes instead of MCG to see
  what we can get from that.  Running this test as 20151207b with
  discount-factors of 0.1 and 0.01 with world weight of 1, as well as df of 0.1
  and ww of 0.5.

----------------git commit a682fdc---------------

8 Dec 15

--For some reason, all my jobs, as well as screen sessions, on aruco last night
  died.  Restarted 20151207b jobs (ww=0.5/df=0.1, ww=1/df={0.1,0.01}) spread out
  over aruco, akili, save.

--Looking at the runs with ww=0.5/df=0.1, for top_k = {10,50} the box still
  jumps around quite a bit, but for top_k={200,500} track seems to stay pretty
  coherent.  Size of box shifts a bit, but since we're only looking at 2 points
  (pixel dist=center of box, world dist=center point of bottom line of box),
  this is still coherent.  But the track is of a nonsense object (baseboard of
  wall). The k=500 tracks for ww=1/df={0.1,0.01} are crap.  For df=0.1, box
  jumps around.  For df=0.01, box jumps around less, but 'object' selected is
  just a chunk of the carpet in front of the robot.  

--The above leads me to think that df of 0.01 is too small-->might actually need
  to be bigger than 0.1 with new unary scores from edgeBoxes.  Also thinking
  that the pixel distance (or maybe some other metric we haven't though up yet)
  is necessary, since the world distance is just a measure of one point.

--Need to implement better pixel distance measure.  Going to try using a 4-d
  distance with the input from each box being [x1 y1 x2 y2].

--**Had to kill 20151207b jobs that used top_k=1000, caused servers to thrash.
  Need to look at the matlab code there (probably main parfor loop in
  new_score_saved_proposals_with_dsift2.m) to see if I've got a memory
  leak-->1000 proposals/frame * 80 frames * a few bytes per proposal shouldn't
  eat up all the memory.  Are there extra copies of variables being made?  Or
  might it be something in the matlab->scheme transfer.  Need to look at this
  more closely if we are going to use that many proposals.-->memory leak may NOT
  be in main parfor loop--gets through this and then eats up memory when
  computing binary scores-->memory explodes in pdist2 call in
  new_binary_scores_world_and_pixel2.m-->need to fix this

--Talking with Dan about new ideas.  Going to try to use 3-D world location of
  box corners (top-left and bottom-right) to incorporate world width and height
  in our world similarity measure--could probably replace bottom center point.
  Dan is figuring out how to compute world Z for top box corners--will have the
  same X and Y as the bottom corners.

--Got new function from Dan in viterbi.sc, box-at-height->world-corners, that
  outputs world xyz locations for box corners.  Probably need to convert that
  into matlab in new_score_saved_proposals_with_dsift2.m

--Changed pixel distance measure in new_binary_scores_world_and_pixel2.m to use
  [x1 y1 x2 y2] for each proposal as a point in 4-d space and the compute
  euclidean distance between.  Hoping this helps maintain a more constant box
  size.  Testing this in 20151208a_ww_0.5_df_0.1_top_k_{10,50,200,500}.

--Today's and yesterday's 50/50 runs look very similar for
  top_k=200-->temporally coherent, but with a garbage object in the box.  Need
  to look at how to get it to select better objects-->objectness score from
  Alexe et al.?

----------------git commit d2e5348---------------

9 Dec 15

--Fixed problem with renaming of viterbi-boxes function in viterbi.sc.  Dan's
  file is viterbi-new.sc.  Copied box-at-height->world-corners function into my
  viterbi.sc, but will probably have to re-implement in matlab since that is
  where I'm computing all other world points.

--Implemented box-at-height->world-corners in matlab in
  new_score_saved_proposals_with_dsift2.m to give 3-d world locations for top
  left and bottom right corners of boxes.  Need to finish fixing the START HERE
  and FIXME sections of that file to do new stuff (try to keep old structure as
  much as possible; then change around new_binary_scores_world_and_pixel2.m

--Talking with Jeff and Dan about using TLD (?? look up) for generating a set of
  unscored tubes/tracks.  Dan also working on improving speed of viterbi and
  having it generate multiple tracks.

----------------git commit 47aca32---------------

10 Dec 15

--Finished fixing new_score_saved_proposals_with_dsift2.m to use new world
  distance metrics and save them.  Also adjusted parfor initialization to only
  happen when top_k > 500, since top_k=500 runs in ~30s and it takes ~40s to
  initialize parfor.  Not entirely sure why parfor seemed to work without init
  before. 

--Completed rewrite of new_binary_scores_world_and_pixel2.m to find world
  distance measure using 6-d (2 3-d corners).  Appears to work.  Running test on
  aruco with 50/50 split and df=0.1 for at least top_k={10,50,100,200}, maybe
  500 as well.  Test is in 20151210a dirs.  Test with 500 didn't work--ran out
  of memory.  Results look pretty much the same as the previous test with the
  world location based on the single point.  Probably need to be able to run
  with more proposals.

--Started on restructuring new_binary_scores_world_and_pixel2.m.  Think I can
  get rid of vectorize/restructure stuff and instead pull location data straight
  from frames in bboxes, the compute and save in new G_mat.  Old G can probably
  go away (or set to 0)--Do I need to create gscore output?  It's saved in text
  file, but is it ever used?
  
----------------git commit aa2851d---------------

11 Dec 15

--Finished restructuring new_binary_scores_world_and_pixel2.m.  Now it just
  computes the adjacent binary scores, not all binary scores.  Runs fairly fast
  in matlab (~30s for 500 proposals, ~120s for 1000 proposals).  Discovered that
  the program is spending a LARGE amount of time writing proposals-similarity to
  frame-data.sc file.  Seems like the big delay is with writing the gscore
  variable, which is the binary scores listed as #(f1 b1 f2 b2 g).  Don't think
  I actually use that anymore, since the Viterbi implementation gets its data
  from the G_mat variable.  Going to take that out and see if it speeds things
  up. 

--Put placeholder value in for gscore and it sped matlab portion up by about a
  factor of 10.  Need to try the full scheme run to see if not having data in
  that variable breaks anything.-->Test didn't seem to break anything, and it
  runs much faster (top_k < 100 all ~60s, 200 in ~90s, 500 in ~5min, 1000 in
  ~15min).  Results look pretty much the same. (20151211b dirs)

--Running test where I hold the top_k at 200 while increasing df to see what
  happens. (20151211c dirs)  Looks like increasing df to 0.3 changes the track,
  but increasing beyond that doesn't make a big difference.

--Got viterbi-multiple from Dan and wrote test to ensure it worked.  Next step
  is to visualize the multiple tubes.

----------------git commit 1abc860---------------

15 Dec 15

--Worked on Dan's tracker-test.sc stuff.  Have C/scheme function to return list
  of boxes tracked from original (first frame) box.  Need to write wrapper to do
  multiple lists, and also plotting the boxes in the floor plan.

----------------git commit fdc6335---------------

16 Dec 15

--NEW PLAN:
  1. Take a video and generate n*K proposals for each of L frames.
  2. Run these proposals through NMS in order to reduce to top K that
     are as distinct as possible from each other.
  3. Run Median Flow forward and backward from each of L frames to get
     K*L tubes. 
  4. Run another round of NMS on the K*L tubes to eliminate similar
     tubes.
     A. RUN THIS when actually selecting boxes to turn into tubes-->if
        starting box overlaps too much with a box from an existing tube,
        discard it and get a new starting box
  5. Visualize remaining tubes to determine if they will be good enough
     to use with the higher-level information. 
     A. Try using Dan's run_and_display_TLD.cpp as a framework for this
        (throwaway visualization rather than saving images)

--Need to still rewrite tracker in C using name *MEDIANFLOW* instead of TLD.
  Use array of images to go forward and backward.

----------------git commit d1f7af0---------------

17 Dec 15

--Still working on task 1 from above.  

--Wrote get-frame-numbers to return the frame numbers (0-based) of the frames
  selected as ones to get proposals in (and thus start the tracker from).

--Found out that edgeBoxes needs some mex files compiled each time matlab
  starts.  Wrote InstallEdgeBoxes.m to do this, and am calling it at the end of
  ~/Documents/MATLAB/startup.m

--To make it simpler, changed the L in generate-proposals from being the number
  of frames to being the interval between frames (i.e., get every Lth frame).
  That way the number of frames getting proposals is num-frames/L.

--Next step is doing non-max suppression on the proposals (probably still in
  matlab) then pulling them out into scheme (probably as a list of lists) in
  order to do the get-tubes... call on them (also need to write that C function)

----------------git commit f479b2c---------------

18 Dec 15

--Wrote enablePool.m in ~/Documents/MATLAB/ and called it before
  InstallEdgeBoxes in startup.m to start parallel processing automatically.
  Found out I had to call before InstallEdgeBoxes because it was causing matlab
  to crash on seykhl (but not on servers) if I didn't.

--Wrote nms_iou.m which does intersection over union on boxes and if >
  threshold, picks box with higher score.

--Have 1 and 2 complete.  Data is pulled back into scheme properly.  Next need
  to rewrite Median Flow tracker in C to run forwards and backwards.

--For the tracker Dan has suggested doing minimal mods to his original
  tracker-->this would cause the video to be reloaded into an array on every box
  in every starting frame--lots of loads.  Is there an easier way?  Maybe write
  generate-proposals output to file then read the file in C and parse the data.

----------------git commit f1b1316---------------

28 Dec 15

--Wrote run_MF.cpp that produces a tube (list of box vectors as #(x1,y1,x2,y2))
  from an input of video path, starting frame number and proposal box as
  #(x,y,w,h).  Appears to work correctly from any starting frame.

--Wrote get-medianflow-tube-from-starting-frame-and-proposal-box in
  codetection-test.sc which calls run-MF.out from scheme and returns a list of
  vectors that represent the boxes in the tube.

--Next step is to write more wrappers which iterate over all proposal boxes in
  all starting frames to generate one big list of tubes for a video (also to
  call generate-proposals and read the output of that)

--Then need to visualize these tubes (29 Dec) and run NMS on them (30 Dec).

----------------git commit f729a0f---------------

 29 Dec 15

--Changed generate-proposals to output a list of lists of (frame-number (list of
  proposal-vectors)) to more easily feed into
  get-medianflow-tube-from-starting-frame-and-proposal-box.

--Wrote get-all-tubes to generate tubes from video-pathname, K, L.  Appears to
  work correctly.  For test video (80 frames) with K=5 and L=20 (20 tubes), runs
  in ~15s. Run time appears to be linear in K and num-frames/L.

--Also have get-all-tubes-sorted to return same tubes as get-all-tubes, but
  sorted by decreasing score

--Wrote render-one-tube which takes the PATH to a video (NOT the full filename),
  a tube-with-score, an output dir, and a num (used for filename of video) and
  creates a video with the tube rendered on it.

--Wrote render-all-tubes which calls render-one-tube on a list of tubes.

--Wrote tube-test-end-to-end which runs on all tubes generated for a given path,
  outdir, K, L.

--Wrote big-tube-test-end-to-end --> runs on 3 dirs
  /aux/sbroniko/vader-rover/logs/house-test-12nov15/{floorplan-0-sentence-0/,floorplan-1-sentence-5/,floorplan-2-sentence-0/}
  and visualizes (as avi videos) the top 20 tubes (based on proposal score) for
  each video with K=10, L=10. Output will be in ./20151229test2 in each video
  directory.  

----------------git commit 5725541---------------

30 Dec 15

--Run of big-tube-test-end-to-end on floorplan-1-sentence-5 by itself took ~49
  minutes (2917.81s).

--Run on floorplan-2-sentence-0 failed--need to investigate this error.

--Also noticed that some part of the test (probably the rendering) is eating up
  a lot of RAM-->look at rendering--am I freeing the images properly?

--Added an imlib:free-image-and-decache to the end of the rendering loop and
  that appears to have solved bot the memory problem and the crash-->after some
  googling, figured out that the error message just before the crash was because
  I ran out of available file handles.  Re-ran on floorplan-2-sentence-0 and got
  complete results in ~57 minutes.

--Added '-loglevel 0' and '>/dev/null 2>&1' to ffmpeg call to suppress massive
  output. 

--Writing my own NMS functions based on the tracks-overlap? and
  voc4-detection-intersection-divided-by-union that Dan showed me, but doing it
  a bit differently.  Dan's function computes an average overlap based on the
  IoU function, which always returns a number between 0 and 1.  In my tracks,
  this would return a 0 for every frame where neither track had a valid box.
  This in turn would make my average overlap smaller because all tracks have a
  large (but variable) number of no-box frames.  The variability of this number
  would make setting a threshold hard.  I think it makes more sense to only
  count frames where at least one track has a detected box towards the
  average--in this case, I think the threshold we set would be comparable to the
  single-frame IoU box threshold (currently at 0.5).

--BASED ON GROUP MEETING--changing idea for how to compute tube overlap.  NO
  MORE individual frame overlap and then average.  Instead, compute a list of
  box intersection and box union for each frame pair, then sum each and divide
  intersection by union.  

  will have some sort of (cond) in the above --> intersection in frame only
  needed if both boxes valid, otherwise intersection is 0 and union is simply
  the area of the valid box

--Once overlap function is working, just sort list of tubes by score, put
  highest-scoring tube into output, then work down the list, tossing a tube if
  it overlaps with any previous tube, otherwise keeping it, until num-tubes is
  reached. 

--Tasks from Jeff email 1638 30 Dec
  1. NMS on tubes working
  2. render top 20 tubes for all videos -- or raise num-tubes till we get all
  interesting objects
  3. replace 0,0,0,0 with boolean for box presence 
     DONE 30 Dec--tested and appears to still work with rendering functions
  4. backproject each box in each tube to world coordinate and compute variance
     over tube -- in scheme over matlab?

----------------git commit c173c0e---------------

1 Jan 16

--Wrote tube-intersection-over-union and tubes-overlap? to implement tube IoU as
  described above.  Seems to work properly.

--Next implemented tube-nms to reject tubes that overlap too much (as described
  above).  Seems to work properly.

--Next step is to render top 20 (?) tubes on all good videos and see if that
  gets us all interesting objects.

----------------git commit 40f902f---------------

2 Jan 16

--1st attempt at running big-tube-test-end-to-end2 (with nms-tubes) started at
  2025 on 1 Jan 16.  Restarted at 2057 after finding divide-by-zero error.
  Restarted again at 2327 after finding I forgot a mkdir-p before saving the
  tubes.sc file.

--Third restart ran until it hit floorplan-0-sentence-4 then gave the same 'too
  many files open' error message that I saw from the memory leak last
  week--where could there be a memory leak now??  Do I fix this by splitting it
  up and restarting dsci for every run?

--Also saw that I had a logic error--was running NMS *AFTER* taking the top
  num-tubes tubes--FIXED.

--Full copy of /aux/sbroniko/vader-rover/logs/house-test-12nov15/ copied to all
  workstations but mohio and all servers but jalitusteabe, upplysingaoflun,
  verstand, arivu

--Restarted run named 20160102test1 at 0718 on 2 Jan.  Had some sort of hang, so
  killed it.

--Restarted 20160102test1 again at 0845.  Crashed again due to too many open
  files error.  Thinking that I need to split it up among a couple of different
  workstations and have dsci exit after completing each run.

--Instead, going to try to skip the rendering and just save the nms-tracks.sc
  file, then deal with the rendering problem later.  Renaming this test as
  20160102test2, starting at 1838 on 2 Jan.

--Need to write a separate rendering function--maybe still split out among
  different workstations?

3 Jan 16

--20160102test2 run crashed in the middle of floorplan-0-sentence-6 with the
  'too many open files' error again.  RAM was maxed out.  The leak must be
  somewhere in the tube collection function--maybe in C++?  Can still probably
  get around this by splitting up between workstations and restarting dsci fresh
  for each run.

--Wrote get-all-nms-tubes-and-render-n to process a single run and
  get-and-render-n-nms-tubes-house-test to process all runs over several
  machines.

--Started run of get-and-render-n-nms-tubes-house-test from seykhl on (chino,
  buddhi, maniishaa, alykkyys) with n = 20 at 1627.

--Still running as of 2108:
"running for:" "15300seconds"
"started-jobs:" 22
"waiting-jobs:" 5
"done-jobs:" 16
"effective cpus" (0. 0. 0. 0.)

----------------git commit a9b38f5---------------

4 Jan 16

--get-and-render-n... test from last night completed 21/27 runs correctly.  Not
  sure why other 6 did not run correctly.  They are:
  done-1  floorplan-0-sentence-0 chino
  done-2  floorplan-0-sentence-1 buddhi
  done-20 floorplan-3-sentence-4 chino
  done-23 floorplan-4-sentence-5 buddhi
  done-24 floorplan-5-sentence-1 maniishaa
  done-25 floorplan-5-sentence-2 alykkyys

--Going to re-run the above using 6 total workstations (previous 4 plus seulki
  and faisneis)

--Talking with Dan about workaround to get around open files problem--CAN'T be
  in C because that program exits at the end of each run--MUST be somewhere in
  scheme, but where??

--While this is running, turning attention to doing backprojection of each box
  in each tube to world.

--Tried doing some memory cleanup in generate-proposals, but it always seemed to
  cause a crash.  I think dsci automatically cleans up any variables (like
  frames) that I declare in a let.  But should still need to clean up after
  matlab--probably call a matlab clear all at the end of the whole deal??

--Experimented with calling a matlab clear all, but it doesn't seem to do
  anything and isn't any different than killing and restarting dsci.  For
  test-segment with K=L=10, still freed up 2GB RAM after killing dsci--WHERE IS
  IT GOING???

--Gave up on finding memory/file handle leak-->doesn't seem productive if the
  workaround works.

--I THINK I've found *A* problem in the 6 runs that didn't complete.  Was able
  to call each directly (separate screen session on each machine) and get all to
  run EXCEPT for floorplan-4-sentence-5-->that one had a matlab error in
  get_proposals_edgeboxes.m.  Figured out the problem was that for a particular
  frame, num_proposals was > than the number of boxes returned from edgeBoxes,
  so dsci was crashing (which prevented any output from getting printed to the
  files).  Fixed it by only requesting the number of boxes available if that
  number is < num_proposals.  Seems to work now.  Not sure why the other runs
  didn't go--0-0 and 0-1 might have been from the old problems somewhere, and
  those might have affected others..???  Probably should try a re-run of these
  on the 2G servers overnight.

--Final try of 2G sever run started at 1852.

----------------git commit c60259b---------------

5 Jan 16

--Test from last night only had 4 completed runs this AM--WTF???  Going to try
  restarting it after removing enablePool from ~/Documents/MATLAB/startup.m and
  moving the start-matlab! call from the end of codetection-test.sc to the
  actual function where matlab is used (generate-proposals).  Also adding
  faisneis, seulki, alykkyys, buddhi, and chino to the server pool. **removed
  buddhi after seeing Jeff running stuff there, added maniishaa in its place**

--Had immediate error on alykkyys on output-6 (floorplan-0-sentence-5):
??? Invalid MEX-file
'/home/sbroniko/codetection/source/new-sentence-codetection/forests_edges_boxes/private/edgesNmsMex.mexa64':
/home/sbroniko/codetection/source/new-sentence-codetection/forests_edges_boxes/private/edgesNmsMex.mexa64:
file too short.

Error in ==> edgeBoxes>edgeBoxesImg at 86
E=edgesNmsMex(E,O,2,0,1,model.opts.nThreads);

Error in ==> edgeBoxes at 74
if(~iscell(I)), bbs=edgeBoxesImg(I,model,o); else n=length(I);

Error in ==> get_proposals_edgeboxes>(parfor body) at 14
        temp_boxes = edgeBoxes(img,model); %proposals

Error in ==> parallel_function at 479
            consume(base, limit, F(base, limit, supply(base, limit)));

Error in ==> get_proposals_edgeboxes at 11
    parfor t = 1:T %main parfor loop to do proposals and histogram scores
 
??? Undefined function or variable 'bbs'.
 
??? Error using ==> save
Variable 'nmsbbs' not found.
 
/home/sbroniko/darpa-collaboration/bin/darpa-wrap: line 13: 54574 Segmentation
fault      (core dumped) LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6 

  The file is there--is it an access issue?  Would adding a random delay help
  fix it?

--Added pause(rand()) to matlab startup.m just before InstallEdgeBoxes call to
  see if that helps.  Also modified InstallEdgeBoxes to not actually call the
  mex compile commands unless edgeBoxesMex.mex64 does not exist, and put a
  similar if statement in startup.m to prevent calling toolboxCompile unless
  necessary. 

--Restarted test (20160105-rerun-test) at 1119.  Time monitoring in the program
  seems to be a little off--when it said it was running for 10800s (3 hours),
  the real time was 1451, or ~3.5 hours.  The rsync at the beginning accounts
  for some of that, but not all.

--20160105-rerun test completed at 1839.  Looks like everything worked
  correctly. 

--Wrote new functions starting in 'backprojection stuff' section of
  codetection-test.sc.  Have find-point-from-lines working using nlopt and it
  gets an expected answer when we zero out the x coordinate.  With the x
  coordinate in, though, we get a bogus answer that seems scaled down from the
  correct answer.  We're thinking that nlopt is trying to minimize the error to
  all by scaling.
  --Is there a way to reject outliers?? (RANSAC?)
  --Or would pseudoinverse be a better method to use to solve?

----------------git commit b3efec9---------------

6 Jan 16

--Email to Jeff: 
We did simple checks yesterday and found that the optimizer gave us the answer
we expected.  I also found a matlab-based 3d line intersection function
(lineIntersect3d) that uses least squares and feeding it the same data as our
nlopt produced the same results.  So I'm more convinced now that the problem is
not in our optimizer. 

The data we're working with (at least in this example tube) seems be
affected by the presence of outliers.

I ran all pairwise combinations of lines and found found some
intersection points (~100/5000) that were near ground truth (which I
measured at home last night).  These points all seemed to have a
pretty low final cost function value (< 1e-5).  There were also bad
points with a similarly low (or lower) cost function value, but these
seem to be points that lie behind the camera.  I'm thinking that this
might be a path to something useful...

I was also thinking that something like RANSAC might be useful to
reduce the influence of outliers, but I'm not sure how to go about
choosing the parameters that RANSAC needs.
----

--FINAL VALUE of cost function from nlopt is the sum of the squared distances
  from the optimized point to the given lines.***

--Wrote a couple of functions in junk.sc (clean-inner-list and
  get-tubes-of-length) to do the selection of points with low cost function
  values and to cut down a set of tubes to those with at least num boxes.  Not
  sure if these will be useful later

--After looking at the data, realized that there is significant drift in the
  robot theta angle (> 1 deg in ~3s while sitting still) which is causing
  problems in projection.  Dan and I came up with the idea to simultaneously
  optimize both the intersection point (as previous) as well as the deltas of
  (x,y,z,theta,phi,psi) given the tube of (x1,y1,x2,y2) pixel locations, the
  (known noisy) odometry, and the camera intrinsics/offset.

--Have structure of big optimization sketched out.  Now need to implement the
  helper functions that go into the giant cost function and run on a single
  tube.  Then expand to run on all tubes of a particular run.

----------------git commit 6de28e4---------------

7 Jan 16

--Talking with Dan; confirmed the need to redo position calculations taking into
  account full 6DOF (x,y,z,theta,phi,psi); also discussed the need to have
  position deltas in ROBOT coordinate system b/c errors accumulate over time.

--Have some functions for 6dof stuff working in codetection-test.sc and
  rover-projection.sc.  Need to work on
  find-6dof-poses-list-from-deltas-vector-and-initial-pose b/c it doesn't seem
  to be giving the answer I'm looking for-->should I change how I compute the
  deltas (i.e., order of operands)?

----------------git commit 181f09a---------------

8 Jan 16

--Rewrote get-deltas-of-* functions to have the delta computed as (v- second
  first) instead of the other way around.  REMEMBER THAT the functions use OF
  instead of FROM.

--Wrote world-track-3dof->world-track-6dof and used it to ensure that world
  track reconstructed from robot-coordinate-system deltas matched odometry
  track.

--Wrote robot-pose-6dof-to-camera->world-txf in rover-projection.sc, confirmed
  that it returns the same camera->world transform matrix that the old 3dof pose
  version does. 

--Wrote pixel-and-pose-6dof->world-line in codetection-test.sc, confirmed that
  it returns the same line points as the old 3dof pose version does.

--Wrote raw-tube-with-score-and-pose-list-6dof->lines which computes lines to
  all 4 box corners like the old aligned-tube-with-score->lines.

--Modified point-line-squared-distance to allow for #f's in list of lines.

--Tested find-point-from-lines with new raw-tube-wtih-score->lines and got same
  answer as using previous aligned-tube-with-score->lines...seems to be working
  correctly. 

--Started writing new optimizer (find-points-and-deltas) but had to stop when we
  realized that there were problems with how I was computing deltas in the world
  and robot frames.

--**NEED TO FIX** working on get-deltas-list-in-robot-6dof.  Takes raw odometry,
    turns it into 6dof world odometry, then finds world deltas (so far).  Next
    needs to transform this into robot deltas at each step (should be just a
    fixed - half-pi shift from world to robot, plus/minus the world theta to
    align the coordinate systems.  This will give robot delta x, delta y.  Delta
    theta will be the same as in world b/c the z axes are aligned.

--**Then will need to work on the reverse, going from the robot frame back to
    world.  Should be able to build two transformation matrices (world1->robot1,
    robot1->robot2), multiply them together, and then use
    my-transform->parameters to pull out the 6dof change in world frame.  

--**test for this working will be that I can take a path, get the deltas into
    robot frame, and then do the reverse and get the original path back out.
    Also should be able to artificially add a half-pi theta shift to the first
    delta and then get back a shifted path.

----------------git commit d949f07--------------

9 Jan 16

--Appear to have get-deltas-list-in-robot-6dof (which relies on
  world-delta-and-pose-6dof->robot-delta-6dof) working correctly.  Sent to Dan
  for him to check.

----------------git commit 62ed3f4--------------

10 Jan 16

--Tested get-deltas-list-in-robot-6dof more and it still appears correct.

--Wrote world-pose-and-robot-delta-6dof->world-pose-6dof, appears to work
  correctly.

--Wrote correct-angle to map angles in [-pi,pi) to [0,2pi) like track data is.

--Wrote robot-deltas-list-and-world-start-3dof->world-track-6dof to apply
  world-pose-and-robot-delta-6dof->world-pose-6dof to a list of deltas with a
  world start-->appears to give correct track back for *fake-track* and its
  deltas as input (at least to the 15th decimal place).

--Tested path recovery with 90 degree shifts and appeared to get out the path I
  was expecting.

--Tested path delta-finding and recovery with a segment of actual path, and got
  the original path back to within a reasonable number (> 10) of decimal
  places.  Declaring success

--The fake track used for testing:
(define *fake-track* (list (vector 0. 0. half-pi) (vector 0. 1. half-pi) (vector
1. 2. (/ pi 4)) (vector 2. 3. (/ pi 4)) (vector 3. 4. 0) (vector 4. 4. half-pi)
(vector 4. 5. (/ (* pi 3) 4)) (vector 3. 6. pi) (vector 2. 6. (/ (* 5 pi) 4))))

--Also used sublist 30 40 of the test-segment track

----------------git commit 04d0648--------------

11 Jan 16

--Have find-points-and-deltas working.  Works reasonably well on 3-point
  synthetic example--finds optimum quickly when started at optimum, but when one
  of the points is perturbed it doesn't do very well.  Could be because of such
  a small number of points--perturbing 1 is 1/3 the total.  

--Currently trying a run on the test-segment (80 frames) with all 25 tubes,
  results going into
  /home/sbroniko/codetection/source/new-sentence-codetection/screenlog.0.  Also
  trying runs using {5,10,15,20} tubes (subset of the 25) in separate processes
  (no logging that output).

----------------git commit d5cfb09--------------

12 Jan 16

--Current status of last night's runs as of 1042: (email to Jeff)
	  Full run with 25 tubes: still running (current cost ~1e-4)

	  Run with 20 tubes: still running (current cost ~6e-5)

	  Run with 15 tubes: Finished at 7:13pm (~2h15m run time), final cost
	  4e-7, xyz points for tubes are bad (all appear to be either behind the
	  camera or a few cm in front of the camera), all the path deltas were
	  very small (<1e-4) 

	  Run with 10 tubes: Finished at 11:15pm (~6h15m run time), final cost 
	  4e-7, xyz points for tubes are bad (similar to the above), path deltas 
	  are larger (a few on the order of 1e-2)

	  Run with 5 tubes: Finished at 11:49pm (~6h45m run time), final cost
	  3e-7, xyz point for first run (candle on top of end table) is bad, but
	  other points might be more reasonable--need to check the videos for
	  those tubes, path deltas are mostly very small, with a few on the
	  order of 1e-2.

	  With the very small path deltas that we're getting, I don't think we're
	  correcting the path error enough, and that is leading to the tube
	  points being just as bad as they were before we started optimizing the
	  path as well.

	  Is there another method we can use to try to lessen the effect of the
	  path drift?  Could optical flow help?  I think there might be some way
	  to use the observed motion from the camera and compare it to what the
	  odometry gives...  Need to think about this more.

--Jeff's reply/my reply
	 On Tue, Jan 12, 2016 at 10:42 AM, Jeffrey Mark Siskind
	 <qobi@purdue.edu> wrote: 

	 > What units are the path deltas that you reported in your previous
	 > email? 
	    xyz deltas are in m, angle deltas are in radians

	 > What is the magnitude of the gradient upon termination?
	    Not sure since we weren't printing this out.  When we did look at
 	    gradients during the single-tube runs yesterday, they were on the
	    order of 1e-10 or 1e-11 on termination.

	 > What is your scale factor for the contribution of path delta and
	 > world xyz error to the overall cost function?
	    The path delta cost is multiplied by 0.01 before being added to the
	    world xyz cost.  Prior to this both costs are averaged by dividing
	    each summed cost by the number of elements used to compute it
	    (number of path points for path, number of boxes in a tube for world
	    xyz) 

	    Related to this, one thing Dan and I discussed yesterday was trying
	    to let the length of a tube affect how much influence it has on the
	    cost--the idea being that a tube with more boxes should have a
	    bigger influence than a tube with fewer boxes.  This probably could
	    be achieved by simply not dividing a tube's cost by its number of
	    boxes. 

	    Another thought I had just now is that we might need to change how
	    we compute the final cost value.  Right now we take the scaled path
	    delta cost and add it to the sum of the world xyz costs for each
	    tube.  I wonder if that is giving too much influence to the world
	    xyz cost. Could we solve this by simply dividing the sum of world
	    xyz costs by the number of tubes?

	 > Here is an experiment to try. Do you know (approximate) ground truth
	 > world xyz of the objects that correspond to the tubes?
	    Yes, I have a rough measurement of the location for the first tube
	    in the test segment.  I could probably estimate a few others from 
	    watching the videos of the tubes.

	 > What if you fix that (i.e. don't optimize that) but just optimize the
	 > path deltas to see if you can determine the correct path deltas. Then
	 > if you can, fix those as input and just optimize the ground truth
	 > world xyz of the objects. I.e. just try to optimize one variable at a
	 > time. 

	    I'll start setting this up now.

--Jeff reply:
      xyz deltas are in m, angle deltas are in radians

      	  An xyz delta of 10^-2 is about 1cm which is about the maximum of what
	  you would expect per frame. 10^-2 radians is about a half of a degree
	  which is also about the maximum of what you would expect per frame. At
	  10fps, deltas like these would accumulate very quickly. So if you are
	  getting path deltas like these it might very well be correctly
	  estimating the path delta. Even 10^-3 would be quite large and
	  accumulate quickly so might also be correct. 

   > What is the magnitude of the gradient upon termination?
      Not sure since we weren't printing this out.

      	  What is was yesterday on the smaller problems is no indication of what
	  it is o the larger problems. In the future, it would be a good idea to
	  print this out. You don't need the gradient at every iteration. Only
	  upon temrination. 

	  Dividing by number of path points, number of boxes, and/or number of
	  tubes only affects the relative scaling of different terms in your
	  cost function. The same thing is accomplished with explicit scale
	  factors for the terms. Making number of path points, number of boxes,
	  and/or number of tubes part of those scale factors is the same as
	  making the scale factors problem-specific. If the algorithm is that
	  sensitive to the scale factors then we are doing something unideal.

    	  Jeff (http://engineering.purdue.edu/~qobi)

--Working on building find-deltas-only with known tube points, then will make
  find-points-only with fixed odometry (isn't this just like the old
  find-points-from-lines??) 

--Find-deltas-only works and gives a pretty good optimized track from a known
  world point.  Then using this track as input to find-points-from-lines gives
  back the original point to w/in 5cm in each direction

--Agreed with Dan that the weighting on odometry-diff-cost (* 0.01) was too
  low. Replaced by * 1 instead.

--Added printout of gradient and gradient magnitude to end of optimization in
  find-deltas-only, find-points-and-deltas.

--Wrote weighted-subtract-pose to allow weighting of the different components of
  the pose.  The idea is to decrease the weight on the angle components compared
  to the position components so that the optimization will not make a lot of
  drastic adjustments to the positions in an effort to account for image
  movement-->think this might be partially to blame in the backwards-forwards
  zigzag in the 5-tube optimized path.

----------------git commit 67e9ff4--------------

13 Jan 16

--Killed still-running tests on 20 and 25 tubes on seykhl b/c of hard drive
  replacement today.  Latest output for 20-tube run: odometry-diff-cost
  6.697387779457994e-07 final-cost 1.806612417980068e-05.  Latest output for
  25-tube run: odometry-diff-cost 7.387563728670102e-07 final-cost
  2.958520810138482e-05 

--Replaced subtract-pose with weighted-subtract-pose (only in optimizing
  functions, not in get-deltas-list-from-world-track-6dof)-->decrease weights on
  angles over xyz-->idea is to make the optimizer less likely to make drastic
  changes to xyz (back-and-forth problem) by making changes to xyz more costly
  than changes to angles. 

--Copied original find-points-and-deltas to find-points-and-deltas-old.  This
  version still has scaling factors on odometry and point costs, but uses
  weighted-subtract-pose.

--Removed scaling factors (/ len) and (/ (*6 (length odometry)) from
  find-points-and-deltas and find-deltas-only.  Thought about scaling the
  odometry score up by multiplying by number of tubes, but decided against it
  b/c it could overweight the odometry score.  **THOUGHT: try the odometry
  scale-up if things don't work as expected**

--Verified that removing scaling factors doesn't change output significantly in
  find-points-and-deltas and not at all in find-deltas-only.  Only change is in
  the final value of the cost function.

--Wrote visualize-two-tracks to take original odometry and optimized path deltas
  and plot them side-by-side

--Implemented optimization of all 4 corners of box.  Results are different, but
  not much better.  For test-segment, first tube (top-left corner ground truth
  is ~ #(-0.02 3.85 0.8)
  Old method (top-left corner only): 
      #(0.02559066362036297 0.5949865979067857 0.2265010279536986)
  New method (all corners):
      #(0.02119356092161943 1.113704968472089 0.3149214437995508)
      #(0.05592208048940997 1.0905847252409 0.3097494718417824)
      #(0.02255761214293257 1.042843167952684 0.267710012469166)
      #(0.05426763414797994 1.018035048313843 0.2632562501238425)
  Also, the optimized track from the new one is much different from the old
  one--and I think the old one is probably better.  See images at
  ./source/new-sentence-codetection/20160113-1-tube-{1,4}-point{s}-track.{eps,fig}. 
  Drifting in the route (fwd/bkwd/loops) still seems to be there.

--Thinking about coplanarity constraint and how to encode it into the model.
  Can write a function that takes 4 points and computes
  (x3-x1)·[(x2-x1)x(x4-x3)].  If that = 0, then the 4 points are coplanar.  But
  if that is nonzero, what does the value represent?  Could be simpler to take
  the 4 points, measure the 4 distances from each point to the plane made from
  the other 3, and then square each and find the mean.

--Started new run with {5,10,15} tubes with new 4-point optimization at ~1720.
  Can check results against plots from yesterday to see if they're any better.

----------------git commit 4527bcb--------------

14 Jan 16

--5-tube run ran for ~6 minutes.  Results saved in
  ./source/new-sentence-codetection/20160113-5-tubes.sc 

--The other 2 runs (10, 15 tubes) finished within 30 minutes and had garbage
  results (points inside camera).  Not sure it's useful to plot the tracks.

--Wrote correct-theta-drift and get-corrected-poses-that-match-frames to attempt
  to null out the drift in theta by looking at how theta changes during the
  initial period of the track when the robot is sitting still.

--Abandoning the idea of trying to find the points in 3D space.  Instead just
  operate on the assumption of objects being on the ground.  Then can reuse old
  code to find location for every box and compute mean location and variance
  over a tube.  Idea is that on-the-ground objects will have low variance, while
  not-on-the-ground objects will have higher variance.  Can also use the object
  location projecting behind the camera or outside the floor plan boundaries if
  necessary. 

--Wrote box-vector-at-height-and-pose->world-corners to take vector from tube
  along with height (always assumed 0) and pose to get world coordinates
  (top-left and bottom-right, same as tube boxes).  Fn is in rover-projection.sc

--In codetection-test.sc, wrote
  raw-tube-with-score-and-pose-list->world-corners-list,
  world-corners-list->world-xywh-list, and
  world-xywh-list->world-mean-and-variance to go from a raw-tube-with-score to
  world means and variances for x,y,w,h for that tube.

--Wrote find-world-means-and-variances to map across all tubes in a run.

--Noticed difference in thetas with paths found from get-paths... and
   get-corrected-paths..., but not necessary to fix since the first frame-pose
   is not necessarily the first actual pose, so the thetas WILL be different.

--Fixed a couple of problems with differences in tube/pose/timing lengths in
  raw-tube-with-score-and-pose-list->world-corners-list and
  get-poses.../get-corrected-poses... 

--Wrote find-and-save-world-means-and-variances-house-test and got results for
  all 27 runs.

--Wrote filter-world-means-and-variances to get rid of boxes that are < 1cm in
  world width or height.  Can change that threshold, also should probably add
  threshold for variances (what to set them at?)

--Working on render-multiple-filtered-tubes.  Will have same general structure
  as render-one-tube, but will go through filtered-wmvs and tubes at each frame
  to render the appropriate boxes. **LOOK AT mapping first and rest on
  tubes-without-scores.  Will also probably have to write
  render-unfiltered-tubes to render all tubes for comparison (could get ugly
  with a lot of tubes--maybe only 1 px thick for each)

**3 questions from Jeff**
1. Can we separate objects on the ground from those not on the ground with
   reasonably low false positives and false negatives?
2. What is the variance (distance for 1 std dev (sqrt var)) for objects on
   ground?  (remember--can filter this)
3. Are the world xy coordinates for the objects on the ground reasonable?

----------------git commit b968917--------------

15 Jan 16

--Wrote render-unfiltered-tubes to draw all boxes from all tubes in cyan, 1px
  wide.

--Wrote render-multiple-filtered-tubes and render-filtered-tubes-house-test.
  All ran fine, ~40 min to do all 27 videos.

--Based on feedback from Jeff, changing filter-world-means-and-variances to not
  worry about world width and height, instead use variance.

--Made changes to get-medianflow-tube... to make sure that bogus boxes #(1 1 1
  1) don't get tracked.

--Made changes to get_proposals_edgeboxes.m to remove any proposals that are <
  10 px wide or tall.

--Wrote filter-tubes-by-length that gives a list of #t/#f based on tube
  length. Currently have thresh set at > 5.

--In talking with Dan, realized that having an absolute threshold on variance
  might not be a good idea.  Variance of an object's position should be lower
  when the variance of the robot's position is lower.

--Implemented world-xywhs-and-poses->world-mean-variance-robot to add a third
  vector to the old world-mean-and-variance output.  Third vector is (rxm rym
  rpv bpv) where (rxm,rym) is the mean robot position, rpv is the variance of
  the distance between each robot position and the mean robot position, and bpv
  is the variance of the distance between each box position and the mean box
  position.  Then using the ratio of bpv/rpv to figure out if a box is
  good-->the idea is that if a box is a good box on the ground, its position
  variance will be lower than position variance of the robot over the frames of
  that tube.  Currently have the ratio set in
  filter-world-means-variances-robots that if ratio > 0.5, box is bad.  Trying
  this alone first.

--Looking at a few of the filtered outputs, seeing that I might want to raise
  the ratio to let more boxes through--or may want to generate more proposals to
  generate more tubes (lower the proposal or tube nms threshold).  Getting rid
  of ~90% of unfiltered proposals.  Eliminate almost all above-ground boxes.  A
  few still slip through though.

--Email to group:
	Just finished revamping how my filtering works.

	For each tube, in addition to computing the mean and variance of world
	(x,y,w,h) for the possible object represented by that tube, I also
	compute the mean world xy position of the robot over the poses that
	comprise that tube.  I then compute the distance between each robot pose
	and the mean robot position, then take the variance of that set, call
	this RPV.  I also compute the distance between the world location of
	each proposal box in the tube and the mean position for that tube, then
	take the variance of that set, call this BPV.

	I then filter on 2 things only:

	  1. Reject tubes that are 5 or fewer frames long, since the variances
	  here can be artificially low due to few samples.

	  2.  I take the ratio of BPV to RPV.  The thinking is that if a tube is
	  on the ground on an object, it will have a significantly lower
	  position variance than the position variance of the robot over the
	  same time.  I currently have the threshold for BPV/RPV set at 0.5
	  (i.e., ratios >= 0.5 get rejected).


        I do no filtering as I had done before, neither on variance of world
	x,y,w,h for each tube, nor on the mean world width or height.

	You can see from the videos that ~85-90% of the unfiltered boxes are
	getting rejected by this new filter.  A few above-the-ground boxes make
	it through, but most of them do not.  The few that do make it through
	all seem to have a negative world height (as expected--represents back
	projection to a location behind the camera), so I could filter on that
	if we can find a principled reason to do so.

	My gut reaction is that this filtering is pretty good.  There are a few
	places I noticed where I thought there should have been a tube on a
	certain object when there wasn't, but when I went back to the original
	unfiltered tubes video, I saw that there usually was either no tube
	there or the tube on that object wasn't fully on the ground, and thus
	got filtered.

	Dan and I watched a few videos together and discussed that it might be
	helpful to let more boxes through (to solve the issue above) by doing
	one or more of the following: raising the BPV/RPV ratio threshold,
	raising the IoU thresholds for proposal and tube NMS, or generating more
	tubes/proposals per video.

	Videos are in:
	/net/seykhl/aux/sbroniko/vader-rover/logs/house-test-12nov15/FOO/20160105-rerun-test/output-{filtered,unfiltered}-*.avi
	Where FOO is one of
	{floorplan-0-sentence-0
	 floorplan-0-sentence-1
	 floorplan-0-sentence-2
	 floorplan-0-sentence-3
 	 floorplan-0-sentence-4
 	 floorplan-0-sentence-5
	 floorplan-0-sentence-6
	 floorplan-0-sentence-7
	 floorplan-1-sentence-0
	 floorplan-1-sentence-3
	 floorplan-1-sentence-4
 	 floorplan-1-sentence-5
	 floorplan-2-sentence-0
	 floorplan-2-sentence-1
	 floorplan-2-sentence-2
	 floorplan-2-sentence-3
	 floorplan-2-sentence-4
	 floorplan-3-sentence-1
	 floorplan-3-sentence-2
	 floorplan-3-sentence-4
	 floorplan-4-sentence-1
	 floorplan-4-sentence-3
	 floorplan-4-sentence-5
	 floorplan-5-sentence-1
	 floorplan-5-sentence-2
	 floorplan-5-sentence-3
	 floorplan-5-sentence-4}
	Note that the number after filtered or unfiltered in the file name is
	the number of tubes rendered in that video.  So by comparing the
	filtered and unfiltered numbers, you can make a quick estimate of the
	percentage of tubes filtered out.

----------------git commit f71724e--------------

17 Jan 16

--Email from Jeff:
   On Sat, Jan 16, 2016 at 4:23 PM, Jeffrey Mark Siskind <qobi@purdue.edu> wrote:
   > A ratio of 0.5 does not constitute a significantly lower variance. With that
   > ratio, your model allows an error in world-position estimates of objects as
   > large as 2m if the robot travels 4m. That is untenable. We need object
   > position estimates within about 50cm. And the robot is naturally going to
   > travel a few meters at a time.
   >
   > Your model does not make sense. I understand your motive that low variance for
   > short tubes would result from lack of measurement. But the whole idea that
   > a tube corresponds to a single stationary object should imply that the variance
   > in measurement of its position should not be dependent on robot motion or the
   > length of the tube. It should be dependent on the noise in the process.
   >
   > But separate from this
   >
   >  1. There is no need for a tight filter to filter out tubes. Neither tubes
   >     that are flawed in some way nor tubes that aren't on the ground. You are
   >     never going to pair down the set of tubes in an accurate fashion with an
   >     simple filter like the one that you are using. All we really need to know
   >     at this point is whether we get a good position estimate. The whole reason
   >     for computing the variance is that high variance, for whatever reason,
   >     even if you excuse it due to high robot motion, means that you simply
   >     don't know what the world position is. And such a tube can't be
   >     used. Because we need an accurate world position to implement the
   >     sentential semantics. If the variance is high, you know your position
   >     estimate is unreliable. But if the variance is low, you don't know that
   >     your position estimate is reliable. For that, you need to compare it to
   >     ground truth. At this point you haven't answered the question of whether
   >     your position estimates are reliable. And your filtering process doesn't
   >     help answer that question. So we are making no progress towards
   >     constructing the graphical model.
   >
   >  2. No matter whether you filter tubes, how you filter tubes, how good that
   >     process is, and how many are left, you still will never reduce it down to
   >     one tube that is always correct. So you are still going to need a
   >     graphical model. Haonan's work shows that a graphical model can reliably
   >     select the single correct tube from a set of hundreds of such. So unless
   >     Haonan's experience doesn't apply to our problem, there is no merit in
   >     trying to filter a few tubes out of a set of 25.
   >
   > In short:
   >
   >  a. Tell me the variance on tubes. Unless it is generally less than about
   >     50cm, our position estimates won't allow us to construct a reliable
   >     graphical model.
   >  b. Tell me how accurate the tube positions are. Unless it is generally
   >     correct to within about 50cm, we also won't be able to construct a
   >     reliable graphical model.
   >  c. If we do get position estimates that satisfy (a) and (b), the most
   >     important thing to do is construct the graphical model. Don't waste your
   >     time with filters unless it becomes necessary to solve the graphical model.
   >
   >     Jeff (http://engineering.purdue.edu/~qobi)

--Wrote find-abc-and-filter, world-xywh-list->world-mean-and-distances,
  find-def-and-filter, find-abcdef, render-b-tubes, render-d-tubes, and
  render-b-d-tubes-house-test to compute all the data mentioned in my response
  to Jeff below.

--My response to Jeff: 
  On Sun, Jan 17, 2016 at 4:44 PM, Scott Bronikowski <sbroniko@purdue.edu> wrote:
  > Understand all.  In response to your questions:
  >
  >> In short:
  >>
  >>  a. Tell me the variance on tubes. Unless it is generally less than about
  >>     50cm, our position estimates won't allow us to construct a reliable
  >>     graphical model.
  >
  > I can give you the variance in world x, y, width, and height for every
  > tube (~300-600 per video in the set of 27 videos from my house), but I
  > don't know how much that will really tell us.  We know there are going
  > to be tubes that are garbage, so finding the mean values for these
  > variances will also likely not tell us much.
  >
  > In order to make some sense of all this data, I'm giving you the following:
  > For each video in the set:
  >
  > #(A #(B C) #(D E F))
  >
  > Where:
  >
  >   A = total number of raw tubes generated for that video
  >
  >   B = number of tubes that meet the following two criteria:
  >        1. Tube length > 5 frames
  >        2. Sum of the variance in the world x and world y are < 0.25
  >            (since the variance is already a squared distance quantity,
  >             var-x + var-y < 0.25 should represent a 1-sigma radius of 50cm)
  >      This should represent the set of tubes that are potentially useful to us.
  >
  >   C = The mean of the summed variances (var-x + var-y) of the tubes in B
  >
  >   As a check on B and C, I also compute the following:
  >
  >   D = number of tubes that meet the following criteria:
  >        1. Tube length > 5 frames
  >        2. All frames in the tube have a distance from the mean location
  >           of the tube < 50 cm.
  >    This set should be smaller than B, since D requires all boxes to be
  >    within 50cm
  >
  >   E = For each tube in D, take the mean of the distances computed
  >         for D.2, then compute the mean for all tubes in D
  >
  >   F = For each tube in D, take the variance of the distances computed
  >         for D.2, then compute the mean for all tubes in D
  >
  > This data is in the attached floorplan-tube-data.sc
  >
  >>  b. Tell me how accurate the tube positions are. Unless it is generally
  >>     correct to within about 50cm, we also won't be able to construct a
  >>     reliable graphical model.
  >
  > For this I need to watch each video, since 1) I don't have ground
  > truth measurements to every possible object in my house and 2) without
  > looking at the tube, I can't tell if the tube is on object.
  >
  > I'm rendering the B and D sets for each video now.  I've only looked
  > at the first few, but from what I can see, the tubes that are on
  > objects located on the ground plane have a reasonably accurate world
  > location, but the tubes that are on objects above the ground plane do
  > not.  I will look more closely at the tubes once the videos finish
  > rendering.

----------------git commit 626769d--------------

18 Jan 16

--Starting work on graphical model.  Working through thinking about the
  different tasks from Jeff's 4Jan email.  My working notes are in a paper
  notebook-->will transpose them here when they are fleshed out and cleaned up.

--Started
  /aux/sbroniko/vader-rover/logs/house-test-12nov15/house-test-sentences.txt for
  sentences to describe every run.

----------------git commit 5d0fb37--------------

19 Jan 16

--Finished writing sentences to describe each run (house-test-sentences.txt)

--Got copy of Haonan's code in ./from-haonan/codetectionlib-cpp.cpp.  Need to
  look at similarity_between_videos, similarity-between-videos-c,
  similarity-between-videos-matlab, similarity-among-videos (codetection.sc)

--Got copy of Dan's code in ./from-dan/dpbarret-darpa-collaboration-rover.  Need
  to look at definitions of prepositions (i.e., right-of, left-of, etc) in
  sentence-to-trace-from-learned-models.sc 

--Need to manually align videos to sentences.  Can do it by just determining
  start and stop frames for each segment.  Then can split up tubes by doing a
  sublist--probably also need to do something with a pose list (written out to
  sc file?)

20 Jan 16

--Need to work on aligning videos/tubes with sentences.  Initial thought is to
  just make a list of start/stop frames for the segments, and a matching list of
  the phrase in the sentence.  This will JUST split the path pieces (from the
  frame-matched poses) since the actual locations found from the tubes can be
  valid anywhere in a particular run (or more likely, a whole floorplan).

TO DO's
-------
 -code to save frame-poses
 -code to split up frames, tubes, tube locations based on alignment
   - each tube must be associated with its world location
   - will tubes have an entry for every frame in the video, or just the frames
   in a particular segment?  Seems the latter is probably better, but not
   totally sure.  List of tubes should probably have an entry for ALL tubes, not
   just the ones in that segment, just to keep numbering the same across
   all-->if a tube is not in that segment, just replace the list with #F.
 -SPLIT WON'T ACTUALLY AFFECT FRAMES OR TUBES, JUST PATH PIECES.
 -DON'T SPLIT TUBES ACROSS SEGMENTS!!!  Each segment's head noun can take the
 value of ANY tube in that video (and possibly all the other videos in that
 floor plan) b/c the object isn't necessarily in view when it is the head noun
 (i.e., behind, right of, etc.).  So a segment's noun variable can vary across
 ALL tubes.
 -THINK ABOUT consolidating nearby objects with a very small (~10cm??) threshold
 in order to reduce number of possible objects.  Can also concatenate the tubes
 from merged objects into mega-tubes.

----------------git commit b50aae9--------------

21 Jan 16

--Finished alignment of videos with sentences.  Data saved in alignment.sc in
  each directory.  Found 2 more that were bad, so now total test set is 25
  videos. 

--Wrote save-poses-house-test that saves the corrected frame-matching poses in
  frame-poses.sc in each run directory.

--Wrote find-low-variance-tubes that applies the D-filter from above and then
  outputs a list of 2 lists: first is #f if tube fails filter, else tube; second
  is #f if tube fails filter, else mean #(x y w h) for tube.  Also have
  save-low-variance-tubes-house-test to write this list to low-variance-tubes.sc
  in each run directory.

--Wrote split-path-by-alignment to split path into poses by the alignment to
  sentence. 

--After talking with Dan, rewrote find-low-variance-tubes to have output be a
  single entry for each tube, either #f or ((#(x y w h) #(xv yv wv hv)) dist-var
  video-name (list tubepixels)) so that when I join the lists from each run on a
  floor plan, all pertinent data to get back to the actual images (for binary scores)
  is there.

--Also discussed how to find unary scores for a particular variable (noun).
  Will have a function that takes the particular tube (value) to be scored, the
  segment of the path to be scored against (or null if scoring a helper noun,
  i.e., 'chair' in 'the robot went towards the table which is left of the
  chair'), and the function that represents the preposition in that sentence
  segment.  Have other code that can sort out the sentence into the nouns and
  prepositions, just do it by hand for now.  For path, average the scores of all
  the locations in the path, then multiply by (1-distvar).  For helper noun,
  (1-distvar) is the whole score.

--Thinking more about unary scores for helper nouns.  When talking with Dan, we
  discussed incorporating the spatial relationship into the binary score, but
  I'm not sure how that would work--everything we do with the prepositions
  happens in the unary score.  **NEED TO THINK ABOUT THIS MORE**

--Started working on find-unary-score.  Need to get more info from Dan on how
  the preposition functions he wrote work before I can do the rest.

----------------git commit 85598d8--------------

22 Jan 16

--Working on preposition functions.  Dan suggested using newer von-mises-based
  functions, but I don't know where they are--waiting on Dan's reply.  Currently
  modifying functions I have.  Old versions had score in range [0.5 1], not sure
  why.  Modifying to have score in range [0 1].

--Talked with Dan about von-mises functions and decided against them for
  now--the hardcoded old versions work well enough and are simpler.

--Talked with Dan about normalizing the variance penalty factor by dividing by
  0.5 (or variance of (list 0 0.5))-->looking at this caused me to realize that
  the number I had been saving in low-variance-tubes.sc was not the distance
  variance, but the distance mean.  Fixed it so that now the second element of
  each tube is #(dist-mean dist-var).  Still, should probably use the first
  number, dist-mean, since the mean of the position differences is very similar
  to the position variance--and dividing by 0.5 makes sense since the mean
  position difference will always be < 0.5.

--Switched to using renormalized-von-mises for preposition functions because the
  kappa parameter makes it easy to adjust the relative steepness of the
  function.

--Have find-unary-score working.  Next need to write wrappers that run it over
  all boxes for a path segment, then over all path segments.

----------------git commit d4f8e14--------------

25 Jan 16
 
--Wrote find-unary-scores-for-all-tubes that finds the unary score for each tube
  for a single path-segment/preposition-function pair.  Returns this as a
  VECTOR, since that is probably what will be necessary to send data to C for
  graphical model.

--Adding 3rd element to alignment.sc files that groups together noun,
  preposition, and path segment--this is what will be provided by other
  software.  This has HIERARCHICAL structure.  For head nouns without helper
  nouns, each element is a list of 3 elements: (noun prep frames).  For head
  nouns WITH helper nouns, 4th and subsequent elements of the head noun's list
  are 3-element lists: (helper-noun prep empty-list).

--Wrote get-noun-preposition-paths which takes the new alignment.sc and keeps
  the hierarchical structure while turning frame numbers into actual poses.

--Wrote get-graphical-model-variables which flattens the structure of
  get-noun-preposition-paths so that each element in the list is (noun
  preposition path/null).

--Wrote find-unary-scores-for-one-run which computes the unary scores for every
  noun-preposition pair in a particular run/sentence.  Returns a vector of
  vectors, where the first-level vectors are one per noun-prep pair, and within
  each vector is the unary score for that noun-prep pair for each tube in that
  run. 

--Had to add a (center-angle-at x 0) for every renormalized-von-mises call in
  the preposition functions b/c the von-mises function needs to have both angles
  (v and mean) between -pi and pi.  Seems to work now.

--Wrote find-unary-score-matrix-for-floorplan that finds all the unary scores
  for a particular floorplan based on a list of input dirs that contain the
  floorplan data.

--Wrote functions for number-nouns and make-helper-noun-list to determine which
  nouns have prepositional relationships and what those relationships are.  This
  will be part of find-binary-score-matrices-for-floorplan, which will compute
  all necessary binary scores for a given floorplan (defined as set of input
  dirs with floorplan data.

--Comments/ideas for find-binary-score-matrices...
 ;;general idea here:
 ;;  1)figure out helper-noun relationships from raw alignment
 ;;  2)compute helper-noun binary scores using preposition functions
 ;;  3)figure out which nouns are the same (string matching)
 ;;  4)compute visual/location similarity for same nouns
 ;;    a)visual similarity using PHOW/Chisq & HOG/L2 from Haonan
 ;;    b)location similarity--multiplier to visual sim; will range
 ;;      between 0.75 and 1; use sigmoid to give things that are nearby
 ;;      a higher multiplier while things further apart get smaller mult;
 ;;      floor of 0.75 on mult so as not to destroy scores for different
 ;;      instances of same object; NEED TO DETERMINE SIG THRESH & SLOPE
 ;;      (sig output * 0.25) + 0.75
 ;;  5)take the two lists/vectors of noun-noun similarity-->if both nouns
 ;;    the same, do element-by-element multiply of matrices
 ;;  6)final output will be a single list of
 ;;     #(idx1 idx2 #(ntubes by ntubes matrix of binary score))
 ;;REMEMBER that score matrices only need to have upper-right triangular part

 ;;binary scores for each pair of matched nouns is
 ;;  #(noun1-idx noun2-idx #(numtubes by numtubes matrix))

----------------git commit cfa957d--------------

25 Jan 16 (2)

--Wrote find-noun-to-helper-noun-binary-score to compute binary scores between
  nouns (only works with STATIONARY prepositions: in-front-of, behind, right-of,
  left-of).

--Wrote find-noun-noun-binary-score-matrix that finds the tube-to-tube binary
  scores for given nouns with prepositional relationship.  Discovered that this
  matrix is NOT symmetric, which makes sense because (1 in-front-of 2) is
  DIFFERENT from (2 in-front-of 1).


TO DO's
-------

--think about how to incorporate helper nouns

--Do binary scores
  --one score for same nouns
  --second score for prep. relationships between head and helper nouns
  --if have both, multiply

**NEED TO THINK ABOUT THIS MORE**
--Thinking more about unary scores for helper nouns.  When talking with Dan, we
  discussed incorporating the spatial relationship into the binary score, but
  I'm not sure how that would work--everything we do with the prepositions
  happens in the unary score. 


**THOUGHT--change frame nms threshold to a parameter (currently hardcoded 0.5)


**TASKS from Jeff email 4 Jan 16
  1. fix bug that prevented completion if 6/27 (by EOD m4jan2016) 
  DONE
  2. backproject each box in each tube to world coordinate and compute variance
       over tube (by EOD m4jan2016)  
  DONE
  3. graphical model
     a. similarity between tubes based on PHOW with chi^2 and HOG with L2
        (by EOD m4jan2016)
     b. get sentences for each temporal segment of each video
        (by EOD t5jan2016)
     DONE
        assume segmentation of sentence in phrase, temporal segmentation of
        video, and alignment between the two
     DONE
     c. construct graphical model with vertex for head noun in each phrase
        ranging  over tubes in corresponding temporal segment
        (by EOD t5jan2016)
     d. binary edge between two vertices in different videos with the same
        head noun encoding similarity
        (by EOD t5jan2016)
     e. compute world position for each box in each tube with odometry for that
        frame
        (by EOD m4jan2016)
     DONE
     f. discard tubes that have high variance in set of world positions
        (by EOD m4jan2016)
     DONE
     g. associate each tube with the average world position of the boxes
        (by EOD m4jan2016)
     DONE
     h. assume we know meanings of prepositions
        (done, get from Dan)
     DONE
     i. unary score for vertex based on how well the world position and robot
        position/velocity satisfy head preposition in sentence segment
        (by EOD t5jan2016)
     j. start running graphical model
        (t5jan2016 night)

  4. future investigate ObjectnessMeasure, if works, replace proposal score
     for proposal NMS and tube average proposal score for tube NMS


***screenlog.0 cleanup***
Once you capture your session in screenlog.n , you can cat the file to the terminal and then use screen's hardcopy command to dump the cat's output to a file . The result will provide you with clean output that does not have any escape sequences.

The only 'gotcha' seems to be to make sure that hardcopy captures eveyrthing in the scrollback buffer and that the scrollback buffer contains only what you want to capture.

1. $ screen
2. $ cd /path/to/screenlog.n directory/
3. $ wc -l screenlog.n 
4. $ screen -X scrollback 245 # 245 is the number of lines found from your wc command + 5 
5. $ cat screenlog.n
6. $ screen -X hardcopy -h screenlog.n.cleaned 
Note that -h ensures that you capture the entire scrollback history and not just whats in immediate view

The screenlog.n.cleaned file will now contain a hardcopy of the cat output and won't include any escape sequences
*** ***

***TO THINK ABOUT: 
      For tube NMS: 
       --Formulate some relationship between num-tubes, K, and L--possibly
      specify num-tubes and L, and let that determine K, similar to how I
      already overgenerate K by a factor (10?) for box NMS.  
      --This will  help ensure that the desired num-tubes is output after tube
      NMS.
      --Need to be careful of limits on proposal generation--maybe set a hard
      limit (1500-2000?) and send the min of the requested number and that limit
      into the proposal generation step.  
      --What happens if number of post-NMS tubes is less than num-tubes?  Can we
      deal with that?



***OLD NOTES from optimization of box xyz locations and path points (abandoned)

*********PROBLEMS!------------------
**multiple points from same rectangle linearly dependent???
**Is assuming a rectangle in image plane will be a rectangle in world plane
  invalid??
**When robot is viewing the same object from multiple angles, there is no
  guarantee that the box corners will project to the same world point--that is
  almost certainly not true
**problem seems ill-posed-->how to fix it??

**THOUGHT** Why not try running some sort of fast feature detector on JUST the
  contents of the tube boxes??

**correspondences between boxes no good-->points probably are not on object and
  certainly don't correspond to same world point.
**Jeff suggests limiting to objects that are on ground...how will this help?
********-----------------

--Constraints to enforce on box points during optimization: (**HOW EXACTLY DO I
  ENCODE AND ENFORCE THESE??)
  -coplanar -- how to compute?? 
   Thinking about coplanarity constraint and how to encode it into the model.
   Can write a function that takes 4 points and computes
   (x3-x1)·[(x2-x1)x(x4-x3)].  If that = 0, then the 4 points are coplanar.  But
   if that is nonzero, what does the value represent?  Could be simpler to take
   the 4 points, measure the 4 distances from each point to the plane made from
   the other 3, and then square each and find the mean.
  -rectangle
  -perpendicular to camera axis
  -axis aligned
  -prior on width & height (size limits), bias against
  -prior on world position
    -not behind camera
    -not outside floorplan

--Think about adding more dtrace output to find-points-and-deltas (similar to
  find-deltas-only)

--Compile find-points-and-deltas, find-points-from-lines, and find-deltas-only
  into dsci.  Should be able to just copy the fns into toollib-codetection.sc
  and then recompile dsci.  Compiler should complain if any functions called in
  those are missing, then can copy those fns in as well.  Rinse and repeat.

--Need to write a visualizing function that takes the boxes and the odometry
  output from the optimization and displays them on the images--will use this to
  see whether the optimized box follows the input box in the image.  Should have
  most of this already written, just need to reverse some operations.

--Need to make sure that the points that are optimized are well-distributed over
  the image (top/bottom/left/right-->this will have to happen at the
  proposal/tube generation stage to make sure there are enough proposals-->might
  also be doable at the stage where we choose which tubes to use in the
  optimization-->might have an optimizing set (much smaller than full tube set)
  that covers the image quadrants, use this to optimize track, then use
  optimized track and old find-points-from-lines to find the world points for
  tubes that were not in the optimizing set.

**THOUGHT: **WHAT ABOUT instead of dividing the reduce over
  point-line-squared-distance by the length of the tube, try instead dividing
  the outside reduce by the total of the tube lengths--not the same thing!
  Would this allow longer tubes to have more influence while still scaling to
  the total number of frames with boxes?

**Thought: find average point-line distance across tubes?  Is the path
  optimization getting overwhelmed by the tubes?

**Thought: could some visual odometry/visual SLAM method help us correct the IMU
  odometry error?

***THOUGHT--need to think more about what Jeff said in meeting on 7Jan about
   treating the position (error?) as a low-order polynomial function and then
   using the optimization to estimate the parameters of this function-->how do
   we do this??
***END OLD NOTES***

**OLD NOTES FROM BEFORE XMAS**
       --NEW PLAN:
         1. Take a video and generate n*K proposals for each of L frames.
	 2. Run these proposals through NMS in order to reduce to top K that
	    are as distinct as possible from each other.
	 3. Run Median Flow forward and backward from each of L frames to get
            K*L tubes. 
	 4. Run another round of NMS on the K*L tubes to eliminate similar
            tubes.
	    A. RUN THIS when actually selecting boxes to turn into tubes-->if
	       starting box overlaps too much with a box from an existing tube,
               discard it and get a new starting box
	 5. Visualize remaining tubes to determine if they will be good enough
            to use with the higher-level information. 
	    A. Try using Dan's run_and_display_TLD.cpp as a framework for this
               (throwaway visualization rather than saving images)


       **based on email from Jeff (16Dec), scratch for now**
       --plot tld tube locations on floor plan-->average location constant with
       current box as well (stitched with actual image)
       --Framework for running on full video:
         --determine frames in which robot is turning
	 --split video into segments by these
	 --generate proposals for first frame in each segment
	 --get multiple tld tubes from each segment  
	 --score them somehow??
       **end scratch**


       --Visualizing multiple viterbi tubes
       **2 possible ways to go** (or possibly do both)
         --try to get objectness measure (fscore) to be better/useful
	 --stick with current fscore and play with df to see if we can raise it
	   without getting incoherent tracks
       **Is there some other type of binary measure we can use that's different
         from pixel and world distance?  Those are 
       --Think about ways to get better tracks out--currently getting a
       temporally coherent track that is of a garbage object--implement/use
       objectness measure (Alexe et al.)??--or use non-maximal suppression
       (would probably be computationally intensive)
       --Compile viterbi-boxes into dsci??
       --**THINK ABOUT** new unary score from edgeBoxes seems to max out around
       0.25...do we need a discount factor? or should it be a multiplier instead?
       **LATER:
         --Image tearing--try to sync barriers with camera shutter (FlyCap
       	   API??)
	 --Figure out why dsci on laptop sometimes crashes
       

**LOOK AT objectness score from Alexe et al.


**THINK ABOUT: using linear functions instead of sigmoids for pixel distance and
  world distance (maybe distance-from-robot as well); also using IoU or 4-D
  distance measurement for pixel distance instead of centers



The full list of runs that didn't complete successfully is:
    floorplan-1-sentence-1
    floorplan-1-sentence-2
    floorplan-3-sentence-0
    floorplan-3-sentence-3
    floorplan-4-sentence-0
    floorplan-4-sentence-4
    floorplan-5-sentence-0 (This one did not have an error, it was just
stuck in the bp.infer call.  I killed it manually)
*MIGHT* need to manually remove these runs from the pool that is trying to
    compute proposal_boxes



**SCREEN: screen -ls to list open screens
          screen -r foo to reconnect to screen foo
	  screen -S foo to name session foo
	  screen -L to log output in screenlog.n
	  ^A d to detach screen
	  ^A k to kill session
	  ^A ESC to scroll back

  rlwrap run-dsci to have history in dsci

Open files 25Nov15
.   notes.text          201366  Text		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/notes.text
  * *shell*             449123  Shell:run	  /etc/
    codetection-test.sc  82554  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/new-sentence-codetection/codetection-test.sc
    rendering-code.sc|c: 45472  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/rendering-code.sc
  * *scheme*           1861104  Inferior Scheme:
    toollib-multi-proce: 11819  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/darpa-collaboration/ideas/toollib-multi-process.sc
 %  rendering-code.sc|s: 18204  Scheme		  /amd/upplysingaoflun/root/aux/home/dpbarret/darpa-collaboration/documentation/ralicra2016/supplementary-material/rendering-code.sc
 %  supplementary-materi: 1738  Dired by name	  /amd/upplysingaoflun/root/aux/home/dpbarret/darpa-collaboration/documentation/ralicra2016/supplementary-material/
 %  ralicra2016           1051  Dired by name	  /amd/upplysingaoflun/root/aux/home/dpbarret/darpa-collaboration/documentation/ralicra2016/
 %  documentation        12168  Dired by name	  /amd/upplysingaoflun/root/aux/home/dpbarret/darpa-collaboration/documentation/
 %  darpa-collaboration   3616  Dired by name	  /amd/upplysingaoflun/root/aux/home/dpbarret/darpa-collaboration/
    generate-msee1-data: 54992  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/generate-msee1-dataset.sc
    toollib-misc.sc      44479  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/darpa-collaboration/ideas/toollib-misc.sc
    today.text          120740  Text		  ~/vader-rover/today.text
    toollib-codetection: 18999  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/sentence-codetection/toollib-codetection.sc
  * *shell*<2>          468670  Shell:run	  ~/darpa-collaboration/ideas/
    QobiScheme-AD.sc    295402  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/QobiScheme/source/QobiScheme-AD.sc
  * *shell*<3>          534825  Shell:run	  ~/darpa-collaboration/rover/
    .bash_history        17988  Fundamental	  ~/.bash_history
    log_to_track.cpp     16043  C++/l		  /amd/upplysingaoflun/root/aux/home/sbroniko/vader-rover/position/log_to_track.cpp
 %  toollib-feature-bas: 28048  Scheme		  /amd/upplysingaoflun/root/aux/home/dpbarret/darpa-collaboration/rover/toollib-feature-based-trace-generator.sc
    output.text          11074  Text		  /tmp/output.text
  * *scratch*             2868  Lisp Interaction
    scott_proposals_onl: 16552  MATLAB		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/sentence-codetection/scott_proposals_only.m
    proposals_and_simila: 3956  MATLAB		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/sentence-codetection/proposals_and_similarity.m
    out_camera_data_25p: 44039  nXML Valid	  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/camera_calibration/out_camera_data_25ppms.xml
    ralicra2016-labeling: 7261  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/sentence-codetection/ralicra2016-labeling.sc
    table-test.sc          898  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/sentence-codetection/table-test.sc
    toollib-matlab.sc    12100  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/darpa-collaboration/ideas/toollib-matlab.sc
 %  *Directory*            230  Help
    *Help*                   0  Fundamental
 %  *Disabled Command*     930  Help
  * *Messages*            4119  Fundamental





***THOUGHT: could we use the Viterbi algorithm instead of branch and bound for
   labeling? 




**RANDOM NOTES BELOW**

--Notes on stitched files (floorplan-x-sentence-y):
  0-0: OK, y distance seems to long
  0-1: might be bad, seems like some slippage on right turns on carpet
  0-2: OK
  0-3: very good
  0-4: good
  0-5: good
  0-6: good
  0-7: BAD--looks like data link broke and lost some frames/sensor readings
  1-0: OK, but trace goes out of image (fixed in new animate-house-tracks)
  1-1: OK, trace goes out of image, maybe drift issues at the end (longer path)
  1-2: OK, trace goes out of image

***IDEA: Do a filtering on the images from each floorplan (fplabel)
   individually.  Get descriptors for each image and find mean and stddev.  Then
   reject images that are below a threshold (mean-2*stddev?)

***IDEA: Look at different percentages of the similarity scores.  For a bad
   match, the similarity score with 100% of images should be very close to the
   score for 50% (or other %) best images, since they are all crap matches.  But
   for a good match, eliminating a % of bad matches should make the similarity
   score better...how to use this?
-->Doesn't seem to work.

***Thinking about changing plot_simi_values so that the plots are separated by
   ground-truth label and the good and bad of each are plotted on the same x
   value--this might show us if there's a different threshold for different
   labels.




**Also need to look at implementing A* search.

**Also thinking about how to eliminate equivalent permutations...how to say that
  two permutations are equivalent without evaluating them?

**OVERLAPPING SUBSETS--can run a bunch of smaller subsets to get independent
  labelings and then use the overlap to remap labels to first labeling.

**can we make best-possible run faster? Do we evaluate the score of the already
  set labels every time?  If so, can we pass that as a parameter and just add to it?



----OPEN FILES 20150714-----
.   scheme-buffer-2015: 113763  Inferior Scheme:  ~/codetection/source/sentence-codetection/scheme-buffer-20150714.txt
    notes.text          120366  Text		  ~/codetection/notes.text
    inference.cpp        22240  C++/l		  ~/codetection/source/sentence-codetection/inference.cpp
    label-test-setup.sc   5896  Scheme		  ~/codetection/source/sentence-codetection/label-test-setup.sc
    inference.h            528  C++/l		  ~/codetection/source/sentence-codetection/inference.h
    codetection-test.sc  65019  Scheme		  ~/codetection/source/sentence-codetection/codetection-test.sc
    toollib-codetection.: 7373  Scheme		  ~/codetection/source/sentence-codetection/toollib-codetection.sc
  * *shell*              86781  Shell:run	  ~/codetection/
  * *shell*<2>             990  Shell:run	  /aux/sbroniko/vader-rover/logs/MSEE1-dataset/generation/plan0/detections-test20150618/
  * *shell*<3>          302382  Shell:run	  ~/darpa-collaboration/ideas/x86_64-Linux-3.2.0-4-amd64/
    rover-projection.sc   9409  Scheme		  ~/codetection/source/rover-projection.sc
    toollib-matlab.sc    12100  Scheme		  ~/darpa-collaboration/ideas/toollib-matlab.sc
    QobiScheme-AD.sc    295402  Scheme		  ~/QobiScheme/source/QobiScheme-AD.sc
    toollib-multi-proce: 11819  Scheme		  ~/darpa-collaboration/ideas/toollib-multi-process.sc
----------




**STARTING COMMAND**
(define *data-output-dirname* "test20150617")
(codetect-sort-templabel-training-or-generation *data-output-dirname*)


(define *data-directory* 
"/aux/sbroniko/vader-rover/logs/MSEE1-dataset/generation/")
(define *output-directory*
"/aux/sbroniko/vader-rover/logs/MSEE1-dataset/results-test20150617")
(define *results-filename* "test20150617/results-0.6-0.6.sc")
(define *frame-data-filename* "test20150617/frame-data-0.6-0.6.sc")
(define *server-list* '("verstand" "arivu" "aruco" "save" "akili" "aql"))
(define *source-machine* "seykhl")
(get-object-detections-all-floorplans
	 data-directory ;; NEED slash on data-dir
	 output-directory ;;NO slash on output-dir--this is a full path
	 results-filename ;;remember to add data-output-dir to this and frame-data...
	 frame-data-filename
	 server-list
	 source-machine ;;just a string, i.e., "seykhl"
	 )

***Need to start thinking about the function to take the individually labeled
   floorplans and do comparisons to put common labels on all objects
   --Have the feature vectors saved in phow_hist_fvcell.mat.  Should use those
     instead of recomputing feature vectors.
   --Will need to take individual matrices in fvcell cells and combine the ones
     that match (look at object_xy_with_label.mat to see fp labels)--also need
     to read in the mat files and rename individually, since all have same
     variable names in them--probably use a 2-d cell array to hold matrices
   --Then do something like the comparison starting at line 45 of
     sort_clusters_single_floorplan.m 


**THEN start figuring out the wrapper to do codetection, sort/label objects in
  single floorplan, and then compare and label objects consistently across all
  floorplans. 
   --probably need some new matlab code to run the similarity comparison
   --have to think about dataflow--detections on each run can be done
     independently, but then will need to have an rsync back to a common dataset
     to do floorplans, then another rsync before doing entire dataset (this will
     probably only be one processor unless I can figure out a way to
     parallelize)
   --should also think about reorganizing output data (like in a 'codetection'
     dir under each run)
   --Do I already have code that ensures track.sc is present?? I think so.


* remember that (run-codetection-with-proposals-similarity ) is the main
  function to generate the results; currently only called in
  (visualize-results), which saves the boxed images as well

--------------------------------------------------------------------------------
***TO JEFF: list of 5-10 bullet points of a publishable increment over tacl2015
   -> schedule of milestones to get a paper out for 15sep15 (ICRA or AAAI).

Bullet points:
1. Detect objects in video frames using off-the-shelf object detector (done).

2. Find real-world locations for detected objects (done).

3. Cluster detection locations and extract N cluster centers to find
detected locations of N objects (done).

4. Measure error as distance between detected and ground-truth locations.

5. Group detection images by cluster and do visual similarity
comparison of clusters--both within and between the various floor
plans--so that like objects have same temporary label.

6. Use observed floor plans with temporary labels as replacement for
hand-coded ground-truth floor plans.  Re-run acquisition with these
observed floor plans.  This should result in learning the object
classes, as was done in the original acquisition runs.

   A. From original Acquisition part of MSEE1-dataset, we only have
3/10 floor plans (75/250 runs) that have observable objects in them,
which is not enough to use for Acquisition.  Can fix this in one of a
couple ways:
     1. Do a manual re-drive of the other 7 floor plans (from original
machine-generated sentences) with physical objects this time.  Traces
should be similar enough to the original traces that we could use the
AMT-generated sentences with these new traces.
     2. Do the same as (1) above, except put the new traces back up on
AMT to get new sentences for each, just to be sure that the traces and
sentences match.
     3. Machine generate 7 new floor plans with 25 new sentences each,
drive them, and get AMT sentences from the traces.

  B. Another option is to repurpose the Generation, Autodrive (MSEE1),
or Autodrive (MSEE2) data for this experiment.  I have been using the
Generation data (manually-driven traces) in the testing of the work
I've done so far.  We don't have human-generated sentences for these
traces, but we do for both the old (MSEE1--10 runs/floor plan) and new
(MSEE2--30 runs/floor plan) Autodrive data sets.  I don't know if it
matters that the Autodrive traces were not driven by hand.  We could
also put the original Generation traces up on AMT to collect
human-created sentences for those 100 traces and then use that for the
new acquisition run.

7. Compare learned object classes to ground truth to measure error.

--------------------------------------------------------------------------------
I think the above should be enough of an increment beyond tacl2015 to
be publishable.  If not, I can look at doing the following additional
tasks:

8. Use detected image data to train class-specific object detectors.

9. Substitute these object detectors for the generic object detector
in (1) above, and re-run steps 1-4 to find out if trained object
detectors give more accurate (i.e., closer to ground truth) detections
than the generic object detector.


Schedule:
1-3: already complete
4: complete by m8jun
5: complete by m15jun
6: Decide on course of action for collecting additional data for new
acquisition run by m15jun.  Collect data and put up on AMT (if
necessary) by f19jun.  Anticipate AMT data back no later than f26jun.
Start running acquisition week of m29jun.
7: Estimate that getting results from new acquisition run will take ~1
week.  If results available m6jul, comparison should be complete by
f10jul.
8&9: Start working on these tasks around m13jul (~2 months to
ICRA/AAAI deadline).
--------------------------------------------------------------------------------


***OTHER BIG STEPS***
--Deal interactively with unknown objects
  --Might need to add rangefinder to reduce processing
  --Might need to rewrite the-force to allow changing of path mid-stream
  (processing done on laptop/workstation)
--Add more prepositions (around, between, past, etc.)
--Change frame of reference: outside observer becomes N,S,E,W, robot frame is
L,R,front, behind.
--

**Now how to collect up all the object images within a certain radius of these
  points? 
  - Have xy points where objects were detected (in each floorplan).
  - results-.-..sc in each run directory has 4 lists: 1st is index of selected
  box in each frame, 2nd is world xy locations of selected box in each frame,
  3rd is pixel coordinates of box in frame (x1 y1 x2 y2), 4th is f-score value
  of winning box.
  - Take list of detections (detection_data in MATLAB) and use first 2 columns
  for xy locations.  Then associate each with one of the object locations.
  - BUT how to go from xy locations back to frame data?  Should I somehow
  restructure my previous code to save that data somewhere?  Or can I match by
  xy location (would have to search in each of the runs, since detection_data is
  aggregated at the floorplan level).
  - Look at putting extra columns into detection_data for run#, frame#,
  proposal#--that should be enough to get back to the actual detection image.

***CONVERSATION with Dan 18 May 15-->Can account for viewing objects from one
   side by taking into account the width of the object--can be as simple as
   using the width of the object to push the object "back" 1/2 object width on
   each detection...need to look at this more systematically to make sure that
   this is really what's happening.
		

****COMMENTS FROM JEFF 6 APR 15: Look at redoing scoring function as some sort
    of Expectation Maximization (k-means, gaussian mixture models, etc.) as a
    faster way of finding clusters.  
**Thoughts from talking with Dan about the above: Can also look at using my
    current scoring function with a larger grid spacing (5cm, 10cm, etc.) to
    find initial points to use with GMM or K-means

******SEGMENT VIDEOS WITH SENTENCES IS NEXT STEP*******
**look at learning.sc in acl2015/code

***In auto-drive dataset, plan0 (and plan1?) have 20 runs, but floorplans are
   different between 1st and 2nd 10***


***BASH SCRIPTS TO LOOK FOR MISSING FOOTER LINES IN CAMERAFRONT.TXT****
**to list filenames with grep output below 
for i in `ls | grep plan`;
 do 
 for j in `ls $i`;
  do
   echo $i/$j 
   fgrep "logging stopped" $i/$j/camera_front.txt
 done
done

**to do a count by directory
for i in `ls | grep plan`;
 do echo $i 
  fgrep "logging stopped" $i/*/camera_front.txt | wc -l
done


**if we need it, Dan and I worked out the math to compute box world height--on
  paper on my desk.


****MIGHT WANT TO KEY DUMMY BOX SCORE TO ADJACENCY--IF FRAMES ARE NOT ADJACENT,
    DUMMY SCORE IS BAD?????**** Maybe not--talking with Dan convinced me otherwise

**Still need to add binary score factor for temporal coherency / optical flow
  (Scheme->C bindings or in C directly?)

OPEN FILES IN MATLAB (28 Feb 15)
--------------------
~/codetection/source/sentence-codetection/27feb.mat (in codetection-no-git also)
~/codetection/source/robottest.m
~/codetection/source/sentence-codetection/proposals_and_similarity.m
~/codetection/source/sentence-codetection/scott_proposals_similarity2.m
~/codetection/vlfeat-0.9.17/apps/phow_box_match.m (???)



OPEN FILES IN EMACS (28 Feb 15)
-------------------
.   notes.text           28772  Text		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/notes.text
    codetection-test.sc  10504  Scheme		  /amd/upplysingaoflun/root/aux/home/sbroniko/codetection/source/sentence-codetection/codetection-test.sc
    toollib-codetection.: 6850  Scheme		  ~/codetection/source/sentence-codetection/toollib-codetection.sc
    rover-projection.sc   9062  Scheme		  ~/codetection/source/rover-projection.sc
 %  toollib-perspective: 18982  Scheme		  /home/dpbarret/imitate/tool/toollib-perspective-projection.sc
 %  tmp-toollib-misc.s: 104697  Scheme		  /home/dpbarret/imitate/tool/tmp-toollib-misc.sc
    inference.cpp         2776  C++/l		  ~/codetection/source/sentence-codetection/inference.cpp
    codetectionlib-c.h    1953  C++/l		  ~/codetection/source/sentence-codetection/codetectionlib-c.h
    idealib-c.c          40213  C/l		  ~/darpa-collaboration/ideas/idealib-c.c
    codetectionlib-c.c    6720  C/l		  ~/codetection/source/sentence-codetection/codetectionlib-c.c
    makefile|sentence-co: 2620  GNUmakefile	  ~/codetection/source/sentence-codetection/makefile
 %  makefile|ideas       14656  GNUmakefile	  /home/dpbarret/darpa-collaboration/ideas/makefile
    pregexp.sc           22557  Scheme		  ~/codetection/source/sentence-codetection/pregexp.sc
    inference.h             83  C++/l		  ~/codetection/source/sentence-codetection/inference.h

.   prelim-notes.txt       368  Text		  ~/research/prelim-notes.txt
    research.bib         43019  BibTeX		  ~/research/research.bib
    notes.text           29627  Text		  ~/research/1_lit_review/notes.text
    toollib-farneback.sc  5122  Scheme		  ~/darpa-collaboration/ideas/toollib-farneback.sc
    toollib-farneback-c: 12750  C/l		  ~/darpa-collaboration/ideas/toollib-farneback-c.c
    toollib-image-proc: 125308  Scheme		  ~/darpa-collaboration/ideas/toollib-image-processing-scott.sc
    rover-projection.sc   9062  Scheme		  ~/codetection/source/rover-projection.sc
    toollib-image-proc: 124889  Scheme		  ~/darpa-collaboration/ideas/toollib-image-processing.sc
    toollib-perspective: 18982  Scheme		  ~/codetection/source/toollib-perspective-projection.sc
    hmm-def.c            11934  C/l		  /amd/upplysingaoflun/root/aux/home/sbroniko/darpa-collaboration/ideas/hmm-def.c
    hmm-rand.c          306415  C/l		  ~/darpa-collaboration/ideas/hmm-rand.c




* incorporate optical flow into binary score...how???
* box optical flow vs. avg. optical flow might not work--need to look at how the
  geometry works out.

**look at doing single-frame optical flow-->Dan has scheme code that can use C
  bindings--will need to pull original scores out of MATLAB into scheme and then
  add appropriately-weighted optical flow scores in scheme.


******Look at saliency score for image/boxes as a substitute for f (proposal)
      score.  saliency.mit.edu
****Can look at using a comparison between some metric inside the box and
      outside the box (or over the whole frame) to get a better proposal score.
      First thought is to use average color.  Can find average color in each of
      the 3 channels and then treat as a 3-vector and compute distance.  Bigger
      distance = better score.  Could also use some different metric.
****Consider optical flow as well.  Can find optical flow for whole image,
      average it, subtract out the average, and then look at the optical flow
      inside the proposal box.

Looking at using a combination of saliency and average color to build a new
unary score.--Current similarity measure seems to do a good job selecting
consistent boxes, but which track wins depends on unary proposal score, which
doesn't seem very good.


